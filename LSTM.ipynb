{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bps_gig8n_ma",
        "outputId": "e9f98fd0-f810-4f0b-c6c3-743cd7fffa78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.7.0\n"
          ]
        }
      ],
      "source": [
        "pip install portalocker"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stacked LSTM implementation\n",
        "# Currently, LSTM layer number is fixed at 3. Investigating how to make layer number variable while allowing gradients to flow properly\n",
        "# At the moment, the issue is the hidden states and cell states not being able to be concantenated into a single tensor. Doing so would\n",
        "# allow for variable layers, but this stops gradient flow due to tensor slicing being an in-place operation, so another method needs to\n",
        "# be found to achieve this result\n",
        "\n",
        "import torch\n",
        "from torchtext import datasets\n",
        "from torch import nn\n",
        "import random\n",
        "\n",
        "# Architecture hyperparams\n",
        "num_chars = 64\n",
        "LSTM_sz = 512\n",
        "LSTM_layer_n = 3\n",
        "dropout_p = 0.5\n",
        "train_batch_sz = 50\n",
        "truncate_len = 100\n",
        "\n",
        "sample_topk = 3\n",
        "sample_freq = 25\n",
        "special_char = [\" \", \"<\", \">\", \"/\", \"\\\"\", \"\\'\", \":\", \";\", \".\", \"(\", \")\", \"!\"]\n",
        "\n",
        "# learning rate\n",
        "lr = 3e-4\n",
        "\n",
        "# Epochs to train model for (each epoch loops through the corpus once)\n",
        "epochs = 50\n",
        "\n",
        "# Number of characters to sample from model each testing cycle\n",
        "sample_length = 500\n",
        "\n",
        "# Convert character to index\n",
        "def char_index(x):\n",
        "  if ord(x) < 91 and ord(x) > 64:\n",
        "    return ord(x) - 65\n",
        "  if ord(x) < 123 and ord(x) > 96:\n",
        "    return ord(x) - 71\n",
        "  return special_char.index(x) + 52\n",
        "\n",
        "# Filter characters in input data for relevant characters\n",
        "def keep_char(x):\n",
        "  return (ord(x) < 91 and ord(x) > 64) or (ord(x) < 123 and ord(x) > 96) or x in special_char\n",
        "\n",
        "# Convert index to character\n",
        "def index_char(ind):\n",
        "  if ind < 26:\n",
        "    return chr(ind + 65)\n",
        "  if ind < 52:\n",
        "    return chr(ind + 71)\n",
        "  return special_char[ind - 52]\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self, sz):\n",
        "    super().__init__()\n",
        "\n",
        "    self.f = nn.Sequential(\n",
        "        nn.Linear(2 * sz, sz),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "    self.i1 = nn.Sequential(\n",
        "        nn.Linear(2 * sz, sz),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "    self.i2 = nn.Sequential(\n",
        "        nn.Linear(2 * sz, sz),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "    self.o = nn.Sequential(\n",
        "        nn.Linear(2 * sz, sz),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "  def forward(self, inp, hidden, cell):\n",
        "\n",
        "    inp = self.dropout(inp)\n",
        "    xh = torch.cat((inp, hidden))\n",
        "    cell = cell * self.f(xh)\n",
        "    cell = cell + (self.i1(xh) * self.i2(xh))\n",
        "    hidden = torch.tanh(cell) * self.o(xh)\n",
        "\n",
        "    return hidden, hidden, cell\n",
        "\n",
        "class StackedLSTM(nn.Module):\n",
        "  def __init__(self, layer_num, inp_sz, LSTM_sz):\n",
        "    super().__init__()\n",
        "    self.layers = layer_num\n",
        "\n",
        "    self.embed = nn.Embedding(inp_sz, LSTM_sz)\n",
        "    self.LSTM_layers = nn.ModuleList([LSTM(LSTM_sz).to(device) for _ in range(layer_num)])\n",
        "    self.output = nn.Linear(LSTM_sz, inp_sz)\n",
        "\n",
        "  def forward(self, inp, hidden_state1, hidden_state2, hidden_state3, cell_state1, cell_state2, cell_state3):\n",
        "    out = self.embed(inp)\n",
        "    new_hidden1 = hidden_state1\n",
        "    new_hidden2 = hidden_state2\n",
        "    new_hidden3 = hidden_state3\n",
        "    new_cell1 = cell_state1\n",
        "    new_cell2 = cell_state2\n",
        "    new_cell3 = cell_state3\n",
        "\n",
        "    out, new_hidden1, new_cell1 = self.LSTM_layers[0](out, new_hidden1, new_cell1)\n",
        "    out, new_hidden2, new_cell2 = self.LSTM_layers[1](out, new_hidden2, new_cell2)\n",
        "    out, new_hidden3, new_cell3 = self.LSTM_layers[2](out, new_hidden3, new_cell3)\n",
        "\n",
        "    out = torch.softmax(self.output(out), dim = 0)\n",
        "\n",
        "    return out, new_hidden1, new_hidden2, new_hidden3, new_cell1, new_cell2, new_cell3\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "\n",
        "model = StackedLSTM(LSTM_layer_n, num_chars, LSTM_sz).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Train model\n",
        "def training_cycle():\n",
        "  # Training data (IMDB database in torchtext)\n",
        "  training_dataloader = iter(datasets.IMDB(split=\"train\", root = \"data\"))\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  train_cycle_num = 1\n",
        "\n",
        "  while True:\n",
        "    try:\n",
        "      print(f\"Train Cycle: {train_cycle_num}\")\n",
        "      train_cycle_num += 1\n",
        "\n",
        "      loss = 0\n",
        "      sampled_chars = 0\n",
        "\n",
        "      for _ in range(train_batch_sz):\n",
        "        string = ''.join(filter(keep_char, next(training_dataloader)[1]))\n",
        "\n",
        "        hidden1 = torch.zeros(LSTM_sz).to(device)\n",
        "        hidden2 = torch.zeros(LSTM_sz).to(device)\n",
        "        hidden3 = torch.zeros(LSTM_sz).to(device)\n",
        "        cell1 = torch.zeros(LSTM_sz).to(device)\n",
        "        cell2 = torch.zeros(LSTM_sz).to(device)\n",
        "        cell3 = torch.zeros(LSTM_sz).to(device)\n",
        "\n",
        "        for i in range(0, min(len(string) - 1, truncate_len - 1)):\n",
        "          out, hidden1, hidden2, hidden3, cell1, cell2, cell3 = model(torch.tensor(char_index(string[i])).to(device), hidden1, hidden2, hidden3, cell1, cell2, cell3)\n",
        "\n",
        "          loss += criterion(torch.log(out), torch.tensor(char_index(string[i+1])).to(device))\n",
        "          sampled_chars += 1\n",
        "\n",
        "      loss = loss / sampled_chars\n",
        "\n",
        "      model.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
        "      opt.step()\n",
        "\n",
        "      print(f\"Average Loss Per Character: {round(loss.item(), 3)}\")\n",
        "\n",
        "      if (train_cycle_num % sample_freq == 0):\n",
        "        test_cycle()\n",
        "\n",
        "    except StopIteration:\n",
        "      break\n",
        "\n",
        "# Sample from model to test progress every once in a while\n",
        "def test_cycle():\n",
        "  model.eval()\n",
        "  init_char = random.randint(0, num_chars - 1)\n",
        "\n",
        "  hidden1 = torch.zeros(LSTM_sz).to(device)\n",
        "  hidden2 = torch.zeros(LSTM_sz).to(device)\n",
        "  hidden3 = torch.zeros(LSTM_sz).to(device)\n",
        "  cell1 = torch.zeros(LSTM_sz).to(device)\n",
        "  cell2 = torch.zeros(LSTM_sz).to(device)\n",
        "  cell3 = torch.zeros(LSTM_sz).to(device)\n",
        "\n",
        "  for i in range(sample_length):\n",
        "    print(index_char(init_char), end = \"\")\n",
        "\n",
        "    out, hidden1, hidden2, hidden3, cell1, cell2, cell3 = model(torch.tensor(init_char).to(device), hidden1, hidden2, hidden3, cell1, cell2, cell3)\n",
        "    top_chars = torch.topk(out, sample_topk)\n",
        "    init_char = top_chars[1][list(torch.utils.data.WeightedRandomSampler(nn.functional.softmax(top_chars[0], dim = 0), 1))[0]].item()\n",
        "\n",
        "# Training cycles\n",
        "for epoch in range(epochs):\n",
        "  print(f\"Epoch {epoch}\")\n",
        "  training_cycle()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIglrGJ5N0gj",
        "outputId": "00d4c5be-7d4a-4bf3-de39-0a7802374a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "Train Cycle: 1\n",
            "Average Loss Per Character: 4.162\n",
            "Train Cycle: 2\n",
            "Average Loss Per Character: 4.153\n",
            "Train Cycle: 3\n",
            "Average Loss Per Character: 4.142\n",
            "Train Cycle: 4\n",
            "Average Loss Per Character: 4.127\n",
            "Train Cycle: 5\n",
            "Average Loss Per Character: 4.097\n",
            "Train Cycle: 6\n",
            "Average Loss Per Character: 4.048\n",
            "Train Cycle: 7\n",
            "Average Loss Per Character: 3.95\n",
            "Train Cycle: 8\n",
            "Average Loss Per Character: 3.751\n",
            "Train Cycle: 9\n",
            "Average Loss Per Character: 3.465\n",
            "Train Cycle: 10\n",
            "Average Loss Per Character: 3.422\n",
            "Train Cycle: 11\n",
            "Average Loss Per Character: 3.334\n",
            "Train Cycle: 12\n",
            "Average Loss Per Character: 3.247\n",
            "Train Cycle: 13\n",
            "Average Loss Per Character: 3.143\n",
            "Train Cycle: 14\n",
            "Average Loss Per Character: 3.167\n",
            "Train Cycle: 15\n",
            "Average Loss Per Character: 3.166\n",
            "Train Cycle: 16\n",
            "Average Loss Per Character: 3.219\n",
            "Train Cycle: 17\n",
            "Average Loss Per Character: 3.2\n",
            "Train Cycle: 18\n",
            "Average Loss Per Character: 3.137\n",
            "Train Cycle: 19\n",
            "Average Loss Per Character: 3.152\n",
            "Train Cycle: 20\n",
            "Average Loss Per Character: 3.17\n",
            "Train Cycle: 21\n",
            "Average Loss Per Character: 3.117\n",
            "Train Cycle: 22\n",
            "Average Loss Per Character: 3.059\n",
            "Train Cycle: 23\n",
            "Average Loss Per Character: 3.1\n",
            "Train Cycle: 24\n",
            "Average Loss Per Character: 3.1\n",
            "Ie  et t ttet  e et eet ttt teeeetttt t  teettet teettt  t  tt  et  eee tetttee ee  t teeete te teet ee   ee ee e    t  e  eeeeet e     e e  ee   ee ee  e ee  teet  ett   t t tee  e ee e e et eee e t eeet   eeeett       e e   e tetett tt t  tte   eettt teteet  etet e t e tt  et te ee ett    ee e te t eet e       t   tee      e  t  tt eeetttt  tt ee te  t  tet   ee    et e  tt   ttttt  eetet ttee te  t et   et et te  e  t  tt t ete  tetett   teee ettte  tettte teee  t ee t   etee eee     et eee  Train Cycle: 25\n",
            "Average Loss Per Character: 3.108\n",
            "Train Cycle: 26\n",
            "Average Loss Per Character: 3.093\n",
            "Train Cycle: 27\n",
            "Average Loss Per Character: 3.158\n",
            "Train Cycle: 28\n",
            "Average Loss Per Character: 3.081\n",
            "Train Cycle: 29\n",
            "Average Loss Per Character: 3.134\n",
            "Train Cycle: 30\n",
            "Average Loss Per Character: 3.086\n",
            "Train Cycle: 31\n",
            "Average Loss Per Character: 3.127\n",
            "Train Cycle: 32\n",
            "Average Loss Per Character: 3.122\n",
            "Train Cycle: 33\n",
            "Average Loss Per Character: 3.092\n",
            "Train Cycle: 34\n",
            "Average Loss Per Character: 3.073\n",
            "Train Cycle: 35\n",
            "Average Loss Per Character: 3.122\n",
            "Train Cycle: 36\n",
            "Average Loss Per Character: 3.057\n",
            "Train Cycle: 37\n",
            "Average Loss Per Character: 3.117\n",
            "Train Cycle: 38\n",
            "Average Loss Per Character: 3.08\n",
            "Train Cycle: 39\n",
            "Average Loss Per Character: 3.08\n",
            "Train Cycle: 40\n",
            "Average Loss Per Character: 3.063\n",
            "Train Cycle: 41\n",
            "Average Loss Per Character: 3.109\n",
            "Train Cycle: 42\n",
            "Average Loss Per Character: 3.102\n",
            "Train Cycle: 43\n",
            "Average Loss Per Character: 3.107\n",
            "Train Cycle: 44\n",
            "Average Loss Per Character: 3.086\n",
            "Train Cycle: 45\n",
            "Average Loss Per Character: 3.129\n",
            "Train Cycle: 46\n",
            "Average Loss Per Character: 3.056\n",
            "Train Cycle: 47\n",
            "Average Loss Per Character: 3.102\n",
            "Train Cycle: 48\n",
            "Average Loss Per Character: 3.062\n",
            "Train Cycle: 49\n",
            "Average Loss Per Character: 3.016\n",
            "qe  eteeete  tet etteee  t e t et   ee eet   tetet  ee ee eett eeet e  te etetttt e e eeee etettt etett  ettt e et  ttteeeeett eetteet  eet    et  teet tttettte   eeett  e ttttetee ett ttet teteetetttt ete    e  et tetet eteet etee ttetee tte ttett t teett tetee  ete eteee  etettee t tt  ee eeee  ett etee   tt  eettete t et tee    te e  et ee  tte eeet e e tteeet e tte et t ee e t eet e  ttette  ttte t ee ettee eettt  t   t      eee t  t  e tt     ttttetee  tet  tttett   et t tt   t e t  teetteeTrain Cycle: 50\n",
            "Average Loss Per Character: 3.088\n",
            "Train Cycle: 51\n",
            "Average Loss Per Character: 3.081\n",
            "Train Cycle: 52\n",
            "Average Loss Per Character: 3.088\n",
            "Train Cycle: 53\n",
            "Average Loss Per Character: 3.106\n",
            "Train Cycle: 54\n",
            "Average Loss Per Character: 3.08\n",
            "Train Cycle: 55\n",
            "Average Loss Per Character: 3.047\n",
            "Train Cycle: 56\n",
            "Average Loss Per Character: 3.072\n",
            "Train Cycle: 57\n",
            "Average Loss Per Character: 3.054\n",
            "Train Cycle: 58\n",
            "Average Loss Per Character: 3.035\n",
            "Train Cycle: 59\n",
            "Average Loss Per Character: 3.075\n",
            "Train Cycle: 60\n",
            "Average Loss Per Character: 3.042\n",
            "Train Cycle: 61\n",
            "Average Loss Per Character: 3.096\n",
            "Train Cycle: 62\n",
            "Average Loss Per Character: 3.082\n",
            "Train Cycle: 63\n",
            "Average Loss Per Character: 3.044\n",
            "Train Cycle: 64\n",
            "Average Loss Per Character: 3.098\n",
            "Train Cycle: 65\n",
            "Average Loss Per Character: 3.053\n",
            "Train Cycle: 66\n",
            "Average Loss Per Character: 3.143\n",
            "Train Cycle: 67\n",
            "Average Loss Per Character: 3.037\n",
            "Train Cycle: 68\n",
            "Average Loss Per Character: 3.098\n",
            "Train Cycle: 69\n",
            "Average Loss Per Character: 3.05\n",
            "Train Cycle: 70\n",
            "Average Loss Per Character: 3.074\n",
            "Train Cycle: 71\n",
            "Average Loss Per Character: 3.053\n",
            "Train Cycle: 72\n",
            "Average Loss Per Character: 3.04\n",
            "Train Cycle: 73\n",
            "Average Loss Per Character: 3.028\n",
            "Train Cycle: 74\n",
            "Average Loss Per Character: 3.095\n",
            "be t te etee ee t   etee eettete  tt tet eeet e   t  tte   tteette eteettte eee t   ee tte  t t    tee e t eee e eette ee ette   eeete  ete et t eet  ee tee  t tttte  ttettt eet  t ee t   tt ee ttet e tet t eteee ee e   e e eeet   eee   etetttee tte tteeet tte  eeteteteeeeteeee    te e eett t e t   te te  tte t  t ete eteett  e  tteee e eee eett   teteeet te ett  t  e te e tet  te ee  e tee t  tteetee ee ttete     etet te t ttt t      ee    e  tte teett e te tete  teeee etett ettt t e  te  ee teTrain Cycle: 75\n",
            "Average Loss Per Character: 3.042\n",
            "Train Cycle: 76\n",
            "Average Loss Per Character: 3.042\n",
            "Train Cycle: 77\n",
            "Average Loss Per Character: 3.052\n",
            "Train Cycle: 78\n",
            "Average Loss Per Character: 3.038\n",
            "Train Cycle: 79\n",
            "Average Loss Per Character: 3.009\n",
            "Train Cycle: 80\n",
            "Average Loss Per Character: 3.056\n",
            "Train Cycle: 81\n",
            "Average Loss Per Character: 3.09\n",
            "Train Cycle: 82\n",
            "Average Loss Per Character: 3.007\n",
            "Train Cycle: 83\n",
            "Average Loss Per Character: 2.992\n",
            "Train Cycle: 84\n",
            "Average Loss Per Character: 2.989\n",
            "Train Cycle: 85\n",
            "Average Loss Per Character: 2.994\n",
            "Train Cycle: 86\n",
            "Average Loss Per Character: 3.062\n",
            "Train Cycle: 87\n",
            "Average Loss Per Character: 2.994\n",
            "Train Cycle: 88\n",
            "Average Loss Per Character: 2.985\n",
            "Train Cycle: 89\n",
            "Average Loss Per Character: 2.985\n",
            "Train Cycle: 90\n",
            "Average Loss Per Character: 2.975\n",
            "Train Cycle: 91\n",
            "Average Loss Per Character: 3.007\n",
            "Train Cycle: 92\n",
            "Average Loss Per Character: 2.966\n",
            "Train Cycle: 93\n",
            "Average Loss Per Character: 2.942\n",
            "Train Cycle: 94\n",
            "Average Loss Per Character: 3.011\n",
            "Train Cycle: 95\n",
            "Average Loss Per Character: 2.923\n",
            "Train Cycle: 96\n",
            "Average Loss Per Character: 2.939\n",
            "Train Cycle: 97\n",
            "Average Loss Per Character: 2.895\n",
            "Train Cycle: 98\n",
            "Average Loss Per Character: 2.894\n",
            "Train Cycle: 99\n",
            "Average Loss Per Character: 2.928\n",
            "yreesss tee ate oe a ta at a to o oaee tet toete tatttte toeee a a teeettesees otett a te oe a oett ate ateee aet att o teete oe tetett o o tet oatt tet oae oa tote otee toeees oetessee tettt teeeeseeee toe tote att to taee to o teet to attese aee te o toe to tat aet ae tet a at oa at atee atee ae a ta oa aeee a oete o oe o toe te oa at o aettt at o o aeee to a oe tet tet a attee oe oaettteesse ot ae oa oeteeeeeee o o teet ateess oe a oaeee atet atte teet o a tat teet oa to oae toe teee ae ta aeTrain Cycle: 100\n",
            "Average Loss Per Character: 2.941\n",
            "Train Cycle: 101\n",
            "Average Loss Per Character: 2.888\n",
            "Train Cycle: 102\n",
            "Average Loss Per Character: 2.91\n",
            "Train Cycle: 103\n",
            "Average Loss Per Character: 2.826\n",
            "Train Cycle: 104\n",
            "Average Loss Per Character: 2.909\n",
            "Train Cycle: 105\n",
            "Average Loss Per Character: 2.838\n",
            "Train Cycle: 106\n",
            "Average Loss Per Character: 2.899\n",
            "Train Cycle: 107\n",
            "Average Loss Per Character: 2.823\n",
            "Train Cycle: 108\n",
            "Average Loss Per Character: 2.843\n",
            "Train Cycle: 109\n",
            "Average Loss Per Character: 2.869\n",
            "Train Cycle: 110\n",
            "Average Loss Per Character: 2.914\n",
            "Train Cycle: 111\n",
            "Average Loss Per Character: 2.84\n",
            "Train Cycle: 112\n",
            "Average Loss Per Character: 2.862\n",
            "Train Cycle: 113\n",
            "Average Loss Per Character: 2.793\n",
            "Train Cycle: 114\n",
            "Average Loss Per Character: 2.791\n",
            "Train Cycle: 115\n",
            "Average Loss Per Character: 2.821\n",
            "Train Cycle: 116\n",
            "Average Loss Per Character: 2.79\n",
            "Train Cycle: 117\n",
            "Average Loss Per Character: 2.837\n",
            "Train Cycle: 118\n",
            "Average Loss Per Character: 2.792\n",
            "Train Cycle: 119\n",
            "Average Loss Per Character: 2.803\n",
            "Train Cycle: 120\n",
            "Average Loss Per Character: 2.796\n",
            "Train Cycle: 121\n",
            "Average Loss Per Character: 2.734\n",
            "Train Cycle: 122\n",
            "Average Loss Per Character: 2.823\n",
            "Train Cycle: 123\n",
            "Average Loss Per Character: 2.78\n",
            "Train Cycle: 124\n",
            "Average Loss Per Character: 2.802\n",
            "Pe hoin on a a a a ta the ton onn taenss the to oie ooe oo oo the tan tann toe oonn oon on tone ant oentes on an taent tonns an oen on tho oe onee a a aensse an ae tan oo ae a oen ant oo thinn onn a ooeees ane oo ta an then anee oon oent a tann toens tane ae on an a an tonees thone aents a oientssseee ant ae ane thi a oe ta toe a aen ta thoe an toeesss thi a a thoee oeeseeess a oie thin a ton ane oe tho aee ooees oon thene taee ae ae ta oo to ae oon a oee antseee ooeesses oon a oens oee ae an toTrain Cycle: 125\n",
            "Average Loss Per Character: 2.731\n",
            "Train Cycle: 126\n",
            "Average Loss Per Character: 2.72\n",
            "Train Cycle: 127\n",
            "Average Loss Per Character: 2.711\n",
            "Train Cycle: 128\n",
            "Average Loss Per Character: 2.704\n",
            "Train Cycle: 129\n",
            "Average Loss Per Character: 2.766\n",
            "Train Cycle: 130\n",
            "Average Loss Per Character: 2.695\n",
            "Train Cycle: 131\n",
            "Average Loss Per Character: 2.676\n",
            "Train Cycle: 132\n",
            "Average Loss Per Character: 2.682\n",
            "Train Cycle: 133\n",
            "Average Loss Per Character: 2.719\n",
            "Train Cycle: 134\n",
            "Average Loss Per Character: 2.78\n",
            "Train Cycle: 135\n",
            "Average Loss Per Character: 2.682\n",
            "Train Cycle: 136\n",
            "Average Loss Per Character: 2.658\n",
            "Train Cycle: 137\n",
            "Average Loss Per Character: 2.703\n",
            "Train Cycle: 138\n",
            "Average Loss Per Character: 2.702\n",
            "Train Cycle: 139\n",
            "Average Loss Per Character: 2.641\n",
            "Train Cycle: 140\n",
            "Average Loss Per Character: 2.677\n",
            "Train Cycle: 141\n",
            "Average Loss Per Character: 2.622\n",
            "Train Cycle: 142\n",
            "Average Loss Per Character: 2.633\n",
            "Train Cycle: 143\n",
            "Average Loss Per Character: 2.602\n",
            "Train Cycle: 144\n",
            "Average Loss Per Character: 2.623\n",
            "Train Cycle: 145\n",
            "Average Loss Per Character: 2.599\n",
            "Train Cycle: 146\n",
            "Average Loss Per Character: 2.582\n",
            "Train Cycle: 147\n",
            "Average Loss Per Character: 2.599\n",
            "Train Cycle: 148\n",
            "Average Loss Per Character: 2.57\n",
            "Train Cycle: 149\n",
            "Average Loss Per Character: 2.555\n",
            "an the a wo a whinttesese tas the to tos ant to as the a as waseds wa the whe anst a an the whestin whiss wans a ans tas tanss as wa ontinntest ast a to that tor ant inteds wonte wan wo ase there antit ther ther to a antisedse waste wa as wo te whas ant in wantins a ta a a whiss wontestin tans a ante anst tor i thites tosess tos wher iss that asteste a as tas wons anstis to a the whit to thassst in tas to a wast ante thas a was whes an tase thite to we isssese whit thes ansed wastit a wonstise tTrain Cycle: 150\n",
            "Average Loss Per Character: 2.622\n",
            "Train Cycle: 151\n",
            "Average Loss Per Character: 2.587\n",
            "Train Cycle: 152\n",
            "Average Loss Per Character: 2.493\n",
            "Train Cycle: 153\n",
            "Average Loss Per Character: 2.557\n",
            "Train Cycle: 154\n",
            "Average Loss Per Character: 2.6\n",
            "Train Cycle: 155\n",
            "Average Loss Per Character: 2.505\n",
            "Train Cycle: 156\n",
            "Average Loss Per Character: 2.506\n",
            "Train Cycle: 157\n",
            "Average Loss Per Character: 2.559\n",
            "Train Cycle: 158\n",
            "Average Loss Per Character: 2.511\n",
            "Train Cycle: 159\n",
            "Average Loss Per Character: 2.594\n",
            "Train Cycle: 160\n",
            "Average Loss Per Character: 2.544\n",
            "Train Cycle: 161\n",
            "Average Loss Per Character: 2.539\n",
            "Train Cycle: 162\n",
            "Average Loss Per Character: 2.531\n",
            "Train Cycle: 163\n",
            "Average Loss Per Character: 2.589\n",
            "Train Cycle: 164\n",
            "Average Loss Per Character: 2.475\n",
            "Train Cycle: 165\n",
            "Average Loss Per Character: 2.54\n",
            "Train Cycle: 166\n",
            "Average Loss Per Character: 2.542\n",
            "Train Cycle: 167\n",
            "Average Loss Per Character: 2.494\n",
            "Train Cycle: 168\n",
            "Average Loss Per Character: 2.477\n",
            "Train Cycle: 169\n",
            "Average Loss Per Character: 2.494\n",
            "Train Cycle: 170\n",
            "Average Loss Per Character: 2.553\n",
            "Train Cycle: 171\n",
            "Average Loss Per Character: 2.44\n",
            "Train Cycle: 172\n",
            "Average Loss Per Character: 2.453\n",
            "Train Cycle: 173\n",
            "Average Loss Per Character: 2.483\n",
            "Train Cycle: 174\n",
            "Average Loss Per Character: 2.434\n",
            "Fh to wo thess tares ontent as who and.es an these ant the asd.es tor ot a a thins. whess. tor as tas a thas an one tas tosedede antesesed the thes.ed whis a whar therteres..ses.e to an onedes.e tor or tan tor an ote wores asd ot tor as wand a a thes. wantente ast a an otite and. thint thinsede where tan wart to as thes the and asd. tart a an on as wo whe a wons ot ored oned. and thare tare wortitese wost ores.es tos as tar as ande an ortin ot a tand and.ed on wose wart a wass as tarentis wast oTrain Cycle: 175\n",
            "Average Loss Per Character: 2.515\n",
            "Train Cycle: 176\n",
            "Average Loss Per Character: 2.405\n",
            "Train Cycle: 177\n",
            "Average Loss Per Character: 2.397\n",
            "Train Cycle: 178\n",
            "Average Loss Per Character: 2.503\n",
            "Train Cycle: 179\n",
            "Average Loss Per Character: 2.467\n",
            "Train Cycle: 180\n",
            "Average Loss Per Character: 2.412\n",
            "Train Cycle: 181\n",
            "Average Loss Per Character: 2.4\n",
            "Train Cycle: 182\n",
            "Average Loss Per Character: 2.44\n",
            "Train Cycle: 183\n",
            "Average Loss Per Character: 2.398\n",
            "Train Cycle: 184\n",
            "Average Loss Per Character: 2.378\n",
            "Train Cycle: 185\n",
            "Average Loss Per Character: 2.425\n",
            "Train Cycle: 186\n",
            "Average Loss Per Character: 2.395\n",
            "Train Cycle: 187\n",
            "Average Loss Per Character: 2.411\n",
            "Train Cycle: 188\n",
            "Average Loss Per Character: 2.395\n",
            "Train Cycle: 189\n",
            "Average Loss Per Character: 2.424\n",
            "Train Cycle: 190\n",
            "Average Loss Per Character: 2.415\n",
            "Train Cycle: 191\n",
            "Average Loss Per Character: 2.344\n",
            "Train Cycle: 192\n",
            "Average Loss Per Character: 2.356\n",
            "Train Cycle: 193\n",
            "Average Loss Per Character: 2.393\n",
            "Train Cycle: 194\n",
            "Average Loss Per Character: 2.35\n",
            "Train Cycle: 195\n",
            "Average Loss Per Character: 2.401\n",
            "Train Cycle: 196\n",
            "Average Loss Per Character: 2.406\n",
            "Train Cycle: 197\n",
            "Average Loss Per Character: 2.468\n",
            "Train Cycle: 198\n",
            "Average Loss Per Character: 2.424\n",
            "Train Cycle: 199\n",
            "Average Loss Per Character: 2.38\n",
            "<heres of anth thates thes. a me an tastend. tas a that on wosed a whint tart of tar tanser thasser an ot the whis wans to a wo tas a merithis wo thatte a moris ofs.res tossess ander asd ofes the whans. anth ot on than tos on a war thir asderes this..s asd ontend and and tos othintite whestiters a to woss onte was thas wan a wass... ofe wore thas.. ot tastend ast of wand..sen asdinne tos.s on tarent ot this the and a tanse wo a meser an an therinnen ofed anth tos of tan a mes othes a tast a worsTrain Cycle: 200\n",
            "Average Loss Per Character: 2.367\n",
            "Train Cycle: 201\n",
            "Average Loss Per Character: 2.37\n",
            "Train Cycle: 202\n",
            "Average Loss Per Character: 2.351\n",
            "Train Cycle: 203\n",
            "Average Loss Per Character: 2.411\n",
            "Train Cycle: 204\n",
            "Average Loss Per Character: 2.351\n",
            "Train Cycle: 205\n",
            "Average Loss Per Character: 2.355\n",
            "Train Cycle: 206\n",
            "Average Loss Per Character: 2.343\n",
            "Train Cycle: 207\n",
            "Average Loss Per Character: 2.314\n",
            "Train Cycle: 208\n",
            "Average Loss Per Character: 2.36\n",
            "Train Cycle: 209\n",
            "Average Loss Per Character: 2.32\n",
            "Train Cycle: 210\n",
            "Average Loss Per Character: 2.364\n",
            "Train Cycle: 211\n",
            "Average Loss Per Character: 2.344\n",
            "Train Cycle: 212\n",
            "Average Loss Per Character: 2.371\n",
            "Train Cycle: 213\n",
            "Average Loss Per Character: 2.313\n",
            "Train Cycle: 214\n",
            "Average Loss Per Character: 2.329\n",
            "Train Cycle: 215\n",
            "Average Loss Per Character: 2.324\n",
            "Train Cycle: 216\n",
            "Average Loss Per Character: 2.313\n",
            "Train Cycle: 217\n",
            "Average Loss Per Character: 2.273\n",
            "Train Cycle: 218\n",
            "Average Loss Per Character: 2.292\n",
            "Train Cycle: 219\n",
            "Average Loss Per Character: 2.319\n",
            "Train Cycle: 220\n",
            "Average Loss Per Character: 2.257\n",
            "Train Cycle: 221\n",
            "Average Loss Per Character: 2.241\n",
            "Train Cycle: 222\n",
            "Average Loss Per Character: 2.349\n",
            "Train Cycle: 223\n",
            "Average Loss Per Character: 2.279\n",
            "Train Cycle: 224\n",
            "Average Loss Per Character: 2.318\n",
            "Chered and tos a was tan the thatt tis than movier tas wo move a tastin ot an tandereder oned on the tand. ande thin thas witerd one thes onte a wo motes onis a movies. an tans ones astis of was was the wistinten ote there to tar onest ote a mintitis thass. tar a worst tosse where wottintis of. ote the morinestinters and ther a milente as a tared than and tan ante to movian anting on tas tan than maviise tastitis othin ot this. toreryede than mavoe a thins as tare ant thess of as a wan mas tos oTrain Cycle: 225\n",
            "Average Loss Per Character: 2.3\n",
            "Train Cycle: 226\n",
            "Average Loss Per Character: 2.29\n",
            "Train Cycle: 227\n",
            "Average Loss Per Character: 2.365\n",
            "Train Cycle: 228\n",
            "Average Loss Per Character: 2.301\n",
            "Train Cycle: 229\n",
            "Average Loss Per Character: 2.3\n",
            "Train Cycle: 230\n",
            "Average Loss Per Character: 2.275\n",
            "Train Cycle: 231\n",
            "Average Loss Per Character: 2.338\n",
            "Train Cycle: 232\n",
            "Average Loss Per Character: 2.262\n",
            "Train Cycle: 233\n",
            "Average Loss Per Character: 2.369\n",
            "Train Cycle: 234\n",
            "Average Loss Per Character: 2.29\n",
            "Train Cycle: 235\n",
            "Average Loss Per Character: 2.233\n",
            "Train Cycle: 236\n",
            "Average Loss Per Character: 2.323\n",
            "Train Cycle: 237\n",
            "Average Loss Per Character: 2.25\n",
            "Train Cycle: 238\n",
            "Average Loss Per Character: 2.328\n",
            "Train Cycle: 239\n",
            "Average Loss Per Character: 2.264\n",
            "Train Cycle: 240\n",
            "Average Loss Per Character: 2.26\n",
            "Train Cycle: 241\n",
            "Average Loss Per Character: 2.25\n",
            "Train Cycle: 242\n",
            "Average Loss Per Character: 2.272\n",
            "Train Cycle: 243\n",
            "Average Loss Per Character: 2.296\n",
            "Train Cycle: 244\n",
            "Average Loss Per Character: 2.23\n",
            "Train Cycle: 245\n",
            "Average Loss Per Character: 2.288\n",
            "Train Cycle: 246\n",
            "Average Loss Per Character: 2.259\n",
            "Train Cycle: 247\n",
            "Average Loss Per Character: 2.246\n",
            "Train Cycle: 248\n",
            "Average Loss Per Character: 2.251\n",
            "Train Cycle: 249\n",
            "Average Loss Per Character: 2.272\n",
            "Whaste fanded tass.stiter tas that itter the saste ford.stere thiss. a to an onerend tores of the so the seer and a to tarlys.. to than so asde of. a tas thas a to anthers a thess of wan asdit oth tare so move an as to marly an thess on to a whats an tas an a thin of.s ot as a morlis tore sastered. to to mare. tostite tarlinge ant on oftis a marile. a whonse on to ast ast of was and. is salest oft an an on on a ther one thass than sothin as an to tost ot ass ot a morine tar on a wo to as tas is Train Cycle: 250\n",
            "Average Loss Per Character: 2.248\n",
            "Train Cycle: 251\n",
            "Average Loss Per Character: 2.283\n",
            "Train Cycle: 252\n",
            "Average Loss Per Character: 2.208\n",
            "Train Cycle: 253\n",
            "Average Loss Per Character: 2.234\n",
            "Train Cycle: 254\n",
            "Average Loss Per Character: 2.343\n",
            "Train Cycle: 255\n",
            "Average Loss Per Character: 2.287\n",
            "Train Cycle: 256\n",
            "Average Loss Per Character: 2.216\n",
            "Train Cycle: 257\n",
            "Average Loss Per Character: 2.264\n",
            "Train Cycle: 258\n",
            "Average Loss Per Character: 2.206\n",
            "Train Cycle: 259\n",
            "Average Loss Per Character: 2.193\n",
            "Train Cycle: 260\n",
            "Average Loss Per Character: 2.203\n",
            "Train Cycle: 261\n",
            "Average Loss Per Character: 2.208\n",
            "Train Cycle: 262\n",
            "Average Loss Per Character: 2.24\n",
            "Train Cycle: 263\n",
            "Average Loss Per Character: 2.195\n",
            "Train Cycle: 264\n",
            "Average Loss Per Character: 2.268\n",
            "Train Cycle: 265\n",
            "Average Loss Per Character: 2.214\n",
            "Train Cycle: 266\n",
            "Average Loss Per Character: 2.229\n",
            "Train Cycle: 267\n",
            "Average Loss Per Character: 2.28\n",
            "Train Cycle: 268\n",
            "Average Loss Per Character: 2.218\n",
            "Train Cycle: 269\n",
            "Average Loss Per Character: 2.242\n",
            "Train Cycle: 270\n",
            "Average Loss Per Character: 2.222\n",
            "Train Cycle: 271\n",
            "Average Loss Per Character: 2.337\n",
            "Train Cycle: 272\n",
            "Average Loss Per Character: 2.231\n",
            "Train Cycle: 273\n",
            "Average Loss Per Character: 2.215\n",
            "Train Cycle: 274\n",
            "Average Loss Per Character: 2.147\n",
            "Herill as a tare a marin tares antil of mother as therise.ss. to morile an tare tassin than is a man that. is a movaring. itss thit to a to a moviis its there.s thass taled. in tore.. and ass than in a movaad ote thats inst taldised itt than and ass of mis tor ones and.. iss. ins theress anterse thitt and inded.s an as as in astess..ystersse to thes as thit is seest thess. it a tarly is.stitinsed. itt a sarles ins a mille talde in a seed as and...rin oft ass ot ottistenstintise tostend..y antentTrain Cycle: 275\n",
            "Average Loss Per Character: 2.128\n",
            "Train Cycle: 276\n",
            "Average Loss Per Character: 2.267\n",
            "Train Cycle: 277\n",
            "Average Loss Per Character: 2.227\n",
            "Train Cycle: 278\n",
            "Average Loss Per Character: 2.208\n",
            "Train Cycle: 279\n",
            "Average Loss Per Character: 2.23\n",
            "Train Cycle: 280\n",
            "Average Loss Per Character: 2.256\n",
            "Train Cycle: 281\n",
            "Average Loss Per Character: 2.239\n",
            "Train Cycle: 282\n",
            "Average Loss Per Character: 2.155\n",
            "Train Cycle: 283\n",
            "Average Loss Per Character: 2.301\n",
            "Train Cycle: 284\n",
            "Average Loss Per Character: 2.182\n",
            "Train Cycle: 285\n",
            "Average Loss Per Character: 2.217\n",
            "Train Cycle: 286\n",
            "Average Loss Per Character: 2.208\n",
            "Train Cycle: 287\n",
            "Average Loss Per Character: 2.188\n",
            "Train Cycle: 288\n",
            "Average Loss Per Character: 2.129\n",
            "Train Cycle: 289\n",
            "Average Loss Per Character: 2.206\n",
            "Train Cycle: 290\n",
            "Average Loss Per Character: 2.151\n",
            "Train Cycle: 291\n",
            "Average Loss Per Character: 2.172\n",
            "Train Cycle: 292\n",
            "Average Loss Per Character: 2.143\n",
            "Train Cycle: 293\n",
            "Average Loss Per Character: 2.205\n",
            "Train Cycle: 294\n",
            "Average Loss Per Character: 2.165\n",
            "Train Cycle: 295\n",
            "Average Loss Per Character: 2.108\n",
            "Train Cycle: 296\n",
            "Average Loss Per Character: 2.15\n",
            "Train Cycle: 297\n",
            "Average Loss Per Character: 2.171\n",
            "Train Cycle: 298\n",
            "Average Loss Per Character: 2.179\n",
            "Train Cycle: 299\n",
            "Average Loss Per Character: 2.152\n",
            "mhissis othe wanserse a that a to wing othist tastite tast talested and as otters a thanse oth ot thas serest of taser a world tast thass tarse of.. ou to wisters of thery tore ant one an of a tous to to wond thans that shone on and a wasten ass than she wattintise this tast one. tas anded a tald that..y tor onish tar the an tald an of. ant thes assilled the thering tassis one.se on tos onish ot anders and of to thingerser otter as to thans and tore she wins othen thanseress talestins tar astiseTrain Cycle: 300\n",
            "Average Loss Per Character: 2.175\n",
            "Train Cycle: 301\n",
            "Average Loss Per Character: 2.177\n",
            "Train Cycle: 302\n",
            "Average Loss Per Character: 2.202\n",
            "Train Cycle: 303\n",
            "Average Loss Per Character: 2.18\n",
            "Train Cycle: 304\n",
            "Average Loss Per Character: 2.097\n",
            "Train Cycle: 305\n",
            "Average Loss Per Character: 2.13\n",
            "Train Cycle: 306\n",
            "Average Loss Per Character: 2.175\n",
            "Train Cycle: 307\n",
            "Average Loss Per Character: 2.214\n",
            "Train Cycle: 308\n",
            "Average Loss Per Character: 2.125\n",
            "Train Cycle: 309\n",
            "Average Loss Per Character: 2.198\n",
            "Train Cycle: 310\n",
            "Average Loss Per Character: 2.168\n",
            "Train Cycle: 311\n",
            "Average Loss Per Character: 2.143\n",
            "Train Cycle: 312\n",
            "Average Loss Per Character: 2.134\n",
            "Train Cycle: 313\n",
            "Average Loss Per Character: 2.085\n",
            "Train Cycle: 314\n",
            "Average Loss Per Character: 2.136\n",
            "Train Cycle: 315\n",
            "Average Loss Per Character: 2.121\n",
            "Train Cycle: 316\n",
            "Average Loss Per Character: 2.149\n",
            "Train Cycle: 317\n",
            "Average Loss Per Character: 2.181\n",
            "Train Cycle: 318\n",
            "Average Loss Per Character: 2.2\n",
            "Train Cycle: 319\n",
            "Average Loss Per Character: 2.102\n",
            "Train Cycle: 320\n",
            "Average Loss Per Character: 2.136\n",
            "Train Cycle: 321\n",
            "Average Loss Per Character: 2.201\n",
            "Train Cycle: 322\n",
            "Average Loss Per Character: 2.162\n",
            "Train Cycle: 323\n",
            "Average Loss Per Character: 2.133\n",
            "Train Cycle: 324\n",
            "Average Loss Per Character: 2.098\n",
            "ji andedented...ythere in itter talys toress it ass torine in a soritilital thitt ot tarle and anded iss of a the thasser of.y taritars.y itt as as a sto mase thingede it taressis an thats a series is is itte toressis a mand and. indengededertititans it a sorlentiog to tand thittister itt to seen and issertile. is ant oft a the to moviis is.y.y.rones a momine. is intonying..ying.y thesting ittent of and othensed anded into in ittersed a to the mine. its. a maress tanter to mis in there stont of Train Cycle: 325\n",
            "Average Loss Per Character: 2.092\n",
            "Train Cycle: 326\n",
            "Average Loss Per Character: 2.177\n",
            "Train Cycle: 327\n",
            "Average Loss Per Character: 2.137\n",
            "Train Cycle: 328\n",
            "Average Loss Per Character: 2.165\n",
            "Train Cycle: 329\n",
            "Average Loss Per Character: 2.214\n",
            "Train Cycle: 330\n",
            "Average Loss Per Character: 2.165\n",
            "Train Cycle: 331\n",
            "Average Loss Per Character: 2.168\n",
            "Train Cycle: 332\n",
            "Average Loss Per Character: 2.108\n",
            "Train Cycle: 333\n",
            "Average Loss Per Character: 2.118\n",
            "Train Cycle: 334\n",
            "Average Loss Per Character: 2.119\n",
            "Train Cycle: 335\n",
            "Average Loss Per Character: 2.087\n",
            "Train Cycle: 336\n",
            "Average Loss Per Character: 2.035\n",
            "Train Cycle: 337\n",
            "Average Loss Per Character: 2.18\n",
            "Train Cycle: 338\n",
            "Average Loss Per Character: 2.22\n",
            "Train Cycle: 339\n",
            "Average Loss Per Character: 2.164\n",
            "Train Cycle: 340\n",
            "Average Loss Per Character: 2.121\n",
            "Train Cycle: 341\n",
            "Average Loss Per Character: 2.115\n",
            "Train Cycle: 342\n",
            "Average Loss Per Character: 2.096\n",
            "Train Cycle: 343\n",
            "Average Loss Per Character: 2.131\n",
            "Train Cycle: 344\n",
            "Average Loss Per Character: 2.108\n",
            "Train Cycle: 345\n",
            "Average Loss Per Character: 2.118\n",
            "Train Cycle: 346\n",
            "Average Loss Per Character: 2.09\n",
            "Train Cycle: 347\n",
            "Average Loss Per Character: 2.085\n",
            "Train Cycle: 348\n",
            "Average Loss Per Character: 2.154\n",
            "Train Cycle: 349\n",
            "Average Loss Per Character: 2.137\n",
            "K or an onestith its this milly a this a shat and't.. onit of sto the torys and.ding of's it thes as a sharlyss oft a mingiting it mitheries.. ouster isseres a mast onineded asteds tony these and is is and'n of shoushe as the tand ist oftint ottinterised. it tory.y on therysse a sele this. inth one anthen thes an the mist talme it thing in tares of' and's it there shoringss...rout.ytony it as its. a motile ast tales indent as as and to sony intere ass an of thattititile anthous an this ant the sTrain Cycle: 350\n",
            "Average Loss Per Character: 2.153\n",
            "Train Cycle: 351\n",
            "Average Loss Per Character: 2.141\n",
            "Train Cycle: 352\n",
            "Average Loss Per Character: 2.039\n",
            "Train Cycle: 353\n",
            "Average Loss Per Character: 2.059\n",
            "Train Cycle: 354\n",
            "Average Loss Per Character: 2.06\n",
            "Train Cycle: 355\n",
            "Average Loss Per Character: 2.098\n",
            "Train Cycle: 356\n",
            "Average Loss Per Character: 2.043\n",
            "Train Cycle: 357\n",
            "Average Loss Per Character: 2.048\n",
            "Train Cycle: 358\n",
            "Average Loss Per Character: 2.056\n",
            "Train Cycle: 359\n",
            "Average Loss Per Character: 2.15\n",
            "Train Cycle: 360\n",
            "Average Loss Per Character: 2.112\n",
            "Train Cycle: 361\n",
            "Average Loss Per Character: 2.136\n",
            "Train Cycle: 362\n",
            "Average Loss Per Character: 2.093\n",
            "Train Cycle: 363\n",
            "Average Loss Per Character: 2.084\n",
            "Train Cycle: 364\n",
            "Average Loss Per Character: 2.076\n",
            "Train Cycle: 365\n",
            "Average Loss Per Character: 2.189\n",
            "Train Cycle: 366\n",
            "Average Loss Per Character: 2.145\n",
            "Train Cycle: 367\n",
            "Average Loss Per Character: 2.108\n",
            "Train Cycle: 368\n",
            "Average Loss Per Character: 2.092\n",
            "Train Cycle: 369\n",
            "Average Loss Per Character: 2.073\n",
            "Train Cycle: 370\n",
            "Average Loss Per Character: 2.12\n",
            "Train Cycle: 371\n",
            "Average Loss Per Character: 2.019\n",
            "Train Cycle: 372\n",
            "Average Loss Per Character: 2.044\n",
            "Train Cycle: 373\n",
            "Average Loss Per Character: 2.068\n",
            "Train Cycle: 374\n",
            "Average Loss Per Character: 2.062\n",
            "Kes the mothiteds a tan ant ound of' and. It't is the see tar orless.. thas a thas a tan of' there there...yited is. antered. and't thing is so wis orly...ytite as therientiof is on and'nd.y and'n out assoute assorint to seensed the missile in ottite tone istorysed and one is therout thit is the mile it onith to stare.d that.d and's inded.. onintantenter tandiof and oning atster of tory ant taled. a stirled. thery. I seotter a that tore these and tall a sto when the thit. tory. Intitile.. to seeTrain Cycle: 375\n",
            "Average Loss Per Character: 2.108\n",
            "Train Cycle: 376\n",
            "Average Loss Per Character: 2.051\n",
            "Train Cycle: 377\n",
            "Average Loss Per Character: 1.97\n",
            "Train Cycle: 378\n",
            "Average Loss Per Character: 2.052\n",
            "Train Cycle: 379\n",
            "Average Loss Per Character: 1.978\n",
            "Train Cycle: 380\n",
            "Average Loss Per Character: 2.052\n",
            "Train Cycle: 381\n",
            "Average Loss Per Character: 2.065\n",
            "Train Cycle: 382\n",
            "Average Loss Per Character: 2.117\n",
            "Train Cycle: 383\n",
            "Average Loss Per Character: 1.996\n",
            "Train Cycle: 384\n",
            "Average Loss Per Character: 2.145\n",
            "Train Cycle: 385\n",
            "Average Loss Per Character: 2.055\n",
            "Train Cycle: 386\n",
            "Average Loss Per Character: 2.041\n",
            "Train Cycle: 387\n",
            "Average Loss Per Character: 2.031\n",
            "Train Cycle: 388\n",
            "Average Loss Per Character: 2.007\n",
            "Train Cycle: 389\n",
            "Average Loss Per Character: 1.991\n",
            "Train Cycle: 390\n",
            "Average Loss Per Character: 1.975\n",
            "Train Cycle: 391\n",
            "Average Loss Per Character: 2.068\n",
            "Train Cycle: 392\n",
            "Average Loss Per Character: 2.002\n",
            "Train Cycle: 393\n",
            "Average Loss Per Character: 2.045\n",
            "Train Cycle: 394\n",
            "Average Loss Per Character: 2.07\n",
            "Train Cycle: 395\n",
            "Average Loss Per Character: 2.083\n",
            "Train Cycle: 396\n",
            "Average Loss Per Character: 2.04\n",
            "Train Cycle: 397\n",
            "Average Loss Per Character: 2.033\n",
            "Train Cycle: 398\n",
            "Average Loss Per Character: 2.002\n",
            "Train Cycle: 399\n",
            "Average Loss Per Character: 1.992\n",
            "Mil as ant as oustent onest one intely thits its' a seranted...rins tatteredsse and of' tale indeles of's't to mot thintint to moniter. these it tamering ast at this tare a witfe antented ant a was tallys on onine tathite inderthat one in the mot a wass a mindione to sorys oun a seens othint oundioged thitsse thit tole... ou haveryse therisint assed one and thes to mast on tar tale ast talme thindilled thit thind one anted in onings iss at that..rant ant of a wonteds oundit ands. a so a wontertiTrain Cycle: 400\n",
            "Average Loss Per Character: 2.062\n",
            "Train Cycle: 401\n",
            "Average Loss Per Character: 1.974\n",
            "Train Cycle: 402\n",
            "Average Loss Per Character: 2.053\n",
            "Train Cycle: 403\n",
            "Average Loss Per Character: 2.013\n",
            "Train Cycle: 404\n",
            "Average Loss Per Character: 2.027\n",
            "Train Cycle: 405\n",
            "Average Loss Per Character: 2.209\n",
            "Train Cycle: 406\n",
            "Average Loss Per Character: 2.048\n",
            "Train Cycle: 407\n",
            "Average Loss Per Character: 2.024\n",
            "Train Cycle: 408\n",
            "Average Loss Per Character: 2.217\n",
            "Train Cycle: 409\n",
            "Average Loss Per Character: 2.114\n",
            "Train Cycle: 410\n",
            "Average Loss Per Character: 2.007\n",
            "Train Cycle: 411\n",
            "Average Loss Per Character: 1.904\n",
            "Train Cycle: 412\n",
            "Average Loss Per Character: 2.002\n",
            "Train Cycle: 413\n",
            "Average Loss Per Character: 1.997\n",
            "Train Cycle: 414\n",
            "Average Loss Per Character: 2.037\n",
            "Train Cycle: 415\n",
            "Average Loss Per Character: 2.019\n",
            "Train Cycle: 416\n",
            "Average Loss Per Character: 2.041\n",
            "Train Cycle: 417\n",
            "Average Loss Per Character: 2.02\n",
            "Train Cycle: 418\n",
            "Average Loss Per Character: 2.014\n",
            "Train Cycle: 419\n",
            "Average Loss Per Character: 1.987\n",
            "Train Cycle: 420\n",
            "Average Loss Per Character: 1.973\n",
            "Train Cycle: 421\n",
            "Average Loss Per Character: 1.916\n",
            "Train Cycle: 422\n",
            "Average Loss Per Character: 2.027\n",
            "Train Cycle: 423\n",
            "Average Loss Per Character: 2.143\n",
            "Train Cycle: 424\n",
            "Average Loss Per Character: 1.974\n",
            "Other it madiass and a was and the missto tamestings in and all a wonting istanginatalles tores ofers tore. I some issong anthat of's. one.. assonting a storine a wout tange its assen thesine tangice tore. I torous anter on ous and and.ythe indingss anteng as a sone. ittontiof is torys and of a son out tandings. Its. thes to sory the thestine a talesse a seention thero anted.. onestens a watteng astite to tore. tandiof is momilie son oning. tat and an one in oustinatare ittare tang ous a the seaTrain Cycle: 425\n",
            "Average Loss Per Character: 1.944\n",
            "Train Cycle: 426\n",
            "Average Loss Per Character: 2.036\n",
            "Train Cycle: 427\n",
            "Average Loss Per Character: 1.998\n",
            "Train Cycle: 428\n",
            "Average Loss Per Character: 1.996\n",
            "Train Cycle: 429\n",
            "Average Loss Per Character: 1.993\n",
            "Train Cycle: 430\n",
            "Average Loss Per Character: 1.883\n",
            "Train Cycle: 431\n",
            "Average Loss Per Character: 1.961\n",
            "Train Cycle: 432\n",
            "Average Loss Per Character: 2.059\n",
            "Train Cycle: 433\n",
            "Average Loss Per Character: 2.015\n",
            "Train Cycle: 434\n",
            "Average Loss Per Character: 1.952\n",
            "Train Cycle: 435\n",
            "Average Loss Per Character: 1.981\n",
            "Train Cycle: 436\n",
            "Average Loss Per Character: 1.998\n",
            "Train Cycle: 437\n",
            "Average Loss Per Character: 2.061\n",
            "Train Cycle: 438\n",
            "Average Loss Per Character: 2.04\n",
            "Train Cycle: 439\n",
            "Average Loss Per Character: 1.957\n",
            "Train Cycle: 440\n",
            "Average Loss Per Character: 1.972\n",
            "Train Cycle: 441\n",
            "Average Loss Per Character: 2.015\n",
            "Train Cycle: 442\n",
            "Average Loss Per Character: 2.004\n",
            "Train Cycle: 443\n",
            "Average Loss Per Character: 1.944\n",
            "Train Cycle: 444\n",
            "Average Loss Per Character: 2.025\n",
            "Train Cycle: 445\n",
            "Average Loss Per Character: 1.929\n",
            "Train Cycle: 446\n",
            "Average Loss Per Character: 2.009\n",
            "Train Cycle: 447\n",
            "Average Loss Per Character: 1.952\n",
            "Train Cycle: 448\n",
            "Average Loss Per Character: 1.917\n",
            "Train Cycle: 449\n",
            "Average Loss Per Character: 1.979\n",
            "Merinasse in a seessersien a thind to sorys a there. thas.'ts out thasse tat it thasseds.y of' tores. tis a stull tame tantentery the sore a storest a wontion... It a werly.y on an a tand of sorites one. there.. on an of thasse out is all.y theser tore the selasing is. at is....rous to somy a seat of. that a tand antic and tamily. I thery and an oudly is momanys. ats an a therines one. tat and an a sereat a that. I westen in as a wattell talmate on onital therised an ands. on outtellentere an taTrain Cycle: 450\n",
            "Average Loss Per Character: 1.999\n",
            "Train Cycle: 451\n",
            "Average Loss Per Character: 1.946\n",
            "Train Cycle: 452\n",
            "Average Loss Per Character: 1.971\n",
            "Train Cycle: 453\n",
            "Average Loss Per Character: 1.903\n",
            "Train Cycle: 454\n",
            "Average Loss Per Character: 1.974\n",
            "Train Cycle: 455\n",
            "Average Loss Per Character: 1.949\n",
            "Train Cycle: 456\n",
            "Average Loss Per Character: 2.022\n",
            "Train Cycle: 457\n",
            "Average Loss Per Character: 1.956\n",
            "Train Cycle: 458\n",
            "Average Loss Per Character: 1.911\n",
            "Train Cycle: 459\n",
            "Average Loss Per Character: 1.937\n",
            "Train Cycle: 460\n",
            "Average Loss Per Character: 1.944\n",
            "Train Cycle: 461\n",
            "Average Loss Per Character: 1.997\n",
            "Train Cycle: 462\n",
            "Average Loss Per Character: 1.966\n",
            "Train Cycle: 463\n",
            "Average Loss Per Character: 1.966\n",
            "Train Cycle: 464\n",
            "Average Loss Per Character: 1.905\n",
            "Train Cycle: 465\n",
            "Average Loss Per Character: 1.863\n",
            "Train Cycle: 466\n",
            "Average Loss Per Character: 1.93\n",
            "Train Cycle: 467\n",
            "Average Loss Per Character: 1.886\n",
            "Train Cycle: 468\n",
            "Average Loss Per Character: 1.97\n",
            "Train Cycle: 469\n",
            "Average Loss Per Character: 1.987\n",
            "Train Cycle: 470\n",
            "Average Loss Per Character: 1.912\n",
            "Train Cycle: 471\n",
            "Average Loss Per Character: 1.968\n",
            "Train Cycle: 472\n",
            "Average Loss Per Character: 1.9\n",
            "Train Cycle: 473\n",
            "Average Loss Per Character: 1.987\n",
            "Train Cycle: 474\n",
            "Average Loss Per Character: 1.905\n",
            ":j. hansed. I samary. Inding to somie to mess thing and. an ofter all shone than it tounge tamas tame...ran alder a seeres. Itrouteds ald these this and.you word a worlly intorestined anlytaly and sorina an outhised to sore. To sar when it andserengen thas alotere anlenter and. Itrousess of this.. In oft ald anteng thiteres issedineds it a sees aldings an a serars a wathing tallite all a wishistic the monestionestinnes is'nes aldionesterester there an and alothatiof tarly. an and a so tand aloteTrain Cycle: 475\n",
            "Average Loss Per Character: 1.995\n",
            "Train Cycle: 476\n",
            "Average Loss Per Character: 2.036\n",
            "Train Cycle: 477\n",
            "Average Loss Per Character: 1.967\n",
            "Train Cycle: 478\n",
            "Average Loss Per Character: 2.034\n",
            "Train Cycle: 479\n",
            "Average Loss Per Character: 1.969\n",
            "Train Cycle: 480\n",
            "Average Loss Per Character: 1.914\n",
            "Train Cycle: 481\n",
            "Average Loss Per Character: 1.992\n",
            "Train Cycle: 482\n",
            "Average Loss Per Character: 1.855\n",
            "Train Cycle: 483\n",
            "Average Loss Per Character: 1.962\n",
            "Train Cycle: 484\n",
            "Average Loss Per Character: 1.962\n",
            "Train Cycle: 485\n",
            "Average Loss Per Character: 1.866\n",
            "Train Cycle: 486\n",
            "Average Loss Per Character: 1.954\n",
            "Train Cycle: 487\n",
            "Average Loss Per Character: 1.86\n",
            "Train Cycle: 488\n",
            "Average Loss Per Character: 1.943\n",
            "Train Cycle: 489\n",
            "Average Loss Per Character: 1.916\n",
            "Train Cycle: 490\n",
            "Average Loss Per Character: 1.945\n",
            "Train Cycle: 491\n",
            "Average Loss Per Character: 1.896\n",
            "Train Cycle: 492\n",
            "Average Loss Per Character: 1.951\n",
            "Train Cycle: 493\n",
            "Average Loss Per Character: 1.998\n",
            "Train Cycle: 494\n",
            "Average Loss Per Character: 1.895\n",
            "Train Cycle: 495\n",
            "Average Loss Per Character: 1.95\n",
            "Train Cycle: 496\n",
            "Average Loss Per Character: 1.974\n",
            "Train Cycle: 497\n",
            "Average Loss Per Character: 1.903\n",
            "Train Cycle: 498\n",
            "Average Loss Per Character: 1.88\n",
            "Train Cycle: 499\n",
            "Average Loss Per Character: 1.87\n",
            "nees alenses aller tatereds. att to tame is' out sondions of seo is a sto allers. To thery a stars ant an tony.'s' as and seo in antheric whiter and a wang and sto all tat alest thesese ones oniteds ton that me movie thes an of't a stan thit thint as a wout its.y of.'n tone itto a stant onesent tamessio a sonest.y. I starts is man thats aloned issed an a wincher thissedser. Tam ou sees on ton andsteds. Itt of a watt tounger in and and. timented..' ands as onic of' in of' intory a wingeds ones a Train Cycle: 500\n",
            "Average Loss Per Character: 1.89\n",
            "Train Cycle: 501\n",
            "Epoch 1\n",
            "Train Cycle: 1\n",
            "Average Loss Per Character: 2.306\n",
            "Train Cycle: 2\n",
            "Average Loss Per Character: 2.286\n",
            "Train Cycle: 3\n",
            "Average Loss Per Character: 2.23\n",
            "Train Cycle: 4\n",
            "Average Loss Per Character: 2.245\n",
            "Train Cycle: 5\n",
            "Average Loss Per Character: 2.22\n",
            "Train Cycle: 6\n",
            "Average Loss Per Character: 2.256\n",
            "Train Cycle: 7\n",
            "Average Loss Per Character: 2.256\n",
            "Train Cycle: 8\n",
            "Average Loss Per Character: 2.258\n",
            "Train Cycle: 9\n",
            "Average Loss Per Character: 2.154\n",
            "Train Cycle: 10\n",
            "Average Loss Per Character: 2.209\n",
            "Train Cycle: 11\n",
            "Average Loss Per Character: 2.153\n",
            "Train Cycle: 12\n",
            "Average Loss Per Character: 2.146\n",
            "Train Cycle: 13\n",
            "Average Loss Per Character: 2.195\n",
            "Train Cycle: 14\n",
            "Average Loss Per Character: 2.15\n",
            "Train Cycle: 15\n",
            "Average Loss Per Character: 2.171\n",
            "Train Cycle: 16\n",
            "Average Loss Per Character: 2.232\n",
            "Train Cycle: 17\n",
            "Average Loss Per Character: 2.099\n",
            "Train Cycle: 18\n",
            "Average Loss Per Character: 2.122\n",
            "Train Cycle: 19\n",
            "Average Loss Per Character: 2.166\n",
            "Train Cycle: 20\n",
            "Average Loss Per Character: 2.17\n",
            "Train Cycle: 21\n",
            "Average Loss Per Character: 2.162\n",
            "Train Cycle: 22\n",
            "Average Loss Per Character: 2.057\n",
            "Train Cycle: 23\n",
            "Average Loss Per Character: 2.15\n",
            "Train Cycle: 24\n",
            "Average Loss Per Character: 2.083\n",
            "Horsters.ystions an this moviiess a movias and.y I somare thit a wor a do the seor a wonden tony masted ous out tat thastile thery tattitares andsting tally ald to sory wattile to most an I was manes.y. /I sarters this a most its onings to sto mind thes almades. tony tores tam this mosk anden is almores and tore. I the to se the mase one.. In's' anly. In a soment an tattint taledsss. Itters.. Tand.. Taminde ton to mene oft thangile tone. I the mestined.y a that mivile..y I wangicel In I won and.Train Cycle: 25\n",
            "Average Loss Per Character: 1.995\n",
            "Train Cycle: 26\n",
            "Average Loss Per Character: 1.996\n",
            "Train Cycle: 27\n",
            "Average Loss Per Character: 2.003\n",
            "Train Cycle: 28\n",
            "Average Loss Per Character: 1.971\n",
            "Train Cycle: 29\n",
            "Average Loss Per Character: 1.951\n",
            "Train Cycle: 30\n",
            "Average Loss Per Character: 1.945\n",
            "Train Cycle: 31\n",
            "Average Loss Per Character: 1.929\n",
            "Train Cycle: 32\n",
            "Average Loss Per Character: 2.005\n",
            "Train Cycle: 33\n",
            "Average Loss Per Character: 1.907\n",
            "Train Cycle: 34\n",
            "Average Loss Per Character: 1.882\n",
            "Train Cycle: 35\n",
            "Average Loss Per Character: 1.991\n",
            "Train Cycle: 36\n",
            "Average Loss Per Character: 1.901\n",
            "Train Cycle: 37\n",
            "Average Loss Per Character: 1.922\n",
            "Train Cycle: 38\n",
            "Average Loss Per Character: 1.894\n",
            "Train Cycle: 39\n",
            "Average Loss Per Character: 1.951\n",
            "Train Cycle: 40\n",
            "Average Loss Per Character: 1.923\n",
            "Train Cycle: 41\n",
            "Average Loss Per Character: 1.989\n",
            "Train Cycle: 42\n",
            "Average Loss Per Character: 1.925\n",
            "Train Cycle: 43\n",
            "Average Loss Per Character: 1.992\n",
            "Train Cycle: 44\n",
            "Average Loss Per Character: 1.972\n",
            "Train Cycle: 45\n",
            "Average Loss Per Character: 1.936\n",
            "Train Cycle: 46\n",
            "Average Loss Per Character: 1.921\n",
            "Train Cycle: 47\n",
            "Average Loss Per Character: 1.875\n",
            "Train Cycle: 48\n",
            "Average Loss Per Character: 1.876\n",
            "Train Cycle: 49\n",
            "Average Loss Per Character: 1.858\n",
            "Qe his. It's mess one a want that.'t' selardion an anysed treed this.' hen and't. I stars tamis a wonteds. I hould.y Ittented.. Thiss is's the to the store. Treentiag of..'s' has moviis one. It a wathed that a sear and thit mine inderers on tale thanten that. Tom at' thint of that to the somere ous of tore..'t as's't. There.. I he some toud tory'ts's to mong and's.'t sear this. thesting.. It and sear and whissed. In I horle as'n was anytith of't sto wisede in was mes anytiteds that mand ou havesTrain Cycle: 50\n",
            "Average Loss Per Character: 1.892\n",
            "Train Cycle: 51\n",
            "Average Loss Per Character: 1.902\n",
            "Train Cycle: 52\n",
            "Average Loss Per Character: 1.926\n",
            "Train Cycle: 53\n",
            "Average Loss Per Character: 1.954\n",
            "Train Cycle: 54\n",
            "Average Loss Per Character: 1.923\n",
            "Train Cycle: 55\n",
            "Average Loss Per Character: 1.838\n",
            "Train Cycle: 56\n",
            "Average Loss Per Character: 1.922\n",
            "Train Cycle: 57\n",
            "Average Loss Per Character: 1.907\n",
            "Train Cycle: 58\n",
            "Average Loss Per Character: 1.809\n",
            "Train Cycle: 59\n",
            "Average Loss Per Character: 1.919\n",
            "Train Cycle: 60\n",
            "Average Loss Per Character: 1.882\n",
            "Train Cycle: 61\n",
            "Average Loss Per Character: 1.941\n",
            "Train Cycle: 62\n",
            "Average Loss Per Character: 1.885\n",
            "Train Cycle: 63\n",
            "Average Loss Per Character: 1.855\n",
            "Train Cycle: 64\n",
            "Average Loss Per Character: 1.881\n",
            "Train Cycle: 65\n",
            "Average Loss Per Character: 1.831\n",
            "Train Cycle: 66\n",
            "Average Loss Per Character: 1.937\n",
            "Train Cycle: 67\n",
            "Average Loss Per Character: 1.87\n",
            "Train Cycle: 68\n",
            "Average Loss Per Character: 1.936\n",
            "Train Cycle: 69\n",
            "Average Loss Per Character: 1.849\n",
            "Train Cycle: 70\n",
            "Average Loss Per Character: 1.924\n",
            "Train Cycle: 71\n",
            "Average Loss Per Character: 1.847\n",
            "Train Cycle: 72\n",
            "Average Loss Per Character: 1.829\n",
            "Train Cycle: 73\n",
            "Average Loss Per Character: 1.899\n",
            "Train Cycle: 74\n",
            "Average Loss Per Character: 1.893\n",
            "Xest an ore thesile andsen a trakasile thas most to seesting the so watcted an of hows touth ouseds a worderst triention these a seel one to sominalys. I stan intorous tamessic a winch one a stranding.....'Tthis movoone it that mistone of a triedssindion.... These on the seliems. of... Thinteng ould.y these tores a somore. these mone oftentinas antengen a worlman tamilys it a to tater of all. I with to movoundice a worder ouseds a sto stant thits that's that a wisting at tory thinks the selatoreTrain Cycle: 75\n",
            "Average Loss Per Character: 1.841\n",
            "Train Cycle: 76\n",
            "Average Loss Per Character: 1.8\n",
            "Train Cycle: 77\n",
            "Average Loss Per Character: 1.877\n",
            "Train Cycle: 78\n",
            "Average Loss Per Character: 1.917\n",
            "Train Cycle: 79\n",
            "Average Loss Per Character: 1.843\n",
            "Train Cycle: 80\n",
            "Average Loss Per Character: 1.852\n",
            "Train Cycle: 81\n",
            "Average Loss Per Character: 1.906\n",
            "Train Cycle: 82\n",
            "Average Loss Per Character: 1.883\n",
            "Train Cycle: 83\n",
            "Average Loss Per Character: 1.868\n",
            "Train Cycle: 84\n",
            "Average Loss Per Character: 1.805\n",
            "Train Cycle: 85\n",
            "Average Loss Per Character: 1.801\n",
            "Train Cycle: 86\n",
            "Average Loss Per Character: 1.934\n",
            "Train Cycle: 87\n",
            "Average Loss Per Character: 1.839\n",
            "Train Cycle: 88\n",
            "Average Loss Per Character: 1.848\n",
            "Train Cycle: 89\n",
            "Average Loss Per Character: 1.917\n",
            "Train Cycle: 90\n",
            "Average Loss Per Character: 1.892\n",
            "Train Cycle: 91\n",
            "Average Loss Per Character: 1.829\n",
            "Train Cycle: 92\n",
            "Average Loss Per Character: 1.882\n",
            "Train Cycle: 93\n",
            "Average Loss Per Character: 1.784\n",
            "Train Cycle: 94\n",
            "Average Loss Per Character: 1.91\n",
            "Train Cycle: 95\n",
            "Average Loss Per Character: 1.855\n",
            "Train Cycle: 96\n",
            "Average Loss Per Character: 1.836\n",
            "Train Cycle: 97\n",
            "Average Loss Per Character: 1.831\n",
            "Train Cycle: 98\n",
            "Average Loss Per Character: 1.826\n",
            "Train Cycle: 99\n",
            "Average Loss Per Character: 1.836\n",
            "oninaton. Trous tory. I watco and..<br Trom to mane anytouthens on of'ts's. />br I'vanders the streangitioll atton and so wintel this man a so whan istered.yrind this momane.. I'd's.' antal and anythedient that as a to the wistic these that thinked seen.. And of ald wist than things trines and.. I stull and. Thiss is mane a sone of a wast at to tamis whan a store.<Tthere trise thas.'s's. I head thas tale is mane... I't'nst out whats.'s a stone istorighs a stony to this.'t as this it a madion. ThTrain Cycle: 100\n",
            "Average Loss Per Character: 1.939\n",
            "Train Cycle: 101\n",
            "Average Loss Per Character: 1.873\n",
            "Train Cycle: 102\n",
            "Average Loss Per Character: 1.934\n",
            "Train Cycle: 103\n",
            "Average Loss Per Character: 1.763\n",
            "Train Cycle: 104\n",
            "Average Loss Per Character: 1.878\n",
            "Train Cycle: 105\n",
            "Average Loss Per Character: 1.773\n",
            "Train Cycle: 106\n",
            "Average Loss Per Character: 1.928\n",
            "Train Cycle: 107\n",
            "Average Loss Per Character: 1.885\n",
            "Train Cycle: 108\n",
            "Average Loss Per Character: 1.769\n",
            "Train Cycle: 109\n",
            "Average Loss Per Character: 1.906\n",
            "Train Cycle: 110\n",
            "Average Loss Per Character: 1.879\n",
            "Train Cycle: 111\n",
            "Average Loss Per Character: 1.844\n",
            "Train Cycle: 112\n",
            "Average Loss Per Character: 1.922\n",
            "Train Cycle: 113\n",
            "Average Loss Per Character: 1.756\n",
            "Train Cycle: 114\n",
            "Average Loss Per Character: 1.773\n",
            "Train Cycle: 115\n",
            "Average Loss Per Character: 1.86\n",
            "Train Cycle: 116\n",
            "Average Loss Per Character: 1.818\n",
            "Train Cycle: 117\n",
            "Average Loss Per Character: 1.839\n",
            "Train Cycle: 118\n",
            "Average Loss Per Character: 1.811\n",
            "Train Cycle: 119\n",
            "Average Loss Per Character: 1.853\n",
            "Train Cycle: 120\n",
            "Average Loss Per Character: 1.815\n",
            "Train Cycle: 121\n",
            "Average Loss Per Character: 1.823\n",
            "Train Cycle: 122\n",
            "Average Loss Per Character: 1.869\n",
            "Train Cycle: 123\n",
            "Average Loss Per Character: 1.823\n",
            "Train Cycle: 124\n",
            "Average Loss Per Character: 1.881\n",
            "nither an an a searl often then time thit a moves.'t as a story trathinterss. Tam thas.'s mostice thas. be therying as thas tourgood and'ts.'t astenged a story as to messers out only wastics. And orlyst on that thery.. A so wo see to touthen an indate trons thiss into talmind one a stalliougo a stant a stull than a mest toung trong a many tory. /bring and a son then..<br />>That any the movorinal indary thintic with as't.'t.' is thinkintintered intaly. be won this. I was tallicice trom as's.' hoTrain Cycle: 125\n",
            "Average Loss Per Character: 1.797\n",
            "Train Cycle: 126\n",
            "Average Loss Per Character: 1.799\n",
            "Train Cycle: 127\n",
            "Average Loss Per Character: 1.794\n",
            "Train Cycle: 128\n",
            "Average Loss Per Character: 1.735\n",
            "Train Cycle: 129\n",
            "Average Loss Per Character: 1.832\n",
            "Train Cycle: 130\n",
            "Average Loss Per Character: 1.773\n",
            "Train Cycle: 131\n",
            "Average Loss Per Character: 1.759\n",
            "Train Cycle: 132\n",
            "Average Loss Per Character: 1.803\n",
            "Train Cycle: 133\n",
            "Average Loss Per Character: 1.83\n",
            "Train Cycle: 134\n",
            "Average Loss Per Character: 1.867\n",
            "Train Cycle: 135\n",
            "Average Loss Per Character: 1.801\n",
            "Train Cycle: 136\n",
            "Average Loss Per Character: 1.758\n",
            "Train Cycle: 137\n",
            "Average Loss Per Character: 1.858\n",
            "Train Cycle: 138\n",
            "Average Loss Per Character: 1.851\n",
            "Train Cycle: 139\n",
            "Average Loss Per Character: 1.807\n",
            "Train Cycle: 140\n",
            "Average Loss Per Character: 1.848\n",
            "Train Cycle: 141\n",
            "Average Loss Per Character: 1.814\n",
            "Train Cycle: 142\n",
            "Average Loss Per Character: 1.816\n",
            "Train Cycle: 143\n",
            "Average Loss Per Character: 1.758\n",
            "Train Cycle: 144\n",
            "Average Loss Per Character: 1.805\n",
            "Train Cycle: 145\n",
            "Average Loss Per Character: 1.776\n",
            "Train Cycle: 146\n",
            "Average Loss Per Character: 1.77\n",
            "Train Cycle: 147\n",
            "Average Loss Per Character: 1.774\n",
            "Train Cycle: 148\n",
            "Average Loss Per Character: 1.768\n",
            "Train Cycle: 149\n",
            "Average Loss Per Character: 1.764\n",
            "<ne tat its' of the wisher a monies tat thit in a sean touter thated tounders to mosk almors. I horlds a sting triendentss. bids and a wattites is'n. It a me stone. That to tarsious this misten a worther thate a starstensing is mady and see it as this fors alman thits an all time is't.'t an to mones andstandit this filned in tath aloughs in to to mastion in wastere see thang thery this an a selmish thit is a selicanssion anysed. Toressed offile that's.. And' where. There a massed of'le thange itTrain Cycle: 150\n",
            "Average Loss Per Character: 1.834\n",
            "Train Cycle: 151\n",
            "Average Loss Per Character: 1.767\n",
            "Train Cycle: 152\n",
            "Average Loss Per Character: 1.746\n",
            "Train Cycle: 153\n",
            "Average Loss Per Character: 1.786\n",
            "Train Cycle: 154\n",
            "Average Loss Per Character: 1.839\n",
            "Train Cycle: 155\n",
            "Average Loss Per Character: 1.76\n",
            "Train Cycle: 156\n",
            "Average Loss Per Character: 1.758\n",
            "Train Cycle: 157\n",
            "Average Loss Per Character: 1.807\n",
            "Train Cycle: 158\n",
            "Average Loss Per Character: 1.748\n",
            "Train Cycle: 159\n",
            "Average Loss Per Character: 1.828\n",
            "Train Cycle: 160\n",
            "Average Loss Per Character: 1.797\n",
            "Train Cycle: 161\n",
            "Average Loss Per Character: 1.881\n",
            "Train Cycle: 162\n",
            "Average Loss Per Character: 1.83\n",
            "Train Cycle: 163\n",
            "Average Loss Per Character: 1.842\n",
            "Train Cycle: 164\n",
            "Average Loss Per Character: 1.772\n",
            "Train Cycle: 165\n",
            "Average Loss Per Character: 1.87\n",
            "Train Cycle: 166\n",
            "Average Loss Per Character: 1.758\n",
            "Train Cycle: 167\n",
            "Average Loss Per Character: 1.807\n",
            "Train Cycle: 168\n",
            "Average Loss Per Character: 1.767\n",
            "Train Cycle: 169\n",
            "Average Loss Per Character: 1.8\n",
            "Train Cycle: 170\n",
            "Average Loss Per Character: 1.848\n",
            "Train Cycle: 171\n",
            "Average Loss Per Character: 1.733\n",
            "Train Cycle: 172\n",
            "Average Loss Per Character: 1.752\n",
            "Train Cycle: 173\n",
            "Average Loss Per Character: 1.74\n",
            "Train Cycle: 174\n",
            "Average Loss Per Character: 1.751\n",
            "Filr that... This. Therien it't iss and anystatiom attevics a filnic to whisteres a winterss to samistion it witch tint to say.. It thit in tront almassed the mases the wite a stirn that almate outher..<I have a could it' sures.<I watctor tor a film. A see an itton.. A starst a some almed a seating.. Ittry woullds any then.'t all a film is wattitions. Then.<It'n's almeds this is manes and with. Aroment off and. The witell trousious offen in talice touth.<This is a some anyto want trake one trinaTrain Cycle: 175\n",
            "Average Loss Per Character: 1.8\n",
            "Train Cycle: 176\n",
            "Average Loss Per Character: 1.732\n",
            "Train Cycle: 177\n",
            "Average Loss Per Character: 1.713\n",
            "Train Cycle: 178\n",
            "Average Loss Per Character: 1.843\n",
            "Train Cycle: 179\n",
            "Average Loss Per Character: 1.799\n",
            "Train Cycle: 180\n",
            "Average Loss Per Character: 1.752\n",
            "Train Cycle: 181\n",
            "Average Loss Per Character: 1.725\n",
            "Train Cycle: 182\n",
            "Average Loss Per Character: 1.761\n",
            "Train Cycle: 183\n",
            "Average Loss Per Character: 1.672\n",
            "Train Cycle: 184\n",
            "Average Loss Per Character: 1.683\n",
            "Train Cycle: 185\n",
            "Average Loss Per Character: 1.722\n",
            "Train Cycle: 186\n",
            "Average Loss Per Character: 1.716\n",
            "Train Cycle: 187\n",
            "Average Loss Per Character: 1.809\n",
            "Train Cycle: 188\n",
            "Average Loss Per Character: 1.782\n",
            "Train Cycle: 189\n",
            "Average Loss Per Character: 1.827\n",
            "Train Cycle: 190\n",
            "Average Loss Per Character: 1.75\n",
            "Train Cycle: 191\n",
            "Average Loss Per Character: 1.686\n",
            "Train Cycle: 192\n",
            "Average Loss Per Character: 1.726\n",
            "Train Cycle: 193\n",
            "Average Loss Per Character: 1.773\n",
            "Train Cycle: 194\n",
            "Average Loss Per Character: 1.766\n",
            "Train Cycle: 195\n",
            "Average Loss Per Character: 1.766\n",
            "Train Cycle: 196\n",
            "Average Loss Per Character: 1.728\n",
            "Train Cycle: 197\n",
            "Average Loss Per Character: 1.907\n",
            "Train Cycle: 198\n",
            "Average Loss Per Character: 1.766\n",
            "Train Cycle: 199\n",
            "Average Loss Per Character: 1.857\n",
            "kesent of'l out that the mostinatond ous there trom thas tilmediof. This is that intating a watto trined offiention this in an is's won is therying a seemeds a wand of horly. Taster in's won then It'n trom interatout atten wanted..<It' worst the moves trate onl to so therings trount of a shat then this mend tronessice thas an thas in tout tory watching in't theristinal to theression it thas mon aldicar on itters a was to tathing. Thas. A sto tore woundions offel storys outh out tinted trougiticaTrain Cycle: 200\n",
            "Average Loss Per Character: 1.739\n",
            "Train Cycle: 201\n",
            "Average Loss Per Character: 1.787\n",
            "Train Cycle: 202\n",
            "Average Loss Per Character: 1.751\n",
            "Train Cycle: 203\n",
            "Average Loss Per Character: 1.833\n",
            "Train Cycle: 204\n",
            "Average Loss Per Character: 1.737\n",
            "Train Cycle: 205\n",
            "Average Loss Per Character: 1.759\n",
            "Train Cycle: 206\n",
            "Average Loss Per Character: 1.75\n",
            "Train Cycle: 207\n",
            "Average Loss Per Character: 1.752\n",
            "Train Cycle: 208\n",
            "Average Loss Per Character: 1.717\n",
            "Train Cycle: 209\n",
            "Average Loss Per Character: 1.697\n",
            "Train Cycle: 210\n",
            "Average Loss Per Character: 1.766\n",
            "Train Cycle: 211\n",
            "Average Loss Per Character: 1.755\n",
            "Train Cycle: 212\n",
            "Average Loss Per Character: 1.773\n",
            "Train Cycle: 213\n",
            "Average Loss Per Character: 1.736\n",
            "Train Cycle: 214\n",
            "Average Loss Per Character: 1.762\n",
            "Train Cycle: 215\n",
            "Average Loss Per Character: 1.787\n",
            "Train Cycle: 216\n",
            "Average Loss Per Character: 1.736\n",
            "Train Cycle: 217\n",
            "Average Loss Per Character: 1.687\n",
            "Train Cycle: 218\n",
            "Average Loss Per Character: 1.701\n",
            "Train Cycle: 219\n",
            "Average Loss Per Character: 1.698\n",
            "Train Cycle: 220\n",
            "Average Loss Per Character: 1.696\n",
            "Train Cycle: 221\n",
            "Average Loss Per Character: 1.669\n",
            "Train Cycle: 222\n",
            "Average Loss Per Character: 1.775\n",
            "Train Cycle: 223\n",
            "Average Loss Per Character: 1.754\n",
            "Train Cycle: 224\n",
            "Average Loss Per Character: 1.771\n",
            "Henericar in an to stind thinted. And thit.' winthing.. Thill to madie is thile and't...<br Thas. They me waste and this moviis and stony.<It where. Thisse one. A wong and tor to seing a strand a so madies. An I wont tame alling.<Tt an times this. A won in's's'le.. Tomigualles a so mes trakind at wich a four. Thas an It' muse at's to ston thit manes an is tame thilm to see. A would a me some anys and anyictint of had manes a seem they'n.<T he me a fone onle therind tround of movie..<The.. IttersTrain Cycle: 225\n",
            "Average Loss Per Character: 1.72\n",
            "Train Cycle: 226\n",
            "Average Loss Per Character: 1.734\n",
            "Train Cycle: 227\n",
            "Average Loss Per Character: 1.9\n",
            "Train Cycle: 228\n",
            "Average Loss Per Character: 1.715\n",
            "Train Cycle: 229\n",
            "Average Loss Per Character: 1.743\n",
            "Train Cycle: 230\n",
            "Average Loss Per Character: 1.733\n",
            "Train Cycle: 231\n",
            "Average Loss Per Character: 1.801\n",
            "Train Cycle: 232\n",
            "Average Loss Per Character: 1.723\n",
            "Train Cycle: 233\n",
            "Average Loss Per Character: 1.821\n",
            "Train Cycle: 234\n",
            "Average Loss Per Character: 1.744\n",
            "Train Cycle: 235\n",
            "Average Loss Per Character: 1.7\n",
            "Train Cycle: 236\n",
            "Average Loss Per Character: 1.777\n",
            "Train Cycle: 237\n",
            "Average Loss Per Character: 1.729\n",
            "Train Cycle: 238\n",
            "Average Loss Per Character: 1.765\n",
            "Train Cycle: 239\n",
            "Average Loss Per Character: 1.716\n",
            "Train Cycle: 240\n",
            "Average Loss Per Character: 1.716\n",
            "Train Cycle: 241\n",
            "Average Loss Per Character: 1.75\n",
            "Train Cycle: 242\n",
            "Average Loss Per Character: 1.791\n",
            "Train Cycle: 243\n",
            "Average Loss Per Character: 1.781\n",
            "Train Cycle: 244\n",
            "Average Loss Per Character: 1.713\n",
            "Train Cycle: 245\n",
            "Average Loss Per Character: 1.766\n",
            "Train Cycle: 246\n",
            "Average Loss Per Character: 1.748\n",
            "Train Cycle: 247\n",
            "Average Loss Per Character: 1.73\n",
            "Train Cycle: 248\n",
            "Average Loss Per Character: 1.772\n",
            "Train Cycle: 249\n",
            "Average Loss Per Character: 1.757\n",
            "Later to this moves a carterandiof is.'s any troud trong onlatousing...<bry />boys and a sene to so alman that the mades. A so an of the wast traked. Thile to the stoles and tory.<br. I've. And one. An this. A makind tor anyuntion on off a living to thing is that. Almass of a lot it'nse tranter when. I had.. Almasting. I her to make as' is a someng a sears aboogion a so almest and trient a sean the wills on times on timan to talker.<If and' wint at's.<b brecies at' then' a mean it tames triland Train Cycle: 250\n",
            "Average Loss Per Character: 1.718\n",
            "Train Cycle: 251\n",
            "Average Loss Per Character: 1.793\n",
            "Train Cycle: 252\n",
            "Average Loss Per Character: 1.674\n",
            "Train Cycle: 253\n",
            "Average Loss Per Character: 1.741\n",
            "Train Cycle: 254\n",
            "Average Loss Per Character: 1.855\n",
            "Train Cycle: 255\n",
            "Average Loss Per Character: 1.82\n",
            "Train Cycle: 256\n",
            "Average Loss Per Character: 1.717\n",
            "Train Cycle: 257\n",
            "Average Loss Per Character: 1.76\n",
            "Train Cycle: 258\n",
            "Average Loss Per Character: 1.704\n",
            "Train Cycle: 259\n",
            "Average Loss Per Character: 1.753\n",
            "Train Cycle: 260\n",
            "Average Loss Per Character: 1.746\n",
            "Train Cycle: 261\n",
            "Average Loss Per Character: 1.69\n",
            "Train Cycle: 262\n",
            "Average Loss Per Character: 1.805\n",
            "Train Cycle: 263\n",
            "Average Loss Per Character: 1.694\n",
            "Train Cycle: 264\n",
            "Average Loss Per Character: 1.834\n",
            "Train Cycle: 265\n",
            "Average Loss Per Character: 1.718\n",
            "Train Cycle: 266\n",
            "Average Loss Per Character: 1.763\n",
            "Train Cycle: 267\n",
            "Average Loss Per Character: 1.83\n",
            "Train Cycle: 268\n",
            "Average Loss Per Character: 1.738\n",
            "Train Cycle: 269\n",
            "Average Loss Per Character: 1.782\n",
            "Train Cycle: 270\n",
            "Average Loss Per Character: 1.714\n",
            "Train Cycle: 271\n",
            "Average Loss Per Character: 1.78\n",
            "Train Cycle: 272\n",
            "Average Loss Per Character: 1.768\n",
            "Train Cycle: 273\n",
            "Average Loss Per Character: 1.783\n",
            "Train Cycle: 274\n",
            "Average Loss Per Character: 1.665\n",
            "es out's alottan then.. This. A seciess..<bry This in is then. It watcterssinss offel onle this fild alouts offiould tor in tory.<I watteds titeryselly onlys it' an a fould it we any to some. Anyicallys all. There mast of. Ittensorige out'l of masted. Thas.'t we sears tilest onle of tingen therile all to moneatiallys..<I wan trace almonding to then.'s.'t he me thissedsed tinks. To me toukes tinglated and till any there a for. And a ston to mon an allseds of mestary is tores a seen thas moneat anTrain Cycle: 275\n",
            "Average Loss Per Character: 1.694\n",
            "Train Cycle: 276\n",
            "Average Loss Per Character: 1.837\n",
            "Train Cycle: 277\n",
            "Average Loss Per Character: 1.786\n",
            "Train Cycle: 278\n",
            "Average Loss Per Character: 1.742\n",
            "Train Cycle: 279\n",
            "Average Loss Per Character: 1.782\n",
            "Train Cycle: 280\n",
            "Average Loss Per Character: 1.779\n",
            "Train Cycle: 281\n",
            "Average Loss Per Character: 1.789\n",
            "Train Cycle: 282\n",
            "Average Loss Per Character: 1.679\n",
            "Train Cycle: 283\n",
            "Average Loss Per Character: 1.839\n",
            "Train Cycle: 284\n",
            "Average Loss Per Character: 1.731\n",
            "Train Cycle: 285\n",
            "Average Loss Per Character: 1.76\n",
            "Train Cycle: 286\n",
            "Average Loss Per Character: 1.752\n",
            "Train Cycle: 287\n",
            "Average Loss Per Character: 1.719\n",
            "Train Cycle: 288\n",
            "Average Loss Per Character: 1.672\n",
            "Train Cycle: 289\n",
            "Average Loss Per Character: 1.724\n",
            "Train Cycle: 290\n",
            "Average Loss Per Character: 1.679\n",
            "Train Cycle: 291\n",
            "Average Loss Per Character: 1.713\n",
            "Train Cycle: 292\n",
            "Average Loss Per Character: 1.692\n",
            "Train Cycle: 293\n",
            "Average Loss Per Character: 1.77\n",
            "Train Cycle: 294\n",
            "Average Loss Per Character: 1.755\n",
            "Train Cycle: 295\n",
            "Average Loss Per Character: 1.669\n",
            "Train Cycle: 296\n",
            "Average Loss Per Character: 1.729\n",
            "Train Cycle: 297\n",
            "Average Loss Per Character: 1.76\n",
            "Train Cycle: 298\n",
            "Average Loss Per Character: 1.755\n",
            "Train Cycle: 299\n",
            "Average Loss Per Character: 1.696\n",
            "cesicites in it when.'s all and. It's shaw abouk only. There onlicess. Aldio is. This moviie ands's thit wine to with.'t is's a sor oft thantes on a shart off till anyice and anythatich a wing till and.<T had secoms of a contertayessinted. Arpan a stind one and... Tas as'sicsing. I seen's' sell to so a charch off ther of the best onles an at a sherigar and see of horlessinest. I wanco to stirst.<br I say this move thas' it a courters.<The I'm a loves on the becent a sortiones thinke in its that.Train Cycle: 300\n",
            "Average Loss Per Character: 1.745\n",
            "Train Cycle: 301\n",
            "Average Loss Per Character: 1.733\n",
            "Train Cycle: 302\n",
            "Average Loss Per Character: 1.807\n",
            "Train Cycle: 303\n",
            "Average Loss Per Character: 1.725\n",
            "Train Cycle: 304\n",
            "Average Loss Per Character: 1.685\n",
            "Train Cycle: 305\n",
            "Average Loss Per Character: 1.742\n",
            "Train Cycle: 306\n",
            "Average Loss Per Character: 1.742\n",
            "Train Cycle: 307\n",
            "Average Loss Per Character: 1.774\n",
            "Train Cycle: 308\n",
            "Average Loss Per Character: 1.728\n",
            "Train Cycle: 309\n",
            "Average Loss Per Character: 1.768\n",
            "Train Cycle: 310\n",
            "Average Loss Per Character: 1.78\n",
            "Train Cycle: 311\n",
            "Average Loss Per Character: 1.76\n",
            "Train Cycle: 312\n",
            "Average Loss Per Character: 1.691\n",
            "Train Cycle: 313\n",
            "Average Loss Per Character: 1.634\n",
            "Train Cycle: 314\n",
            "Average Loss Per Character: 1.744\n",
            "Train Cycle: 315\n",
            "Average Loss Per Character: 1.727\n",
            "Train Cycle: 316\n",
            "Average Loss Per Character: 1.693\n",
            "Train Cycle: 317\n",
            "Average Loss Per Character: 1.719\n",
            "Train Cycle: 318\n",
            "Average Loss Per Character: 1.784\n",
            "Train Cycle: 319\n",
            "Average Loss Per Character: 1.726\n",
            "Train Cycle: 320\n",
            "Average Loss Per Character: 1.735\n",
            "Train Cycle: 321\n",
            "Average Loss Per Character: 1.763\n",
            "Train Cycle: 322\n",
            "Average Loss Per Character: 1.73\n",
            "Train Cycle: 323\n",
            "Average Loss Per Character: 1.756\n",
            "Train Cycle: 324\n",
            "Average Loss Per Character: 1.692\n",
            "xoliter. In's the madys.. Thery. I'vies.<blers originary a storiaging they a moviin it any the made out.<In. It wen in touthense one on a monely..<br.. A deal a foring.<b trist oulmsseds on and't a storie wistinas anytalys abausione... A men indelly a mundinaly than tince. I have a foor time astelliogeds oft they a ferine and.. Theye is.'t's far thill an and sondien thery a feree of. Itterner oused a somater they' fors out. Tomathicalles it as touguess.<b been. This issinas on the sorinat onle aTrain Cycle: 325\n",
            "Average Loss Per Character: 1.684\n",
            "Train Cycle: 326\n",
            "Average Loss Per Character: 1.756\n",
            "Train Cycle: 327\n",
            "Average Loss Per Character: 1.718\n",
            "Train Cycle: 328\n",
            "Average Loss Per Character: 1.761\n",
            "Train Cycle: 329\n",
            "Average Loss Per Character: 1.757\n",
            "Train Cycle: 330\n",
            "Average Loss Per Character: 1.783\n",
            "Train Cycle: 331\n",
            "Average Loss Per Character: 1.772\n",
            "Train Cycle: 332\n",
            "Average Loss Per Character: 1.701\n",
            "Train Cycle: 333\n",
            "Average Loss Per Character: 1.719\n",
            "Train Cycle: 334\n",
            "Average Loss Per Character: 1.682\n",
            "Train Cycle: 335\n",
            "Average Loss Per Character: 1.679\n",
            "Train Cycle: 336\n",
            "Average Loss Per Character: 1.648\n",
            "Train Cycle: 337\n",
            "Average Loss Per Character: 1.743\n",
            "Train Cycle: 338\n",
            "Average Loss Per Character: 1.816\n",
            "Train Cycle: 339\n",
            "Average Loss Per Character: 1.763\n",
            "Train Cycle: 340\n",
            "Average Loss Per Character: 1.697\n",
            "Train Cycle: 341\n",
            "Average Loss Per Character: 1.739\n",
            "Train Cycle: 342\n",
            "Average Loss Per Character: 1.699\n",
            "Train Cycle: 343\n",
            "Average Loss Per Character: 1.722\n",
            "Train Cycle: 344\n",
            "Average Loss Per Character: 1.734\n",
            "Train Cycle: 345\n",
            "Average Loss Per Character: 1.754\n",
            "Train Cycle: 346\n",
            "Average Loss Per Character: 1.696\n",
            "Train Cycle: 347\n",
            "Average Loss Per Character: 1.665\n",
            "Train Cycle: 348\n",
            "Average Loss Per Character: 1.742\n",
            "Train Cycle: 349\n",
            "Average Loss Per Character: 1.766\n",
            "<vrind with. I hadn in that alougo and's. A sautely an is tore.<br.<br.<Thile and. Tin astenter of allic thinders an an it wasse of to ton tingless. Tomathen tile a find it any wanded at's' seems trilal and's a stromeratess. That. I'd some a livicionstallys in ton's any wasten't sone of has mes is anyur the scar to that tiless aboused at't and' see a so an ittersterssinalys'....<bly titter from and the seen and thas move so an it and than at a madin wilm tinken watco iss and.<Ittrandigh as almotTrain Cycle: 350\n",
            "Average Loss Per Character: 1.773\n",
            "Train Cycle: 351\n",
            "Average Loss Per Character: 1.748\n",
            "Train Cycle: 352\n",
            "Average Loss Per Character: 1.627\n",
            "Train Cycle: 353\n",
            "Average Loss Per Character: 1.662\n",
            "Train Cycle: 354\n",
            "Average Loss Per Character: 1.67\n",
            "Train Cycle: 355\n",
            "Average Loss Per Character: 1.743\n",
            "Train Cycle: 356\n",
            "Average Loss Per Character: 1.658\n",
            "Train Cycle: 357\n",
            "Average Loss Per Character: 1.663\n",
            "Train Cycle: 358\n",
            "Average Loss Per Character: 1.686\n",
            "Train Cycle: 359\n",
            "Average Loss Per Character: 1.793\n",
            "Train Cycle: 360\n",
            "Average Loss Per Character: 1.795\n",
            "Train Cycle: 361\n",
            "Average Loss Per Character: 1.817\n",
            "Train Cycle: 362\n",
            "Average Loss Per Character: 1.712\n",
            "Train Cycle: 363\n",
            "Average Loss Per Character: 1.7\n",
            "Train Cycle: 364\n",
            "Average Loss Per Character: 1.698\n",
            "Train Cycle: 365\n",
            "Average Loss Per Character: 1.732\n",
            "Train Cycle: 366\n",
            "Average Loss Per Character: 1.761\n",
            "Train Cycle: 367\n",
            "Average Loss Per Character: 1.706\n",
            "Train Cycle: 368\n",
            "Average Loss Per Character: 1.73\n",
            "Train Cycle: 369\n",
            "Average Loss Per Character: 1.717\n",
            "Train Cycle: 370\n",
            "Average Loss Per Character: 1.727\n",
            "Train Cycle: 371\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchtext import datasets\n",
        "from torch import nn\n",
        "import random\n",
        "\n",
        "# Architecture hyperparams\n",
        "num_chars = 64\n",
        "LSTM_inp_sz = 256\n",
        "LSTM_hidden_sz = 512\n",
        "sample_topk = 3\n",
        "sample_freq = 50\n",
        "special_char = [\" \", \"<\", \">\", \"/\", \"\\\"\", \"\\'\", \":\", \";\", \".\", \"(\", \")\", \"!\"]\n",
        "\n",
        "# learning rate\n",
        "lr = 1e-3\n",
        "\n",
        "# Epochs to train model for (each epoch loops through the corpus once)\n",
        "epochs = 50\n",
        "\n",
        "# Number of characters to sample from model each testing cycle\n",
        "sample_length = 250\n",
        "\n",
        "# Convert character to index\n",
        "def char_index(x):\n",
        "  if ord(x) < 91 and ord(x) > 64:\n",
        "    return ord(x) - 65\n",
        "  if ord(x) < 123 and ord(x) > 96:\n",
        "    return ord(x) - 71\n",
        "  return special_char.index(x) + 52\n",
        "\n",
        "# Filter characters in input data for relevant characters\n",
        "def keep_char(x):\n",
        "  return (ord(x) < 91 and ord(x) > 64) or (ord(x) < 123 and ord(x) > 96) or x in special_char\n",
        "\n",
        "# Convert index to character\n",
        "def index_char(ind):\n",
        "  if ind < 26:\n",
        "    return chr(ind + 65)\n",
        "  if ind < 52:\n",
        "    return chr(ind + 71)\n",
        "  return special_char[ind - 52]\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self, inp_sz, hidden_sz):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embed = nn.Embedding(num_chars, inp_sz)\n",
        "\n",
        "    self.f = nn.Sequential(\n",
        "        nn.Linear(inp_sz + hidden_sz, hidden_sz),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "    self.i1 = nn.Sequential(\n",
        "        nn.Linear(inp_sz + hidden_sz, hidden_sz),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "    self.i2 = nn.Sequential(\n",
        "        nn.Linear(inp_sz + hidden_sz, hidden_sz),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "    self.o = nn.Sequential(\n",
        "        nn.Linear(inp_sz + hidden_sz, hidden_sz),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "    self.out = nn.Sequential(\n",
        "        nn.Linear(hidden_sz, num_chars),\n",
        "        nn.Softmax(dim = 0)\n",
        "    )\n",
        "\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "  def forward(self, inp, hidden, cell):\n",
        "    new_cell = cell\n",
        "\n",
        "    inp = self.dropout(self.embed(inp))\n",
        "    xh = torch.cat((inp, hidden))\n",
        "    new_cell = new_cell * self.f(xh)\n",
        "    new_cell = new_cell + (self.i1(xh) * self.i2(xh))\n",
        "    new_hidden = torch.tanh(new_cell) * self.o(xh)\n",
        "    new_out = self.out(new_hidden)\n",
        "\n",
        "    return new_out, new_hidden, new_cell\n",
        "\n",
        "class StackedLSTM(nn.Module):\n",
        "  def __init__(self, layer_num, inp_sz, hidden_sz):\n",
        "    super().__init__()\n",
        "\n",
        "\n",
        "# Training data (IMDB database in torchtext)\n",
        "training_dataloader = iter(datasets.IMDB(split=\"train\", root = \"data\"))\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "\n",
        "model = LSTM(LSTM_inp_sz, LSTM_hidden_sz).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Train model\n",
        "def training_cycle():\n",
        "  model.train()\n",
        "\n",
        "  train_cycle_num = 1\n",
        "\n",
        "  while True:\n",
        "    try:\n",
        "      print(f\"Train Cycle: {train_cycle_num}\")\n",
        "\n",
        "      train_cycle_num += 1\n",
        "      string = ''.join(filter(keep_char, next(training_dataloader)[1]))\n",
        "\n",
        "      hidden = torch.zeros(LSTM_hidden_sz).to(device)\n",
        "      cell = torch.zeros(LSTM_hidden_sz).to(device)\n",
        "      loss = 0\n",
        "\n",
        "      for i in range(0, len(string) - 1):\n",
        "        out, hidden, cell = model(torch.tensor(char_index(string[i])).to(device), hidden, cell)\n",
        "\n",
        "        # print(index_char(torch.argmax(out).item()), end = \"\")\n",
        "        loss += criterion(torch.log(out), torch.tensor(char_index(string[i+1])).to(device))\n",
        "\n",
        "      loss = loss / (len(string) - 1)\n",
        "\n",
        "      model.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "      opt.step()\n",
        "\n",
        "      print(f\"Loss Per Character: {loss}\")\n",
        "\n",
        "      if (train_cycle_num % sample_freq == 0):\n",
        "        test_cycle()\n",
        "\n",
        "    except StopIteration:\n",
        "      break\n",
        "\n",
        "# Sample from model to test progress every once in a while\n",
        "def test_cycle():\n",
        "  model.eval()\n",
        "  init_char = random.randint(0, num_chars - 1)\n",
        "\n",
        "  hidden = torch.zeros(LSTM_hidden_sz).to(device)\n",
        "  cell = torch.zeros(LSTM_hidden_sz).to(device)\n",
        "\n",
        "  for i in range(sample_length):\n",
        "    print(index_char(init_char), end = \"\")\n",
        "\n",
        "    out, hidden, cell = model(torch.tensor(init_char).to(device), hidden, cell)\n",
        "    top_chars = torch.topk(out, sample_topk)\n",
        "    init_char = top_chars[1][list(torch.utils.data.WeightedRandomSampler(nn.functional.softmax(top_chars[0], dim = 0), 1))[0]].item()\n",
        "\n",
        "# Training cycles\n",
        "for epoch in range(epochs):\n",
        "  print(f\"Epoch {epoch}\")\n",
        "  training_cycle()\n",
        "\n",
        "  test_cycle()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4CHNWSZHZWx-",
        "outputId": "347af200-42b2-4ccd-c1e9-e4bf07dd639c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Loss Per Character: 1.5031567811965942\n",
            "Train Cycle: 830\n",
            "Loss Per Character: 1.6725573539733887\n",
            "Train Cycle: 831\n",
            "Loss Per Character: 1.4136106967926025\n",
            "Train Cycle: 832\n",
            "Loss Per Character: 1.5947377681732178\n",
            "Train Cycle: 833\n",
            "Loss Per Character: 1.5626455545425415\n",
            "Train Cycle: 834\n",
            "Loss Per Character: 1.403511643409729\n",
            "Train Cycle: 835\n",
            "Loss Per Character: 1.4312916994094849\n",
            "Train Cycle: 836\n",
            "Loss Per Character: 1.583768367767334\n",
            "Train Cycle: 837\n",
            "Loss Per Character: 1.8262192010879517\n",
            "Train Cycle: 838\n",
            "Loss Per Character: 1.5295615196228027\n",
            "Train Cycle: 839\n",
            "Loss Per Character: 1.366904854774475\n",
            "Train Cycle: 840\n",
            "Loss Per Character: 1.5089192390441895\n",
            "Train Cycle: 841\n",
            "Loss Per Character: 1.4516977071762085\n",
            "Train Cycle: 842\n",
            "Loss Per Character: 1.4279417991638184\n",
            "Train Cycle: 843\n",
            "Loss Per Character: 1.792280912399292\n",
            "Train Cycle: 844\n",
            "Loss Per Character: 1.84310781955719\n",
            "Train Cycle: 845\n",
            "Loss Per Character: 1.5804636478424072\n",
            "Train Cycle: 846\n",
            "Loss Per Character: 1.8661516904830933\n",
            "Train Cycle: 847\n",
            "Loss Per Character: 1.8281053304672241\n",
            "Train Cycle: 848\n",
            "Loss Per Character: 1.636167287826538\n",
            "Train Cycle: 849\n",
            "Loss Per Character: 1.7019606828689575\n",
            ")' good by thank try ack too be the felt.< Wannist an a can and a crancious on a stantion offended. I' creaph asperes trately thirkill isnapling.. I wannerse isn't alm actior and somethey seen isnated or than accent a sere than in than a serest oriesTrain Cycle: 850\n",
            "Loss Per Character: 1.7429354190826416\n",
            "Train Cycle: 851\n",
            "Loss Per Character: 1.5785821676254272\n",
            "Train Cycle: 852\n",
            "Loss Per Character: 1.5803104639053345\n",
            "Train Cycle: 853\n",
            "Loss Per Character: 1.3959894180297852\n",
            "Train Cycle: 854\n",
            "Loss Per Character: 1.6916931867599487\n",
            "Train Cycle: 855\n",
            "Loss Per Character: 1.4341628551483154\n",
            "Train Cycle: 856\n",
            "Loss Per Character: 1.695676326751709\n",
            "Train Cycle: 857\n",
            "Loss Per Character: 1.582079291343689\n",
            "Train Cycle: 858\n",
            "Loss Per Character: 1.6142789125442505\n",
            "Train Cycle: 859\n",
            "Loss Per Character: 1.7539470195770264\n",
            "Train Cycle: 860\n",
            "Loss Per Character: 1.7373075485229492\n",
            "Train Cycle: 861\n",
            "Loss Per Character: 1.428221344947815\n",
            "Train Cycle: 862\n",
            "Loss Per Character: 1.5366992950439453\n",
            "Train Cycle: 863\n",
            "Loss Per Character: 1.4914696216583252\n",
            "Train Cycle: 864\n",
            "Loss Per Character: 1.6749694347381592\n",
            "Train Cycle: 865\n",
            "Loss Per Character: 1.4739001989364624\n",
            "Train Cycle: 866\n",
            "Loss Per Character: 1.6710320711135864\n",
            "Train Cycle: 867\n",
            "Loss Per Character: 1.4067860841751099\n",
            "Train Cycle: 868\n",
            "Loss Per Character: 1.8477956056594849\n",
            "Train Cycle: 869\n",
            "Loss Per Character: 1.7523478269577026\n",
            "Train Cycle: 870\n",
            "Loss Per Character: 1.6284418106079102\n",
            "Train Cycle: 871\n",
            "Loss Per Character: 1.4844387769699097\n",
            "Train Cycle: 872\n",
            "Loss Per Character: 1.6563301086425781\n",
            "Train Cycle: 873\n",
            "Loss Per Character: 1.5254727602005005\n",
            "Train Cycle: 874\n",
            "Loss Per Character: 1.6090706586837769\n",
            "Train Cycle: 875\n",
            "Loss Per Character: 1.4720289707183838\n",
            "Train Cycle: 876\n",
            "Loss Per Character: 1.5537664890289307\n",
            "Train Cycle: 877\n",
            "Loss Per Character: 1.3908120393753052\n",
            "Train Cycle: 878\n",
            "Loss Per Character: 1.5977146625518799\n",
            "Train Cycle: 879\n",
            "Loss Per Character: 1.8305796384811401\n",
            "Train Cycle: 880\n",
            "Loss Per Character: 1.494484305381775\n",
            "Train Cycle: 881\n",
            "Loss Per Character: 1.3159476518630981\n",
            "Train Cycle: 882\n",
            "Loss Per Character: 1.4534591436386108\n",
            "Train Cycle: 883\n",
            "Loss Per Character: 1.4592745304107666\n",
            "Train Cycle: 884\n",
            "Loss Per Character: 1.4976338148117065\n",
            "Train Cycle: 885\n",
            "Loss Per Character: 1.3834941387176514\n",
            "Train Cycle: 886\n",
            "Loss Per Character: 1.7004488706588745\n",
            "Train Cycle: 887\n",
            "Loss Per Character: 1.7020378112792969\n",
            "Train Cycle: 888\n",
            "Loss Per Character: 1.5534557104110718\n",
            "Train Cycle: 889\n",
            "Loss Per Character: 1.6486992835998535\n",
            "Train Cycle: 890\n",
            "Loss Per Character: 1.4929684400558472\n",
            "Train Cycle: 891\n",
            "Loss Per Character: 1.6990070343017578\n",
            "Train Cycle: 892\n",
            "Loss Per Character: 1.723708987236023\n",
            "Train Cycle: 893\n",
            "Loss Per Character: 1.7657734155654907\n",
            "Train Cycle: 894\n",
            "Loss Per Character: 1.8108527660369873\n",
            "Train Cycle: 895\n",
            "Loss Per Character: 1.5061930418014526\n",
            "Train Cycle: 896\n",
            "Loss Per Character: 1.475595474243164\n",
            "Train Cycle: 897\n",
            "Loss Per Character: 1.578650951385498\n",
            "Train Cycle: 898\n",
            "Loss Per Character: 1.721343994140625\n",
            "Train Cycle: 899\n",
            "Loss Per Character: 1.6515216827392578\n",
            "briss aren't butcrestic.. Antorand trused this.<br / /<br.<br /If yo he and. Tristious. And it it wanne to bad thankes trassibalitio wathorinessing. It's sturnite tor is trust as shows who a problems in to so a lack as a start toryle. Triestion and tTrain Cycle: 900\n",
            "Loss Per Character: 1.7207684516906738\n",
            "Train Cycle: 901\n",
            "Loss Per Character: 1.4824774265289307\n",
            "Train Cycle: 902\n",
            "Loss Per Character: 1.5854158401489258\n",
            "Train Cycle: 903\n",
            "Loss Per Character: 1.494767427444458\n",
            "Train Cycle: 904\n",
            "Loss Per Character: 1.4087550640106201\n",
            "Train Cycle: 905\n",
            "Loss Per Character: 1.5506972074508667\n",
            "Train Cycle: 906\n",
            "Loss Per Character: 1.3824669122695923\n",
            "Train Cycle: 907\n",
            "Loss Per Character: 1.542141318321228\n",
            "Train Cycle: 908\n",
            "Loss Per Character: 1.4179847240447998\n",
            "Train Cycle: 909\n",
            "Loss Per Character: 1.6353216171264648\n",
            "Train Cycle: 910\n",
            "Loss Per Character: 1.3737305402755737\n",
            "Train Cycle: 911\n",
            "Loss Per Character: 1.6805531978607178\n",
            "Train Cycle: 912\n",
            "Loss Per Character: 1.3638142347335815\n",
            "Train Cycle: 913\n",
            "Loss Per Character: 1.3757609128952026\n",
            "Train Cycle: 914\n",
            "Loss Per Character: 1.4926244020462036\n",
            "Train Cycle: 915\n",
            "Loss Per Character: 1.5495554208755493\n",
            "Train Cycle: 916\n",
            "Loss Per Character: 1.8357670307159424\n",
            "Train Cycle: 917\n",
            "Loss Per Character: 1.5697482824325562\n",
            "Train Cycle: 918\n",
            "Loss Per Character: 1.5311381816864014\n",
            "Train Cycle: 919\n",
            "Loss Per Character: 1.459090232849121\n",
            "Train Cycle: 920\n",
            "Loss Per Character: 1.51011323928833\n",
            "Train Cycle: 921\n",
            "Loss Per Character: 1.627467393875122\n",
            "Train Cycle: 922\n",
            "Loss Per Character: 1.5066642761230469\n",
            "Train Cycle: 923\n",
            "Loss Per Character: 1.637673020362854\n",
            "Train Cycle: 924\n",
            "Loss Per Character: 1.6188639402389526\n",
            "Train Cycle: 925\n",
            "Loss Per Character: 1.517968773841858\n",
            "Train Cycle: 926\n",
            "Loss Per Character: 1.3811702728271484\n",
            "Train Cycle: 927\n",
            "Loss Per Character: 1.5228245258331299\n",
            "Train Cycle: 928\n",
            "Loss Per Character: 1.5129238367080688\n",
            "Train Cycle: 929\n",
            "Loss Per Character: 1.6731318235397339\n",
            "Train Cycle: 930\n",
            "Loss Per Character: 1.3921465873718262\n",
            "Train Cycle: 931\n",
            "Loss Per Character: 1.397333025932312\n",
            "Train Cycle: 932\n",
            "Loss Per Character: 1.4438955783843994\n",
            "Train Cycle: 933\n",
            "Loss Per Character: 1.6762003898620605\n",
            "Train Cycle: 934\n",
            "Loss Per Character: 1.429383397102356\n",
            "Train Cycle: 935\n",
            "Loss Per Character: 1.5583925247192383\n",
            "Train Cycle: 936\n",
            "Loss Per Character: 1.6723814010620117\n",
            "Train Cycle: 937\n",
            "Loss Per Character: 1.6462044715881348\n",
            "Train Cycle: 938\n",
            "Loss Per Character: 1.7215360403060913\n",
            "Train Cycle: 939\n",
            "Loss Per Character: 1.7906512022018433\n",
            "Train Cycle: 940\n",
            "Loss Per Character: 1.408217191696167\n",
            "Train Cycle: 941\n",
            "Loss Per Character: 1.6484593152999878\n",
            "Train Cycle: 942\n",
            "Loss Per Character: 1.6553598642349243\n",
            "Train Cycle: 943\n",
            "Loss Per Character: 1.651427149772644\n",
            "Train Cycle: 944\n",
            "Loss Per Character: 1.494930386543274\n",
            "Train Cycle: 945\n",
            "Loss Per Character: 1.553902506828308\n",
            "Train Cycle: 946\n",
            "Loss Per Character: 1.3888438940048218\n",
            "Train Cycle: 947\n",
            "Loss Per Character: 1.7078150510787964\n",
            "Train Cycle: 948\n",
            "Loss Per Character: 1.4777215719223022\n",
            "Train Cycle: 949\n",
            "Loss Per Character: 1.4372546672821045\n",
            "qinates a minds on a longe out.<bot of hels off that' seen is they's. In tirl as the fill is too torge tried thir miss a land tryiless too trie things on think its a sturted it's an and stand to murrely with a secors a stant a few. An and..<rre a litTrain Cycle: 950\n",
            "Loss Per Character: 1.3660880327224731\n",
            "Train Cycle: 951\n",
            "Loss Per Character: 1.5478801727294922\n",
            "Train Cycle: 952\n",
            "Loss Per Character: 1.592054009437561\n",
            "Train Cycle: 953\n",
            "Loss Per Character: 1.4921575784683228\n",
            "Train Cycle: 954\n",
            "Loss Per Character: 1.4260482788085938\n",
            "Train Cycle: 955\n",
            "Loss Per Character: 1.5305843353271484\n",
            "Train Cycle: 956\n",
            "Loss Per Character: 1.709800124168396\n",
            "Train Cycle: 957\n",
            "Loss Per Character: 1.7226957082748413\n",
            "Train Cycle: 958\n",
            "Loss Per Character: 1.641663908958435\n",
            "Train Cycle: 959\n",
            "Loss Per Character: 1.8268498182296753\n",
            "Train Cycle: 960\n",
            "Loss Per Character: 1.5710370540618896\n",
            "Train Cycle: 961\n",
            "Loss Per Character: 1.6446771621704102\n",
            "Train Cycle: 962\n",
            "Loss Per Character: 1.4247004985809326\n",
            "Train Cycle: 963\n",
            "Loss Per Character: 1.554398775100708\n",
            "Train Cycle: 964\n",
            "Loss Per Character: 1.454856514930725\n",
            "Train Cycle: 965\n",
            "Loss Per Character: 1.6638691425323486\n",
            "Train Cycle: 966\n",
            "Loss Per Character: 1.7799025774002075\n",
            "Train Cycle: 967\n",
            "Loss Per Character: 1.5473287105560303\n",
            "Train Cycle: 968\n",
            "Loss Per Character: 1.687888503074646\n",
            "Train Cycle: 969\n",
            "Loss Per Character: 1.585529088973999\n",
            "Train Cycle: 970\n",
            "Loss Per Character: 1.5758053064346313\n",
            "Train Cycle: 971\n",
            "Loss Per Character: 1.5275415182113647\n",
            "Train Cycle: 972\n",
            "Loss Per Character: 1.4920035600662231\n",
            "Train Cycle: 973\n",
            "Loss Per Character: 1.8198378086090088\n",
            "Train Cycle: 974\n",
            "Loss Per Character: 1.4837993383407593\n",
            "Train Cycle: 975\n",
            "Loss Per Character: 1.7232455015182495\n",
            "Train Cycle: 976\n",
            "Loss Per Character: 1.5437853336334229\n",
            "Train Cycle: 977\n",
            "Loss Per Character: 1.818723440170288\n",
            "Train Cycle: 978\n",
            "Loss Per Character: 1.5637555122375488\n",
            "Train Cycle: 979\n",
            "Loss Per Character: 1.4076824188232422\n",
            "Train Cycle: 980\n",
            "Loss Per Character: 1.6276425123214722\n",
            "Train Cycle: 981\n",
            "Loss Per Character: 1.6291308403015137\n",
            "Train Cycle: 982\n",
            "Loss Per Character: 1.4751490354537964\n",
            "Train Cycle: 983\n",
            "Loss Per Character: 1.7418365478515625\n",
            "Train Cycle: 984\n",
            "Loss Per Character: 1.3867615461349487\n",
            "Train Cycle: 985\n",
            "Loss Per Character: 1.5560358762741089\n",
            "Train Cycle: 986\n",
            "Loss Per Character: 1.3056496381759644\n",
            "Train Cycle: 987\n",
            "Loss Per Character: 1.7093803882598877\n",
            "Train Cycle: 988\n",
            "Loss Per Character: 1.4691983461380005\n",
            "Train Cycle: 989\n",
            "Loss Per Character: 1.330801010131836\n",
            "Train Cycle: 990\n",
            "Loss Per Character: 1.5057677030563354\n",
            "Train Cycle: 991\n",
            "Loss Per Character: 1.3289204835891724\n",
            "Train Cycle: 992\n",
            "Loss Per Character: 1.3841307163238525\n",
            "Train Cycle: 993\n",
            "Loss Per Character: 1.667090892791748\n",
            "Train Cycle: 994\n",
            "Loss Per Character: 1.7178034782409668\n",
            "Train Cycle: 995\n",
            "Loss Per Character: 1.6328469514846802\n",
            "Train Cycle: 996\n",
            "Loss Per Character: 1.506723165512085\n",
            "Train Cycle: 997\n",
            "Loss Per Character: 1.7335625886917114\n",
            "Train Cycle: 998\n",
            "Loss Per Character: 1.542529821395874\n",
            "Train Cycle: 999\n",
            "Loss Per Character: 1.5394837856292725\n",
            "adie too musicated it a studicably shen' incrond to sames out then is shew a chind to tention of. A charancinal anyone with intilest abot any they. Toodien withe actina a like abounder.. There isnow thankits out tom and abarted to ser ounting abottesTrain Cycle: 1000\n",
            "Loss Per Character: 1.6062641143798828\n",
            "Train Cycle: 1001\n",
            "Loss Per Character: 1.530226469039917\n",
            "Train Cycle: 1002\n",
            "Loss Per Character: 1.8174420595169067\n",
            "Train Cycle: 1003\n",
            "Loss Per Character: 1.7942440509796143\n",
            "Train Cycle: 1004\n",
            "Loss Per Character: 1.7321735620498657\n",
            "Train Cycle: 1005\n",
            "Loss Per Character: 1.8694016933441162\n",
            "Train Cycle: 1006\n",
            "Loss Per Character: 1.4732223749160767\n",
            "Train Cycle: 1007\n",
            "Loss Per Character: 1.3868021965026855\n",
            "Train Cycle: 1008\n",
            "Loss Per Character: 1.550533652305603\n",
            "Train Cycle: 1009\n",
            "Loss Per Character: 1.2467142343521118\n",
            "Train Cycle: 1010\n",
            "Loss Per Character: 1.3198974132537842\n",
            "Train Cycle: 1011\n",
            "Loss Per Character: 1.5387531518936157\n",
            "Train Cycle: 1012\n",
            "Loss Per Character: 1.4029794931411743\n",
            "Train Cycle: 1013\n",
            "Loss Per Character: 1.5333784818649292\n",
            "Train Cycle: 1014\n",
            "Loss Per Character: 1.3845369815826416\n",
            "Train Cycle: 1015\n",
            "Loss Per Character: 1.7423930168151855\n",
            "Train Cycle: 1016\n",
            "Loss Per Character: 1.7559607028961182\n",
            "Train Cycle: 1017\n",
            "Loss Per Character: 1.754331350326538\n",
            "Train Cycle: 1018\n",
            "Loss Per Character: 1.4867279529571533\n",
            "Train Cycle: 1019\n",
            "Loss Per Character: 1.7204228639602661\n",
            "Train Cycle: 1020\n",
            "Loss Per Character: 1.5806411504745483\n",
            "Train Cycle: 1021\n",
            "Loss Per Character: 1.840593695640564\n",
            "Train Cycle: 1022\n",
            "Loss Per Character: 1.6587958335876465\n",
            "Train Cycle: 1023\n",
            "Loss Per Character: 1.9385851621627808\n",
            "Train Cycle: 1024\n",
            "Loss Per Character: 1.594467043876648\n",
            "Train Cycle: 1025\n",
            "Loss Per Character: 1.461685061454773\n",
            "Train Cycle: 1026\n",
            "Loss Per Character: 1.6575335264205933\n",
            "Train Cycle: 1027\n",
            "Loss Per Character: 1.7943195104599\n",
            "Train Cycle: 1028\n",
            "Loss Per Character: 1.8134516477584839\n",
            "Train Cycle: 1029\n",
            "Loss Per Character: 1.8781213760375977\n",
            "Train Cycle: 1030\n",
            "Loss Per Character: 1.6779553890228271\n",
            "Train Cycle: 1031\n",
            "Loss Per Character: 1.5397164821624756\n",
            "Train Cycle: 1032\n",
            "Loss Per Character: 1.682507038116455\n",
            "Train Cycle: 1033\n",
            "Loss Per Character: 1.8142386674880981\n",
            "Train Cycle: 1034\n",
            "Loss Per Character: 1.781362771987915\n",
            "Train Cycle: 1035\n",
            "Loss Per Character: 1.674608588218689\n",
            "Train Cycle: 1036\n",
            "Loss Per Character: 1.6536285877227783\n",
            "Train Cycle: 1037\n",
            "Loss Per Character: 1.541047215461731\n",
            "Train Cycle: 1038\n",
            "Loss Per Character: 1.4723647832870483\n",
            "Train Cycle: 1039\n",
            "Loss Per Character: 1.490755558013916\n",
            "Train Cycle: 1040\n",
            "Loss Per Character: 1.6777770519256592\n",
            "Train Cycle: 1041\n",
            "Loss Per Character: 1.6058518886566162\n",
            "Train Cycle: 1042\n",
            "Loss Per Character: 1.5114666223526\n",
            "Train Cycle: 1043\n",
            "Loss Per Character: 1.4943783283233643\n",
            "Train Cycle: 1044\n",
            "Loss Per Character: 1.7327982187271118\n",
            "Train Cycle: 1045\n",
            "Loss Per Character: 1.6607743501663208\n",
            "Train Cycle: 1046\n",
            "Loss Per Character: 1.597160816192627\n",
            "Train Cycle: 1047\n",
            "Loss Per Character: 1.4061120748519897\n",
            "Train Cycle: 1048\n",
            "Loss Per Character: 1.461045265197754\n",
            "Train Cycle: 1049\n",
            "Loss Per Character: 1.8082865476608276\n",
            "J.<rrandencined' anyon an also but it.. That thas assen tother. Too sour to bad. I chacker is toons.<out...... any to termallers. Tranching as sould an and any offentioge the canst more.<ro /ure astenting and... assest to stuller.<bor/<bl is toodentlTrain Cycle: 1050\n",
            "Loss Per Character: 1.3824831247329712\n",
            "Train Cycle: 1051\n",
            "Loss Per Character: 1.4141499996185303\n",
            "Train Cycle: 1052\n",
            "Loss Per Character: 1.641634225845337\n",
            "Train Cycle: 1053\n",
            "Loss Per Character: 1.3500193357467651\n",
            "Train Cycle: 1054\n",
            "Loss Per Character: 1.305367112159729\n",
            "Train Cycle: 1055\n",
            "Loss Per Character: 1.7443300485610962\n",
            "Train Cycle: 1056\n",
            "Loss Per Character: 1.6325311660766602\n",
            "Train Cycle: 1057\n",
            "Loss Per Character: 1.7083346843719482\n",
            "Train Cycle: 1058\n",
            "Loss Per Character: 1.6035399436950684\n",
            "Train Cycle: 1059\n",
            "Loss Per Character: 1.4221076965332031\n",
            "Train Cycle: 1060\n",
            "Loss Per Character: 1.655917763710022\n",
            "Train Cycle: 1061\n",
            "Loss Per Character: 1.385778784751892\n",
            "Train Cycle: 1062\n",
            "Loss Per Character: 1.3919665813446045\n",
            "Train Cycle: 1063\n",
            "Loss Per Character: 1.6991417407989502\n",
            "Train Cycle: 1064\n",
            "Loss Per Character: 1.5016837120056152\n",
            "Train Cycle: 1065\n",
            "Loss Per Character: 1.32300865650177\n",
            "Train Cycle: 1066\n",
            "Loss Per Character: 1.5151634216308594\n",
            "Train Cycle: 1067\n",
            "Loss Per Character: 1.6221165657043457\n",
            "Train Cycle: 1068\n",
            "Loss Per Character: 1.5828032493591309\n",
            "Train Cycle: 1069\n",
            "Loss Per Character: 1.4661985635757446\n",
            "Train Cycle: 1070\n",
            "Loss Per Character: 1.4644691944122314\n",
            "Train Cycle: 1071\n",
            "Loss Per Character: 1.4139171838760376\n",
            "Train Cycle: 1072\n",
            "Loss Per Character: 1.488889217376709\n",
            "Train Cycle: 1073\n",
            "Loss Per Character: 1.3505594730377197\n",
            "Train Cycle: 1074\n",
            "Loss Per Character: 1.643679141998291\n",
            "Train Cycle: 1075\n",
            "Loss Per Character: 1.4985847473144531\n",
            "Train Cycle: 1076\n",
            "Loss Per Character: 1.913724422454834\n",
            "Train Cycle: 1077\n",
            "Loss Per Character: 1.5455776453018188\n",
            "Train Cycle: 1078\n",
            "Loss Per Character: 1.3102085590362549\n",
            "Train Cycle: 1079\n",
            "Loss Per Character: 1.7389789819717407\n",
            "Train Cycle: 1080\n",
            "Loss Per Character: 1.5124459266662598\n",
            "Train Cycle: 1081\n",
            "Loss Per Character: 1.4853242635726929\n",
            "Train Cycle: 1082\n",
            "Loss Per Character: 1.6570594310760498\n",
            "Train Cycle: 1083\n",
            "Loss Per Character: 1.7610230445861816\n",
            "Train Cycle: 1084\n",
            "Loss Per Character: 1.8750789165496826\n",
            "Train Cycle: 1085\n",
            "Loss Per Character: 1.640725016593933\n",
            "Train Cycle: 1086\n",
            "Loss Per Character: 1.5738385915756226\n",
            "Train Cycle: 1087\n",
            "Loss Per Character: 1.4337749481201172\n",
            "Train Cycle: 1088\n",
            "Loss Per Character: 1.5825681686401367\n",
            "Train Cycle: 1089\n",
            "Loss Per Character: 1.2237352132797241\n",
            "Train Cycle: 1090\n",
            "Loss Per Character: 1.7509856224060059\n",
            "Train Cycle: 1091\n",
            "Loss Per Character: 1.3922700881958008\n",
            "Train Cycle: 1092\n",
            "Loss Per Character: 1.533820390701294\n",
            "Train Cycle: 1093\n",
            "Loss Per Character: 1.6399093866348267\n",
            "Train Cycle: 1094\n",
            "Loss Per Character: 1.4158152341842651\n",
            "Train Cycle: 1095\n",
            "Loss Per Character: 1.6926592588424683\n",
            "Train Cycle: 1096\n",
            "Loss Per Character: 1.4696866273880005\n",
            "Train Cycle: 1097\n",
            "Loss Per Character: 1.4757789373397827\n",
            "Train Cycle: 1098\n",
            "Loss Per Character: 1.6522164344787598\n",
            "Train Cycle: 1099\n",
            "Loss Per Character: 1.5146515369415283\n",
            "sto with a film. I condencer to thing..<br Winner is. I condly. I chorie instand a bad acked. I won'te accention of to sucher.... In'vines the moral too maki wasn's poinst on the form trangles ofte of hour asself. A can an is all a case tries. A can Train Cycle: 1100\n",
            "Loss Per Character: 1.3452847003936768\n",
            "Train Cycle: 1101\n",
            "Loss Per Character: 1.5302236080169678\n",
            "Train Cycle: 1102\n",
            "Loss Per Character: 1.4443738460540771\n",
            "Train Cycle: 1103\n",
            "Loss Per Character: 1.4129220247268677\n",
            "Train Cycle: 1104\n",
            "Loss Per Character: 1.1914852857589722\n",
            "Train Cycle: 1105\n",
            "Loss Per Character: 1.4861516952514648\n",
            "Train Cycle: 1106\n",
            "Loss Per Character: 1.6783885955810547\n",
            "Train Cycle: 1107\n",
            "Loss Per Character: 1.6485041379928589\n",
            "Train Cycle: 1108\n",
            "Loss Per Character: 1.5569090843200684\n",
            "Train Cycle: 1109\n",
            "Loss Per Character: 1.678688883781433\n",
            "Train Cycle: 1110\n",
            "Loss Per Character: 1.585699439048767\n",
            "Train Cycle: 1111\n",
            "Loss Per Character: 1.5095735788345337\n",
            "Train Cycle: 1112\n",
            "Loss Per Character: 1.27676522731781\n",
            "Train Cycle: 1113\n",
            "Loss Per Character: 1.5281823873519897\n",
            "Train Cycle: 1114\n",
            "Loss Per Character: 1.4663457870483398\n",
            "Train Cycle: 1115\n",
            "Loss Per Character: 1.691818356513977\n",
            "Train Cycle: 1116\n",
            "Loss Per Character: 1.5864866971969604\n",
            "Train Cycle: 1117\n",
            "Loss Per Character: 1.5832786560058594\n",
            "Train Cycle: 1118\n",
            "Loss Per Character: 1.664411187171936\n",
            "Train Cycle: 1119\n",
            "Loss Per Character: 1.4286798238754272\n",
            "Train Cycle: 1120\n",
            "Loss Per Character: 1.5783181190490723\n",
            "Train Cycle: 1121\n",
            "Loss Per Character: 1.4056298732757568\n",
            "Train Cycle: 1122\n",
            "Loss Per Character: 1.688306212425232\n",
            "Train Cycle: 1123\n",
            "Loss Per Character: 1.573535442352295\n",
            "Train Cycle: 1124\n",
            "Loss Per Character: 1.5198675394058228\n",
            "Train Cycle: 1125\n",
            "Loss Per Character: 1.8798397779464722\n",
            "Train Cycle: 1126\n",
            "Loss Per Character: 1.7978538274765015\n",
            "Train Cycle: 1127\n",
            "Loss Per Character: 1.679442286491394\n",
            "Train Cycle: 1128\n",
            "Loss Per Character: 1.6683294773101807\n",
            "Train Cycle: 1129\n",
            "Loss Per Character: 1.5421591997146606\n",
            "Train Cycle: 1130\n",
            "Loss Per Character: 1.5171581506729126\n",
            "Train Cycle: 1131\n",
            "Loss Per Character: 1.4966634511947632\n",
            "Train Cycle: 1132\n",
            "Loss Per Character: 1.6056909561157227\n",
            "Train Cycle: 1133\n",
            "Loss Per Character: 1.4362071752548218\n",
            "Train Cycle: 1134\n",
            "Loss Per Character: 1.5250575542449951\n",
            "Train Cycle: 1135\n",
            "Loss Per Character: 1.5193467140197754\n",
            "Train Cycle: 1136\n",
            "Loss Per Character: 1.3350119590759277\n",
            "Train Cycle: 1137\n",
            "Loss Per Character: 1.323865532875061\n",
            "Train Cycle: 1138\n",
            "Loss Per Character: 1.3866065740585327\n",
            "Train Cycle: 1139\n",
            "Loss Per Character: 1.3232992887496948\n",
            "Train Cycle: 1140\n",
            "Loss Per Character: 2.089200735092163\n",
            "Train Cycle: 1141\n",
            "Loss Per Character: 1.426872968673706\n",
            "Train Cycle: 1142\n",
            "Loss Per Character: 1.6711963415145874\n",
            "Train Cycle: 1143\n",
            "Loss Per Character: 1.618260145187378\n",
            "Train Cycle: 1144\n",
            "Loss Per Character: 1.6108325719833374\n",
            "Train Cycle: 1145\n",
            "Loss Per Character: 1.6643736362457275\n",
            "Train Cycle: 1146\n",
            "Loss Per Character: 1.5558080673217773\n",
            "Train Cycle: 1147\n",
            "Loss Per Character: 1.5535609722137451\n",
            "Train Cycle: 1148\n",
            "Loss Per Character: 1.624775767326355\n",
            "Train Cycle: 1149\n",
            "Loss Per Character: 1.4160727262496948\n",
            "Ye all to he does not well backsor thin assomancies that interoody timilary\"). That' way is they an it it' wasn' ten was a surpons this. Timb and haves. Shey. It. To be an or tilds this fine.<rres and. That too mostence thas. Thises is any onle of teTrain Cycle: 1150\n",
            "Loss Per Character: 1.4880632162094116\n",
            "Train Cycle: 1151\n",
            "Loss Per Character: 1.402475357055664\n",
            "Train Cycle: 1152\n",
            "Loss Per Character: 1.571343183517456\n",
            "Train Cycle: 1153\n",
            "Loss Per Character: 1.584524154663086\n",
            "Train Cycle: 1154\n",
            "Loss Per Character: 1.501088261604309\n",
            "Train Cycle: 1155\n",
            "Loss Per Character: 1.3237024545669556\n",
            "Train Cycle: 1156\n",
            "Loss Per Character: 1.4499458074569702\n",
            "Train Cycle: 1157\n",
            "Loss Per Character: 1.4831537008285522\n",
            "Train Cycle: 1158\n",
            "Loss Per Character: 1.666127324104309\n",
            "Train Cycle: 1159\n",
            "Loss Per Character: 1.3921902179718018\n",
            "Train Cycle: 1160\n",
            "Loss Per Character: 1.4878299236297607\n",
            "Train Cycle: 1161\n",
            "Loss Per Character: 1.5100420713424683\n",
            "Train Cycle: 1162\n",
            "Loss Per Character: 1.6191177368164062\n",
            "Train Cycle: 1163\n",
            "Loss Per Character: 1.6864726543426514\n",
            "Train Cycle: 1164\n",
            "Loss Per Character: 1.5948535203933716\n",
            "Train Cycle: 1165\n",
            "Loss Per Character: 1.4436497688293457\n",
            "Train Cycle: 1166\n",
            "Loss Per Character: 1.690080165863037\n",
            "Train Cycle: 1167\n",
            "Loss Per Character: 1.6555603742599487\n",
            "Train Cycle: 1168\n",
            "Loss Per Character: 1.3822277784347534\n",
            "Train Cycle: 1169\n",
            "Loss Per Character: 1.604132890701294\n",
            "Train Cycle: 1170\n",
            "Loss Per Character: 1.2852568626403809\n",
            "Train Cycle: 1171\n",
            "Loss Per Character: 1.3675318956375122\n",
            "Train Cycle: 1172\n",
            "Loss Per Character: 1.8889141082763672\n",
            "Train Cycle: 1173\n",
            "Loss Per Character: 1.4079982042312622\n",
            "Train Cycle: 1174\n",
            "Loss Per Character: 1.8568447828292847\n",
            "Train Cycle: 1175\n",
            "Loss Per Character: 1.6072912216186523\n",
            "Train Cycle: 1176\n",
            "Loss Per Character: 1.6183286905288696\n",
            "Train Cycle: 1177\n",
            "Loss Per Character: 1.1491260528564453\n",
            "Train Cycle: 1178\n",
            "Loss Per Character: 1.3717515468597412\n",
            "Train Cycle: 1179\n",
            "Loss Per Character: 1.3975255489349365\n",
            "Train Cycle: 1180\n",
            "Loss Per Character: 1.6184498071670532\n",
            "Train Cycle: 1181\n",
            "Loss Per Character: 1.4315307140350342\n",
            "Train Cycle: 1182\n",
            "Loss Per Character: 1.4161653518676758\n",
            "Train Cycle: 1183\n",
            "Loss Per Character: 1.680567741394043\n",
            "Train Cycle: 1184\n",
            "Loss Per Character: 1.3750981092453003\n",
            "Train Cycle: 1185\n",
            "Loss Per Character: 1.5148404836654663\n",
            "Train Cycle: 1186\n",
            "Loss Per Character: 1.753597617149353\n",
            "Train Cycle: 1187\n",
            "Loss Per Character: 1.5343222618103027\n",
            "Train Cycle: 1188\n",
            "Loss Per Character: 1.3620842695236206\n",
            "Train Cycle: 1189\n",
            "Loss Per Character: 1.4549719095230103\n",
            "Train Cycle: 1190\n",
            "Loss Per Character: 1.3620436191558838\n",
            "Train Cycle: 1191\n",
            "Loss Per Character: 1.3341467380523682\n",
            "Train Cycle: 1192\n",
            "Loss Per Character: 1.3735452890396118\n",
            "Train Cycle: 1193\n",
            "Loss Per Character: 1.6284743547439575\n",
            "Train Cycle: 1194\n",
            "Loss Per Character: 1.3687294721603394\n",
            "Train Cycle: 1195\n",
            "Loss Per Character: 1.578366756439209\n",
            "Train Cycle: 1196\n",
            "Loss Per Character: 1.5573878288269043\n",
            "Train Cycle: 1197\n",
            "Loss Per Character: 1.2310537099838257\n",
            "Train Cycle: 1198\n",
            "Loss Per Character: 1.6220436096191406\n",
            "Train Cycle: 1199\n",
            "Loss Per Character: 1.2811779975891113\n",
            "Afther.. to trate the static for they also tryical is think intelemested!<br..<br/<or.<br.<or />I tryic. Watch they was tries tit to seq a good... that. It sured.<but than it was totatons on a far andoy to..<br/<bo and if years to.<rignating and. WhiTrain Cycle: 1200\n",
            "Loss Per Character: 1.5471477508544922\n",
            "Train Cycle: 1201\n",
            "Loss Per Character: 1.8117421865463257\n",
            "Train Cycle: 1202\n",
            "Loss Per Character: 1.6261850595474243\n",
            "Train Cycle: 1203\n",
            "Loss Per Character: 1.6055026054382324\n",
            "Train Cycle: 1204\n",
            "Loss Per Character: 1.5518299341201782\n",
            "Train Cycle: 1205\n",
            "Loss Per Character: 1.4326114654541016\n",
            "Train Cycle: 1206\n",
            "Loss Per Character: 1.5428075790405273\n",
            "Train Cycle: 1207\n",
            "Loss Per Character: 1.4813891649246216\n",
            "Train Cycle: 1208\n",
            "Loss Per Character: 1.4989773035049438\n",
            "Train Cycle: 1209\n",
            "Loss Per Character: 1.4830586910247803\n",
            "Train Cycle: 1210\n",
            "Loss Per Character: 1.3793431520462036\n",
            "Train Cycle: 1211\n",
            "Loss Per Character: 1.5514262914657593\n",
            "Train Cycle: 1212\n",
            "Loss Per Character: 1.5761256217956543\n",
            "Train Cycle: 1213\n",
            "Loss Per Character: 1.6124802827835083\n",
            "Train Cycle: 1214\n",
            "Loss Per Character: 1.6083462238311768\n",
            "Train Cycle: 1215\n",
            "Loss Per Character: 1.613315224647522\n",
            "Train Cycle: 1216\n",
            "Loss Per Character: 1.3586314916610718\n",
            "Train Cycle: 1217\n",
            "Loss Per Character: 1.5610134601593018\n",
            "Train Cycle: 1218\n",
            "Loss Per Character: 1.5108089447021484\n",
            "Train Cycle: 1219\n",
            "Loss Per Character: 1.342095971107483\n",
            "Train Cycle: 1220\n",
            "Loss Per Character: 1.2836447954177856\n",
            "Train Cycle: 1221\n",
            "Loss Per Character: 1.4902971982955933\n",
            "Train Cycle: 1222\n",
            "Loss Per Character: 1.5526723861694336\n",
            "Train Cycle: 1223\n",
            "Loss Per Character: 1.7493407726287842\n",
            "Train Cycle: 1224\n",
            "Loss Per Character: 1.3790639638900757\n",
            "Train Cycle: 1225\n",
            "Loss Per Character: 1.629979133605957\n",
            "Train Cycle: 1226\n",
            "Loss Per Character: 1.4700267314910889\n",
            "Train Cycle: 1227\n",
            "Loss Per Character: 1.6383872032165527\n",
            "Train Cycle: 1228\n",
            "Loss Per Character: 1.5178489685058594\n",
            "Train Cycle: 1229\n",
            "Loss Per Character: 1.7734131813049316\n",
            "Train Cycle: 1230\n",
            "Loss Per Character: 1.5975528955459595\n",
            "Train Cycle: 1231\n",
            "Loss Per Character: 1.6759974956512451\n",
            "Train Cycle: 1232\n",
            "Loss Per Character: 2.0027105808258057\n",
            "Train Cycle: 1233\n",
            "Loss Per Character: 1.3524459600448608\n",
            "Train Cycle: 1234\n",
            "Loss Per Character: 1.6316304206848145\n",
            "Train Cycle: 1235\n",
            "Loss Per Character: 1.470487117767334\n",
            "Train Cycle: 1236\n",
            "Loss Per Character: 1.5365982055664062\n",
            "Train Cycle: 1237\n",
            "Loss Per Character: 1.378287434577942\n",
            "Train Cycle: 1238\n",
            "Loss Per Character: 1.23324453830719\n",
            "Train Cycle: 1239\n",
            "Loss Per Character: 1.369480848312378\n",
            "Train Cycle: 1240\n",
            "Loss Per Character: 1.5745997428894043\n",
            "Train Cycle: 1241\n",
            "Loss Per Character: 1.314075231552124\n",
            "Train Cycle: 1242\n",
            "Loss Per Character: 1.6501474380493164\n",
            "Train Cycle: 1243\n",
            "Loss Per Character: 1.7472491264343262\n",
            "Train Cycle: 1244\n",
            "Loss Per Character: 1.4728946685791016\n",
            "Train Cycle: 1245\n",
            "Loss Per Character: 1.612609624862671\n",
            "Train Cycle: 1246\n",
            "Loss Per Character: 1.5202311277389526\n",
            "Train Cycle: 1247\n",
            "Loss Per Character: 1.5667098760604858\n",
            "Train Cycle: 1248\n",
            "Loss Per Character: 1.3238239288330078\n",
            "Train Cycle: 1249\n",
            "Loss Per Character: 1.5165519714355469\n",
            "brathow..<br Sandon's charance an itself..<br.. that wortunes....<br Seen a bad. And setive this movie well. Altoon an imedy's. A welk out.. a first. A worse. Anyon things to third.<br So doesngeringly)'s. In seen.<br />The predicusing and they so trTrain Cycle: 1250\n",
            "Loss Per Character: 1.4115610122680664\n",
            "Train Cycle: 1251\n",
            "Loss Per Character: 1.5649994611740112\n",
            "Train Cycle: 1252\n",
            "Loss Per Character: 1.6570217609405518\n",
            "Train Cycle: 1253\n",
            "Loss Per Character: 1.6919499635696411\n",
            "Train Cycle: 1254\n",
            "Loss Per Character: 1.5930147171020508\n",
            "Train Cycle: 1255\n",
            "Loss Per Character: 1.5509971380233765\n",
            "Train Cycle: 1256\n",
            "Loss Per Character: 1.5688114166259766\n",
            "Train Cycle: 1257\n",
            "Loss Per Character: 1.5050747394561768\n",
            "Train Cycle: 1258\n",
            "Loss Per Character: 1.583614706993103\n",
            "Train Cycle: 1259\n",
            "Loss Per Character: 1.4238873720169067\n",
            "Train Cycle: 1260\n",
            "Loss Per Character: 1.5504506826400757\n",
            "Train Cycle: 1261\n",
            "Loss Per Character: 1.6000995635986328\n",
            "Train Cycle: 1262\n",
            "Loss Per Character: 1.6718522310256958\n",
            "Train Cycle: 1263\n",
            "Loss Per Character: 1.638759732246399\n",
            "Train Cycle: 1264\n",
            "Loss Per Character: 1.5405025482177734\n",
            "Train Cycle: 1265\n",
            "Loss Per Character: 1.4008550643920898\n",
            "Train Cycle: 1266\n",
            "Loss Per Character: 1.4864040613174438\n",
            "Train Cycle: 1267\n",
            "Loss Per Character: 1.4392441511154175\n",
            "Train Cycle: 1268\n",
            "Loss Per Character: 1.536451816558838\n",
            "Train Cycle: 1269\n",
            "Loss Per Character: 1.684993028640747\n",
            "Train Cycle: 1270\n",
            "Loss Per Character: 1.4807403087615967\n",
            "Train Cycle: 1271\n",
            "Loss Per Character: 1.4766391515731812\n",
            "Train Cycle: 1272\n",
            "Loss Per Character: 1.4459751844406128\n",
            "Train Cycle: 1273\n",
            "Loss Per Character: 1.368047833442688\n",
            "Train Cycle: 1274\n",
            "Loss Per Character: 1.3786615133285522\n",
            "Train Cycle: 1275\n",
            "Loss Per Character: 1.4891183376312256\n",
            "Train Cycle: 1276\n",
            "Loss Per Character: 1.4180917739868164\n",
            "Train Cycle: 1277\n",
            "Loss Per Character: 1.59613037109375\n",
            "Train Cycle: 1278\n",
            "Loss Per Character: 1.674755573272705\n",
            "Train Cycle: 1279\n",
            "Loss Per Character: 1.5345311164855957\n",
            "Train Cycle: 1280\n",
            "Loss Per Character: 1.3387359380722046\n",
            "Train Cycle: 1281\n",
            "Loss Per Character: 1.553574800491333\n",
            "Train Cycle: 1282\n",
            "Loss Per Character: 1.5328503847122192\n",
            "Train Cycle: 1283\n",
            "Loss Per Character: 1.6971336603164673\n",
            "Train Cycle: 1284\n",
            "Loss Per Character: 1.6904925107955933\n",
            "Train Cycle: 1285\n",
            "Loss Per Character: 1.6386107206344604\n",
            "Train Cycle: 1286\n",
            "Loss Per Character: 1.5177212953567505\n",
            "Train Cycle: 1287\n",
            "Loss Per Character: 1.4598042964935303\n",
            "Train Cycle: 1288\n",
            "Loss Per Character: 1.495734453201294\n",
            "Train Cycle: 1289\n",
            "Loss Per Character: 1.5764169692993164\n",
            "Train Cycle: 1290\n",
            "Loss Per Character: 1.614427089691162\n",
            "Train Cycle: 1291\n",
            "Loss Per Character: 1.7249802350997925\n",
            "Train Cycle: 1292\n",
            "Loss Per Character: 1.3998842239379883\n",
            "Train Cycle: 1293\n",
            "Loss Per Character: 1.4566543102264404\n",
            "Train Cycle: 1294\n",
            "Loss Per Character: 1.559372901916504\n",
            "Train Cycle: 1295\n",
            "Loss Per Character: 1.5951722860336304\n",
            "Train Cycle: 1296\n",
            "Loss Per Character: 1.6990363597869873\n",
            "Train Cycle: 1297\n",
            "Loss Per Character: 1.608174443244934\n",
            "Train Cycle: 1298\n",
            "Loss Per Character: 1.3797928094863892\n",
            "Train Cycle: 1299\n",
            "Loss Per Character: 1.5304434299468994\n",
            "! A lood top on so a man online ord trusedo is trance tranger trance a man actrounds. Althing.<or his.<br //><blo and top train tookious. Its an alowers.<on instending it takes. If. As so thake.<rromm tryen. Anyteriot a direcent on star absorient. ItTrain Cycle: 1300\n",
            "Loss Per Character: 1.4617106914520264\n",
            "Train Cycle: 1301\n",
            "Loss Per Character: 1.6318062543869019\n",
            "Train Cycle: 1302\n",
            "Loss Per Character: 1.6855815649032593\n",
            "Train Cycle: 1303\n",
            "Loss Per Character: 1.369195818901062\n",
            "Train Cycle: 1304\n",
            "Loss Per Character: 1.5454117059707642\n",
            "Train Cycle: 1305\n",
            "Loss Per Character: 1.48552668094635\n",
            "Train Cycle: 1306\n",
            "Loss Per Character: 1.467076063156128\n",
            "Train Cycle: 1307\n",
            "Loss Per Character: 1.6830933094024658\n",
            "Train Cycle: 1308\n",
            "Loss Per Character: 1.3308366537094116\n",
            "Train Cycle: 1309\n",
            "Loss Per Character: 1.7036641836166382\n",
            "Train Cycle: 1310\n",
            "Loss Per Character: 1.4265960454940796\n",
            "Train Cycle: 1311\n",
            "Loss Per Character: 1.6996549367904663\n",
            "Train Cycle: 1312\n",
            "Loss Per Character: 1.4251940250396729\n",
            "Train Cycle: 1313\n",
            "Loss Per Character: 1.3707454204559326\n",
            "Train Cycle: 1314\n",
            "Loss Per Character: 1.4941962957382202\n",
            "Train Cycle: 1315\n",
            "Loss Per Character: 1.4725658893585205\n",
            "Train Cycle: 1316\n",
            "Loss Per Character: 1.5930235385894775\n",
            "Train Cycle: 1317\n",
            "Loss Per Character: 1.4310435056686401\n",
            "Train Cycle: 1318\n",
            "Loss Per Character: 1.513332724571228\n",
            "Train Cycle: 1319\n",
            "Loss Per Character: 1.8420119285583496\n",
            "Train Cycle: 1320\n",
            "Loss Per Character: 1.6262696981430054\n",
            "Train Cycle: 1321\n",
            "Loss Per Character: 1.7566066980361938\n",
            "Train Cycle: 1322\n",
            "Loss Per Character: 1.5392755270004272\n",
            "Train Cycle: 1323\n",
            "Loss Per Character: 1.6601996421813965\n",
            "Train Cycle: 1324\n",
            "Loss Per Character: 1.3948299884796143\n",
            "Train Cycle: 1325\n",
            "Loss Per Character: 1.3435982465744019\n",
            "Train Cycle: 1326\n",
            "Loss Per Character: 1.603170394897461\n",
            "Train Cycle: 1327\n",
            "Loss Per Character: 1.655415415763855\n",
            "Train Cycle: 1328\n",
            "Loss Per Character: 1.404811143875122\n",
            "Train Cycle: 1329\n",
            "Loss Per Character: 1.5810412168502808\n",
            "Train Cycle: 1330\n",
            "Loss Per Character: 1.4477342367172241\n",
            "Train Cycle: 1331\n",
            "Loss Per Character: 1.5599968433380127\n",
            "Train Cycle: 1332\n",
            "Loss Per Character: 1.5668599605560303\n",
            "Train Cycle: 1333\n",
            "Loss Per Character: 1.4770143032073975\n",
            "Train Cycle: 1334\n",
            "Loss Per Character: 1.6162331104278564\n",
            "Train Cycle: 1335\n",
            "Loss Per Character: 1.7732645273208618\n",
            "Train Cycle: 1336\n",
            "Loss Per Character: 1.7100340127944946\n",
            "Train Cycle: 1337\n",
            "Loss Per Character: 1.701209545135498\n",
            "Train Cycle: 1338\n",
            "Loss Per Character: 1.6643645763397217\n",
            "Train Cycle: 1339\n",
            "Loss Per Character: 1.4947336912155151\n",
            "Train Cycle: 1340\n",
            "Loss Per Character: 1.5728031396865845\n",
            "Train Cycle: 1341\n",
            "Loss Per Character: 1.5342378616333008\n",
            "Train Cycle: 1342\n",
            "Loss Per Character: 1.6427668333053589\n",
            "Train Cycle: 1343\n",
            "Loss Per Character: 1.551458716392517\n",
            "Train Cycle: 1344\n",
            "Loss Per Character: 1.4172829389572144\n",
            "Train Cycle: 1345\n",
            "Loss Per Character: 1.429511308670044\n",
            "Train Cycle: 1346\n",
            "Loss Per Character: 1.4797478914260864\n",
            "Train Cycle: 1347\n",
            "Loss Per Character: 1.5995591878890991\n",
            "Train Cycle: 1348\n",
            "Loss Per Character: 1.6313672065734863\n",
            "Train Cycle: 1349\n",
            "Loss Per Character: 1.7354477643966675\n",
            "probblish it wourned to try or servoun for morrible thir mover.<bl/ I wasne it'se a like the movie. This films's. It's alm any second she'se shots of my.... As so the sech ther firs trastiously)<ord or there instant of a likes that I wound him trie fTrain Cycle: 1350\n",
            "Loss Per Character: 1.629373550415039\n",
            "Train Cycle: 1351\n",
            "Loss Per Character: 1.4956557750701904\n",
            "Train Cycle: 1352\n",
            "Loss Per Character: 1.2447501420974731\n",
            "Train Cycle: 1353\n",
            "Loss Per Character: 1.7923822402954102\n",
            "Train Cycle: 1354\n",
            "Loss Per Character: 1.5783928632736206\n",
            "Train Cycle: 1355\n",
            "Loss Per Character: 1.2790969610214233\n",
            "Train Cycle: 1356\n",
            "Loss Per Character: 1.8357164859771729\n",
            "Train Cycle: 1357\n",
            "Loss Per Character: 1.6327764987945557\n",
            "Train Cycle: 1358\n",
            "Loss Per Character: 1.5395078659057617\n",
            "Train Cycle: 1359\n",
            "Loss Per Character: 1.3943067789077759\n",
            "Train Cycle: 1360\n",
            "Loss Per Character: 1.4929572343826294\n",
            "Train Cycle: 1361\n",
            "Loss Per Character: 1.9415419101715088\n",
            "Train Cycle: 1362\n",
            "Loss Per Character: 1.5979564189910889\n",
            "Train Cycle: 1363\n",
            "Loss Per Character: 1.7959849834442139\n",
            "Train Cycle: 1364\n",
            "Loss Per Character: 1.4667876958847046\n",
            "Train Cycle: 1365\n",
            "Loss Per Character: 1.6254324913024902\n",
            "Train Cycle: 1366\n",
            "Loss Per Character: 1.470342755317688\n",
            "Train Cycle: 1367\n",
            "Loss Per Character: 1.4760349988937378\n",
            "Train Cycle: 1368\n",
            "Loss Per Character: 1.4710297584533691\n",
            "Train Cycle: 1369\n",
            "Loss Per Character: 1.6952979564666748\n",
            "Train Cycle: 1370\n",
            "Loss Per Character: 1.894157886505127\n",
            "Train Cycle: 1371\n",
            "Loss Per Character: 1.6628295183181763\n",
            "Train Cycle: 1372\n",
            "Loss Per Character: 1.7377022504806519\n",
            "Train Cycle: 1373\n",
            "Loss Per Character: 1.6015599966049194\n",
            "Train Cycle: 1374\n",
            "Loss Per Character: 1.3403427600860596\n",
            "Train Cycle: 1375\n",
            "Loss Per Character: 1.412452220916748\n",
            "Train Cycle: 1376\n",
            "Loss Per Character: 1.5186845064163208\n",
            "Train Cycle: 1377\n",
            "Loss Per Character: 1.5664159059524536\n",
            "Train Cycle: 1378\n",
            "Loss Per Character: 1.4370251893997192\n",
            "Train Cycle: 1379\n",
            "Loss Per Character: 1.3122727870941162\n",
            "Train Cycle: 1380\n",
            "Loss Per Character: 1.3902554512023926\n",
            "Train Cycle: 1381\n",
            "Loss Per Character: 1.401913046836853\n",
            "Train Cycle: 1382\n",
            "Loss Per Character: 1.5521063804626465\n",
            "Train Cycle: 1383\n",
            "Loss Per Character: 1.8424091339111328\n",
            "Train Cycle: 1384\n",
            "Loss Per Character: 1.5852086544036865\n",
            "Train Cycle: 1385\n",
            "Loss Per Character: 1.5597654581069946\n",
            "Train Cycle: 1386\n",
            "Loss Per Character: 1.4413303136825562\n",
            "Train Cycle: 1387\n",
            "Loss Per Character: 1.6981509923934937\n",
            "Train Cycle: 1388\n",
            "Loss Per Character: 1.340539574623108\n",
            "Train Cycle: 1389\n",
            "Loss Per Character: 1.4600802659988403\n",
            "Train Cycle: 1390\n",
            "Loss Per Character: 1.6085984706878662\n",
            "Train Cycle: 1391\n",
            "Loss Per Character: 1.454119086265564\n",
            "Train Cycle: 1392\n",
            "Loss Per Character: 1.538703441619873\n",
            "Train Cycle: 1393\n",
            "Loss Per Character: 1.4086209535598755\n",
            "Train Cycle: 1394\n",
            "Loss Per Character: 1.4799752235412598\n",
            "Train Cycle: 1395\n",
            "Loss Per Character: 1.5752497911453247\n",
            "Train Cycle: 1396\n",
            "Loss Per Character: 1.3694489002227783\n",
            "Train Cycle: 1397\n",
            "Loss Per Character: 1.5332049131393433\n",
            "Train Cycle: 1398\n",
            "Loss Per Character: 1.4154467582702637\n",
            "Train Cycle: 1399\n",
            "Loss Per Character: 1.5234702825546265\n",
            "Ye stoppinations and took intelevate as teliced to be somethowers.. I won't.. and trackition too maninatioghable we was all there. And in an it.<ribliand insilled. Treaters whenere arous andly...it is third inside as temples the stority) are senso waTrain Cycle: 1400\n",
            "Loss Per Character: 1.4044238328933716\n",
            "Train Cycle: 1401\n",
            "Loss Per Character: 1.580132246017456\n",
            "Train Cycle: 1402\n",
            "Loss Per Character: 1.9008796215057373\n",
            "Train Cycle: 1403\n",
            "Loss Per Character: 1.7128921747207642\n",
            "Train Cycle: 1404\n",
            "Loss Per Character: 1.5538325309753418\n",
            "Train Cycle: 1405\n",
            "Loss Per Character: 1.3774632215499878\n",
            "Train Cycle: 1406\n",
            "Loss Per Character: 1.5184266567230225\n",
            "Train Cycle: 1407\n",
            "Loss Per Character: 1.5817421674728394\n",
            "Train Cycle: 1408\n",
            "Loss Per Character: 1.5682461261749268\n",
            "Train Cycle: 1409\n",
            "Loss Per Character: 1.4824258089065552\n",
            "Train Cycle: 1410\n",
            "Loss Per Character: 1.672203779220581\n",
            "Train Cycle: 1411\n",
            "Loss Per Character: 1.57327139377594\n",
            "Train Cycle: 1412\n",
            "Loss Per Character: 1.6767594814300537\n",
            "Train Cycle: 1413\n",
            "Loss Per Character: 1.4263622760772705\n",
            "Train Cycle: 1414\n",
            "Loss Per Character: 1.5166921615600586\n",
            "Train Cycle: 1415\n",
            "Loss Per Character: 1.6090362071990967\n",
            "Train Cycle: 1416\n",
            "Loss Per Character: 1.375281572341919\n",
            "Train Cycle: 1417\n",
            "Loss Per Character: 1.4266843795776367\n",
            "Train Cycle: 1418\n",
            "Loss Per Character: 1.4838433265686035\n",
            "Train Cycle: 1419\n",
            "Loss Per Character: 1.4525700807571411\n",
            "Train Cycle: 1420\n",
            "Loss Per Character: 1.2647039890289307\n",
            "Train Cycle: 1421\n",
            "Loss Per Character: 1.4834476709365845\n",
            "Train Cycle: 1422\n",
            "Loss Per Character: 1.4930686950683594\n",
            "Train Cycle: 1423\n",
            "Loss Per Character: 1.12945556640625\n",
            "Train Cycle: 1424\n",
            "Loss Per Character: 1.4706040620803833\n",
            "Train Cycle: 1425\n",
            "Loss Per Character: 1.4571245908737183\n",
            "Train Cycle: 1426\n",
            "Loss Per Character: 1.3656991720199585\n",
            "Train Cycle: 1427\n",
            "Loss Per Character: 1.3460915088653564\n",
            "Train Cycle: 1428\n",
            "Loss Per Character: 1.6414339542388916\n",
            "Train Cycle: 1429\n",
            "Loss Per Character: 1.3674218654632568\n",
            "Train Cycle: 1430\n",
            "Loss Per Character: 1.3792314529418945\n",
            "Train Cycle: 1431\n",
            "Loss Per Character: 1.353793740272522\n",
            "Train Cycle: 1432\n",
            "Loss Per Character: 1.5265133380889893\n",
            "Train Cycle: 1433\n",
            "Loss Per Character: 1.6821057796478271\n",
            "Train Cycle: 1434\n",
            "Loss Per Character: 1.6166332960128784\n",
            "Train Cycle: 1435\n",
            "Loss Per Character: 1.3249597549438477\n",
            "Train Cycle: 1436\n",
            "Loss Per Character: 1.4615468978881836\n",
            "Train Cycle: 1437\n",
            "Loss Per Character: 1.382772445678711\n",
            "Train Cycle: 1438\n",
            "Loss Per Character: 1.5387417078018188\n",
            "Train Cycle: 1439\n",
            "Loss Per Character: 1.7688493728637695\n",
            "Train Cycle: 1440\n",
            "Loss Per Character: 1.6669938564300537\n",
            "Train Cycle: 1441\n",
            "Loss Per Character: 1.7266186475753784\n",
            "Train Cycle: 1442\n",
            "Loss Per Character: 1.6707043647766113\n",
            "Train Cycle: 1443\n",
            "Loss Per Character: 1.4991356134414673\n",
            "Train Cycle: 1444\n",
            "Loss Per Character: 1.4992873668670654\n",
            "Train Cycle: 1445\n",
            "Loss Per Character: 1.4066568613052368\n",
            "Train Cycle: 1446\n",
            "Loss Per Character: 1.5580275058746338\n",
            "Train Cycle: 1447\n",
            "Loss Per Character: 1.4730322360992432\n",
            "Train Cycle: 1448\n",
            "Loss Per Character: 1.5082844495773315\n",
            "Train Cycle: 1449\n",
            "Loss Per Character: 1.372771143913269\n",
            "Chestic a companish trust any of a can be thast to be alsear traship. I wasten an a bad tootage. The stop isn't.<r base togeredy) thiss outs arous togather who hish is a changerst a morrit off assine of morish..<br/<or the screen. To monstrong.<br.<oTrain Cycle: 1450\n",
            "Loss Per Character: 1.6045933961868286\n",
            "Train Cycle: 1451\n",
            "Loss Per Character: 1.4027332067489624\n",
            "Train Cycle: 1452\n",
            "Loss Per Character: 1.4708372354507446\n",
            "Train Cycle: 1453\n",
            "Loss Per Character: 1.7004011869430542\n",
            "Train Cycle: 1454\n",
            "Loss Per Character: 1.5026649236679077\n",
            "Train Cycle: 1455\n",
            "Loss Per Character: 1.419180154800415\n",
            "Train Cycle: 1456\n",
            "Loss Per Character: 1.3790678977966309\n",
            "Train Cycle: 1457\n",
            "Loss Per Character: 1.5878702402114868\n",
            "Train Cycle: 1458\n",
            "Loss Per Character: 1.4628777503967285\n",
            "Train Cycle: 1459\n",
            "Loss Per Character: 1.5014419555664062\n",
            "Train Cycle: 1460\n",
            "Loss Per Character: 1.4252910614013672\n",
            "Train Cycle: 1461\n",
            "Loss Per Character: 1.5364035367965698\n",
            "Train Cycle: 1462\n",
            "Loss Per Character: 1.659131646156311\n",
            "Train Cycle: 1463\n",
            "Loss Per Character: 1.2296620607376099\n",
            "Train Cycle: 1464\n",
            "Loss Per Character: 1.5599853992462158\n",
            "Train Cycle: 1465\n",
            "Loss Per Character: 1.310808777809143\n",
            "Train Cycle: 1466\n",
            "Loss Per Character: 1.3973685503005981\n",
            "Train Cycle: 1467\n",
            "Loss Per Character: 1.722471833229065\n",
            "Train Cycle: 1468\n",
            "Loss Per Character: 1.4775162935256958\n",
            "Train Cycle: 1469\n",
            "Loss Per Character: 1.5400269031524658\n",
            "Train Cycle: 1470\n",
            "Loss Per Character: 1.7122820615768433\n",
            "Train Cycle: 1471\n",
            "Loss Per Character: 1.784656047821045\n",
            "Train Cycle: 1472\n",
            "Loss Per Character: 1.4214826822280884\n",
            "Train Cycle: 1473\n",
            "Loss Per Character: 1.4675267934799194\n",
            "Train Cycle: 1474\n",
            "Loss Per Character: 1.626973032951355\n",
            "Train Cycle: 1475\n",
            "Loss Per Character: 1.5655375719070435\n",
            "Train Cycle: 1476\n",
            "Loss Per Character: 1.6401069164276123\n",
            "Train Cycle: 1477\n",
            "Loss Per Character: 1.577539086341858\n",
            "Train Cycle: 1478\n",
            "Loss Per Character: 1.575322151184082\n",
            "Train Cycle: 1479\n",
            "Loss Per Character: 1.4205974340438843\n",
            "Train Cycle: 1480\n",
            "Loss Per Character: 1.6228070259094238\n",
            "Train Cycle: 1481\n",
            "Loss Per Character: 1.4276971817016602\n",
            "Train Cycle: 1482\n",
            "Loss Per Character: 1.6037229299545288\n",
            "Train Cycle: 1483\n",
            "Loss Per Character: 1.5549908876419067\n",
            "Train Cycle: 1484\n",
            "Loss Per Character: 1.5831905603408813\n",
            "Train Cycle: 1485\n",
            "Loss Per Character: 1.63344407081604\n",
            "Train Cycle: 1486\n",
            "Loss Per Character: 1.5205051898956299\n",
            "Train Cycle: 1487\n",
            "Loss Per Character: 1.3682940006256104\n",
            "Train Cycle: 1488\n",
            "Loss Per Character: 1.3960801362991333\n",
            "Train Cycle: 1489\n",
            "Loss Per Character: 1.3951060771942139\n",
            "Train Cycle: 1490\n",
            "Loss Per Character: 1.505147099494934\n",
            "Train Cycle: 1491\n",
            "Loss Per Character: 1.198447585105896\n",
            "Train Cycle: 1492\n",
            "Loss Per Character: 1.7252618074417114\n",
            "Train Cycle: 1493\n",
            "Loss Per Character: 1.5051114559173584\n",
            "Train Cycle: 1494\n",
            "Loss Per Character: 1.4147517681121826\n",
            "Train Cycle: 1495\n",
            "Loss Per Character: 1.323947548866272\n",
            "Train Cycle: 1496\n",
            "Loss Per Character: 1.671877145767212\n",
            "Train Cycle: 1497\n",
            "Loss Per Character: 1.4107325077056885\n",
            "Train Cycle: 1498\n",
            "Loss Per Character: 1.3610790967941284\n",
            "Train Cycle: 1499\n",
            "Loss Per Character: 1.5969489812850952\n",
            "kes often actourself ther fill it's.<ords....<or... Her he is not. I howevers its alse off insullitine an evalitic. It sawn into all isn't evil secensed to belinies tells..<bor/><br Hitler is. Telmany this.<br /><br/>I wink is anythinkst account a coTrain Cycle: 1500\n",
            "Loss Per Character: 1.4358396530151367\n",
            "Train Cycle: 1501\n",
            "Loss Per Character: 1.598874807357788\n",
            "Train Cycle: 1502\n",
            "Loss Per Character: 1.4650893211364746\n",
            "Train Cycle: 1503\n",
            "Loss Per Character: 1.2888323068618774\n",
            "Train Cycle: 1504\n",
            "Loss Per Character: 1.720668911933899\n",
            "Train Cycle: 1505\n",
            "Loss Per Character: 1.6036208868026733\n",
            "Train Cycle: 1506\n",
            "Loss Per Character: 1.498410940170288\n",
            "Train Cycle: 1507\n",
            "Loss Per Character: 1.3736542463302612\n",
            "Train Cycle: 1508\n",
            "Loss Per Character: 1.5051119327545166\n",
            "Train Cycle: 1509\n",
            "Loss Per Character: 1.55179762840271\n",
            "Train Cycle: 1510\n",
            "Loss Per Character: 1.6019363403320312\n",
            "Train Cycle: 1511\n",
            "Loss Per Character: 1.600283145904541\n",
            "Train Cycle: 1512\n",
            "Loss Per Character: 1.598401665687561\n",
            "Train Cycle: 1513\n",
            "Loss Per Character: 1.612708568572998\n",
            "Train Cycle: 1514\n",
            "Loss Per Character: 1.5047978162765503\n",
            "Train Cycle: 1515\n",
            "Loss Per Character: 1.6338990926742554\n",
            "Train Cycle: 1516\n",
            "Loss Per Character: 1.759102463722229\n",
            "Train Cycle: 1517\n",
            "Loss Per Character: 1.665652871131897\n",
            "Train Cycle: 1518\n",
            "Loss Per Character: 1.5009392499923706\n",
            "Train Cycle: 1519\n",
            "Loss Per Character: 1.5837745666503906\n",
            "Train Cycle: 1520\n",
            "Loss Per Character: 1.3647058010101318\n",
            "Train Cycle: 1521\n",
            "Loss Per Character: 1.5574992895126343\n",
            "Train Cycle: 1522\n",
            "Loss Per Character: 1.6745460033416748\n",
            "Train Cycle: 1523\n",
            "Loss Per Character: 1.485642910003662\n",
            "Train Cycle: 1524\n",
            "Loss Per Character: 1.619240164756775\n",
            "Train Cycle: 1525\n",
            "Loss Per Character: 1.493565320968628\n",
            "Train Cycle: 1526\n",
            "Loss Per Character: 1.495055079460144\n",
            "Train Cycle: 1527\n",
            "Loss Per Character: 2.1070644855499268\n",
            "Train Cycle: 1528\n",
            "Loss Per Character: 1.448243260383606\n",
            "Train Cycle: 1529\n",
            "Loss Per Character: 1.7283446788787842\n",
            "Train Cycle: 1530\n",
            "Loss Per Character: 1.3796228170394897\n",
            "Train Cycle: 1531\n",
            "Loss Per Character: 1.4871176481246948\n",
            "Train Cycle: 1532\n",
            "Loss Per Character: 1.4497994184494019\n",
            "Train Cycle: 1533\n",
            "Loss Per Character: 1.5538897514343262\n",
            "Train Cycle: 1534\n",
            "Loss Per Character: 1.5468302965164185\n",
            "Train Cycle: 1535\n",
            "Loss Per Character: 1.641711711883545\n",
            "Train Cycle: 1536\n",
            "Loss Per Character: 1.6279314756393433\n",
            "Train Cycle: 1537\n",
            "Loss Per Character: 1.407651662826538\n",
            "Train Cycle: 1538\n",
            "Loss Per Character: 1.5169295072555542\n",
            "Train Cycle: 1539\n",
            "Loss Per Character: 1.5458804368972778\n",
            "Train Cycle: 1540\n",
            "Loss Per Character: 1.6076388359069824\n",
            "Train Cycle: 1541\n",
            "Loss Per Character: 1.2505091428756714\n",
            "Train Cycle: 1542\n",
            "Loss Per Character: 1.3685024976730347\n",
            "Train Cycle: 1543\n",
            "Loss Per Character: 1.4519299268722534\n",
            "Train Cycle: 1544\n",
            "Loss Per Character: 1.553574562072754\n",
            "Train Cycle: 1545\n",
            "Loss Per Character: 1.3763504028320312\n",
            "Train Cycle: 1546\n",
            "Loss Per Character: 1.4917652606964111\n",
            "Train Cycle: 1547\n",
            "Loss Per Character: 1.365028738975525\n",
            "Train Cycle: 1548\n",
            "Loss Per Character: 1.4439210891723633\n",
            "Train Cycle: 1549\n",
            "Loss Per Character: 1.5932624340057373\n",
            "Q>THas seriol scepe ofter thas seeing this. There we how.<rribe any winner athouther trusten. Instered ofted. And it waste only wasn' in ten is.<r stret within thirs. Althing third on all.<bl Tood. There and tensing to stat thas show is.... Itself. TTrain Cycle: 1550\n",
            "Loss Per Character: 1.6134287118911743\n",
            "Train Cycle: 1551\n",
            "Loss Per Character: 1.729670524597168\n",
            "Train Cycle: 1552\n",
            "Loss Per Character: 1.264195442199707\n",
            "Train Cycle: 1553\n",
            "Loss Per Character: 1.8963998556137085\n",
            "Train Cycle: 1554\n",
            "Loss Per Character: 1.310685157775879\n",
            "Train Cycle: 1555\n",
            "Loss Per Character: 1.6235153675079346\n",
            "Train Cycle: 1556\n",
            "Loss Per Character: 1.3094499111175537\n",
            "Train Cycle: 1557\n",
            "Loss Per Character: 1.499345064163208\n",
            "Train Cycle: 1558\n",
            "Loss Per Character: 1.417928695678711\n",
            "Train Cycle: 1559\n",
            "Loss Per Character: 1.3690294027328491\n",
            "Train Cycle: 1560\n",
            "Loss Per Character: 1.5107158422470093\n",
            "Train Cycle: 1561\n",
            "Loss Per Character: 1.6587568521499634\n",
            "Train Cycle: 1562\n",
            "Loss Per Character: 1.390384316444397\n",
            "Train Cycle: 1563\n",
            "Loss Per Character: 1.3314510583877563\n",
            "Train Cycle: 1564\n",
            "Loss Per Character: 1.5757431983947754\n",
            "Train Cycle: 1565\n",
            "Loss Per Character: 1.4925439357757568\n",
            "Train Cycle: 1566\n",
            "Loss Per Character: 1.8858299255371094\n",
            "Train Cycle: 1567\n",
            "Loss Per Character: 1.5301616191864014\n",
            "Train Cycle: 1568\n",
            "Loss Per Character: 1.6804637908935547\n",
            "Train Cycle: 1569\n",
            "Loss Per Character: 1.580727458000183\n",
            "Train Cycle: 1570\n",
            "Loss Per Character: 1.5843703746795654\n",
            "Train Cycle: 1571\n",
            "Loss Per Character: 1.694814920425415\n",
            "Train Cycle: 1572\n",
            "Loss Per Character: 1.7251063585281372\n",
            "Train Cycle: 1573\n",
            "Loss Per Character: 1.7792733907699585\n",
            "Train Cycle: 1574\n",
            "Loss Per Character: 1.3409620523452759\n",
            "Train Cycle: 1575\n",
            "Loss Per Character: 1.6714187860488892\n",
            "Train Cycle: 1576\n",
            "Loss Per Character: 1.528775930404663\n",
            "Train Cycle: 1577\n",
            "Loss Per Character: 1.8184773921966553\n",
            "Train Cycle: 1578\n",
            "Loss Per Character: 1.679551124572754\n",
            "Train Cycle: 1579\n",
            "Loss Per Character: 1.5697462558746338\n",
            "Train Cycle: 1580\n",
            "Loss Per Character: 1.3764359951019287\n",
            "Train Cycle: 1581\n",
            "Loss Per Character: 1.285704493522644\n",
            "Train Cycle: 1582\n",
            "Loss Per Character: 1.4072200059890747\n",
            "Train Cycle: 1583\n",
            "Loss Per Character: 1.3193024396896362\n",
            "Train Cycle: 1584\n",
            "Loss Per Character: 1.5387367010116577\n",
            "Train Cycle: 1585\n",
            "Loss Per Character: 1.8494359254837036\n",
            "Train Cycle: 1586\n",
            "Loss Per Character: 1.763259768486023\n",
            "Train Cycle: 1587\n",
            "Loss Per Character: 1.7510102987289429\n",
            "Train Cycle: 1588\n",
            "Loss Per Character: 1.617900013923645\n",
            "Train Cycle: 1589\n",
            "Loss Per Character: 1.516618013381958\n",
            "Train Cycle: 1590\n",
            "Loss Per Character: 1.6249780654907227\n",
            "Train Cycle: 1591\n",
            "Loss Per Character: 1.6178317070007324\n",
            "Train Cycle: 1592\n",
            "Loss Per Character: 1.6648074388504028\n",
            "Train Cycle: 1593\n",
            "Loss Per Character: 1.5808451175689697\n",
            "Train Cycle: 1594\n",
            "Loss Per Character: 1.6607935428619385\n",
            "Train Cycle: 1595\n",
            "Loss Per Character: 1.673965334892273\n",
            "Train Cycle: 1596\n",
            "Loss Per Character: 1.523762583732605\n",
            "Train Cycle: 1597\n",
            "Loss Per Character: 1.532447099685669\n",
            "Train Cycle: 1598\n",
            "Loss Per Character: 1.3962823152542114\n",
            "Train Cycle: 1599\n",
            "Loss Per Character: 1.633715033531189\n",
            "vacksticating aborach a pressing thin to sear order a story it and a sex of a praps orinitingly a stall a cholicess origar on a pasicious....<our havids. They seen. It in anyond too stanshe isn't a count thase sometonderful. Itsessiving in terns off Train Cycle: 1600\n",
            "Loss Per Character: 1.5958685874938965\n",
            "Train Cycle: 1601\n",
            "Loss Per Character: 1.4737896919250488\n",
            "Train Cycle: 1602\n",
            "Loss Per Character: 1.3382645845413208\n",
            "Train Cycle: 1603\n",
            "Loss Per Character: 1.5686860084533691\n",
            "Train Cycle: 1604\n",
            "Loss Per Character: 1.4864557981491089\n",
            "Train Cycle: 1605\n",
            "Loss Per Character: 1.471853256225586\n",
            "Train Cycle: 1606\n",
            "Loss Per Character: 1.6451398134231567\n",
            "Train Cycle: 1607\n",
            "Loss Per Character: 1.7476495504379272\n",
            "Train Cycle: 1608\n",
            "Loss Per Character: 1.6190932989120483\n",
            "Train Cycle: 1609\n",
            "Loss Per Character: 1.5982692241668701\n",
            "Train Cycle: 1610\n",
            "Loss Per Character: 1.4592063426971436\n",
            "Train Cycle: 1611\n",
            "Loss Per Character: 1.6375924348831177\n",
            "Train Cycle: 1612\n",
            "Loss Per Character: 1.3518519401550293\n",
            "Train Cycle: 1613\n",
            "Loss Per Character: 1.466599941253662\n",
            "Train Cycle: 1614\n",
            "Loss Per Character: 1.356114387512207\n",
            "Train Cycle: 1615\n",
            "Loss Per Character: 1.4335827827453613\n",
            "Train Cycle: 1616\n",
            "Loss Per Character: 1.2077914476394653\n",
            "Train Cycle: 1617\n",
            "Loss Per Character: 1.4578667879104614\n",
            "Train Cycle: 1618\n",
            "Loss Per Character: 1.5589056015014648\n",
            "Train Cycle: 1619\n",
            "Loss Per Character: 1.454593300819397\n",
            "Train Cycle: 1620\n",
            "Loss Per Character: 1.721532940864563\n",
            "Train Cycle: 1621\n",
            "Loss Per Character: 1.2304437160491943\n",
            "Train Cycle: 1622\n",
            "Loss Per Character: 1.5620750188827515\n",
            "Train Cycle: 1623\n",
            "Loss Per Character: 1.700504183769226\n",
            "Train Cycle: 1624\n",
            "Loss Per Character: 1.6947801113128662\n",
            "Train Cycle: 1625\n",
            "Loss Per Character: 1.5524463653564453\n",
            "Train Cycle: 1626\n",
            "Loss Per Character: 1.414229154586792\n",
            "Train Cycle: 1627\n",
            "Loss Per Character: 1.4461745023727417\n",
            "Train Cycle: 1628\n",
            "Loss Per Character: 1.4341137409210205\n",
            "Train Cycle: 1629\n",
            "Loss Per Character: 1.2419323921203613\n",
            "Train Cycle: 1630\n",
            "Loss Per Character: 1.3714020252227783\n",
            "Train Cycle: 1631\n",
            "Loss Per Character: 1.306518316268921\n",
            "Train Cycle: 1632\n",
            "Loss Per Character: 1.6740024089813232\n",
            "Train Cycle: 1633\n",
            "Loss Per Character: 1.3866769075393677\n",
            "Train Cycle: 1634\n",
            "Loss Per Character: 1.6176400184631348\n",
            "Train Cycle: 1635\n",
            "Loss Per Character: 1.3971562385559082\n",
            "Train Cycle: 1636\n",
            "Loss Per Character: 1.4185359477996826\n",
            "Train Cycle: 1637\n",
            "Loss Per Character: 1.3326520919799805\n",
            "Train Cycle: 1638\n",
            "Loss Per Character: 1.2627769708633423\n",
            "Train Cycle: 1639\n",
            "Loss Per Character: 1.3516032695770264\n",
            "Train Cycle: 1640\n",
            "Loss Per Character: 1.5921456813812256\n",
            "Train Cycle: 1641\n",
            "Loss Per Character: 1.3760936260223389\n",
            "Train Cycle: 1642\n",
            "Loss Per Character: 1.0904083251953125\n",
            "Train Cycle: 1643\n",
            "Loss Per Character: 1.4437798261642456\n",
            "Train Cycle: 1644\n",
            "Loss Per Character: 1.5208827257156372\n",
            "Train Cycle: 1645\n",
            "Loss Per Character: 1.5425071716308594\n",
            "Train Cycle: 1646\n",
            "Loss Per Character: 1.627966284751892\n",
            "Train Cycle: 1647\n",
            "Loss Per Character: 1.7762970924377441\n",
            "Train Cycle: 1648\n",
            "Loss Per Character: 1.3616071939468384\n",
            "Train Cycle: 1649\n",
            "Loss Per Character: 1.489274263381958\n",
            "Vernedy is terns are acculances.. I had. And terror this middles. If yon it worster. Alse. Iffevers a laugh a mover thas alson tell. A lood me this mades a lot of... I worse.<br. And intelvies andsell it.. They aressed to study or settion anyone.<bl/Train Cycle: 1650\n",
            "Loss Per Character: 1.5721327066421509\n",
            "Train Cycle: 1651\n",
            "Loss Per Character: 1.5977460145950317\n",
            "Train Cycle: 1652\n",
            "Loss Per Character: 1.334600806236267\n",
            "Train Cycle: 1653\n",
            "Loss Per Character: 1.3818484544754028\n",
            "Train Cycle: 1654\n",
            "Loss Per Character: 1.4942455291748047\n",
            "Train Cycle: 1655\n",
            "Loss Per Character: 1.5428861379623413\n",
            "Train Cycle: 1656\n",
            "Loss Per Character: 1.1467170715332031\n",
            "Train Cycle: 1657\n",
            "Loss Per Character: 1.3621351718902588\n",
            "Train Cycle: 1658\n",
            "Loss Per Character: 1.4688291549682617\n",
            "Train Cycle: 1659\n",
            "Loss Per Character: 1.6941684484481812\n",
            "Train Cycle: 1660\n",
            "Loss Per Character: 1.7630826234817505\n",
            "Train Cycle: 1661\n",
            "Loss Per Character: 1.4798322916030884\n",
            "Train Cycle: 1662\n",
            "Loss Per Character: 1.5271530151367188\n",
            "Train Cycle: 1663\n",
            "Loss Per Character: 1.507189393043518\n",
            "Train Cycle: 1664\n",
            "Loss Per Character: 1.4711599349975586\n",
            "Train Cycle: 1665\n",
            "Loss Per Character: 1.432356357574463\n",
            "Train Cycle: 1666\n",
            "Loss Per Character: 1.5508699417114258\n",
            "Train Cycle: 1667\n",
            "Loss Per Character: 1.5039774179458618\n",
            "Train Cycle: 1668\n",
            "Loss Per Character: 1.6179052591323853\n",
            "Train Cycle: 1669\n",
            "Loss Per Character: 1.4573478698730469\n",
            "Train Cycle: 1670\n",
            "Loss Per Character: 1.5384318828582764\n",
            "Train Cycle: 1671\n",
            "Loss Per Character: 1.4531896114349365\n",
            "Train Cycle: 1672\n",
            "Loss Per Character: 1.4407079219818115\n",
            "Train Cycle: 1673\n",
            "Loss Per Character: 1.4423693418502808\n",
            "Train Cycle: 1674\n",
            "Loss Per Character: 1.3976078033447266\n",
            "Train Cycle: 1675\n",
            "Loss Per Character: 1.606620192527771\n",
            "Train Cycle: 1676\n",
            "Loss Per Character: 1.6545288562774658\n",
            "Train Cycle: 1677\n",
            "Loss Per Character: 1.47272527217865\n",
            "Train Cycle: 1678\n",
            "Loss Per Character: 1.6144959926605225\n",
            "Train Cycle: 1679\n",
            "Loss Per Character: 1.6291946172714233\n",
            "Train Cycle: 1680\n",
            "Loss Per Character: 1.3858885765075684\n",
            "Train Cycle: 1681\n",
            "Loss Per Character: 1.5016560554504395\n",
            "Train Cycle: 1682\n",
            "Loss Per Character: 1.367889642715454\n",
            "Train Cycle: 1683\n",
            "Loss Per Character: 1.4207522869110107\n",
            "Train Cycle: 1684\n",
            "Loss Per Character: 1.3957792520523071\n",
            "Train Cycle: 1685\n",
            "Loss Per Character: 1.0914387702941895\n",
            "Train Cycle: 1686\n",
            "Loss Per Character: 1.5186134576797485\n",
            "Train Cycle: 1687\n",
            "Loss Per Character: 1.4284796714782715\n",
            "Train Cycle: 1688\n",
            "Loss Per Character: 1.3694506883621216\n",
            "Train Cycle: 1689\n",
            "Loss Per Character: 1.5682342052459717\n",
            "Train Cycle: 1690\n",
            "Loss Per Character: 1.490907907485962\n",
            "Train Cycle: 1691\n",
            "Loss Per Character: 1.6603021621704102\n",
            "Train Cycle: 1692\n",
            "Loss Per Character: 1.326069712638855\n",
            "Train Cycle: 1693\n",
            "Loss Per Character: 1.4823049306869507\n",
            "Train Cycle: 1694\n",
            "Loss Per Character: 1.480348825454712\n",
            "Train Cycle: 1695\n",
            "Loss Per Character: 1.4333683252334595\n",
            "Train Cycle: 1696\n",
            "Loss Per Character: 1.4082199335098267\n",
            "Train Cycle: 1697\n",
            "Loss Per Character: 1.6624183654785156\n",
            "Train Cycle: 1698\n",
            "Loss Per Character: 1.5472687482833862\n",
            "Train Cycle: 1699\n",
            "Loss Per Character: 1.5190684795379639\n",
            "\" an a mothers took. Althing..<or his moverstands instead. Alivious\" are to tele it talleds town intrittives.. to believe tood a mosk aspecabes it was the movoryline it to theme ways trise.. It' watce and that's the stant out they she do wortules andTrain Cycle: 1700\n",
            "Loss Per Character: 1.5091921091079712\n",
            "Train Cycle: 1701\n",
            "Loss Per Character: 1.4639685153961182\n",
            "Train Cycle: 1702\n",
            "Loss Per Character: 1.5645681619644165\n",
            "Train Cycle: 1703\n",
            "Loss Per Character: 1.489029884338379\n",
            "Train Cycle: 1704\n",
            "Loss Per Character: 1.5238245725631714\n",
            "Train Cycle: 1705\n",
            "Loss Per Character: 1.544616937637329\n",
            "Train Cycle: 1706\n",
            "Loss Per Character: 1.5496134757995605\n",
            "Train Cycle: 1707\n",
            "Loss Per Character: 1.6229912042617798\n",
            "Train Cycle: 1708\n",
            "Loss Per Character: 1.3920916318893433\n",
            "Train Cycle: 1709\n",
            "Loss Per Character: 1.6335091590881348\n",
            "Train Cycle: 1710\n",
            "Loss Per Character: 1.237676739692688\n",
            "Train Cycle: 1711\n",
            "Loss Per Character: 1.5423924922943115\n",
            "Train Cycle: 1712\n",
            "Loss Per Character: 1.6226569414138794\n",
            "Train Cycle: 1713\n",
            "Loss Per Character: 1.4983211755752563\n",
            "Train Cycle: 1714\n",
            "Loss Per Character: 1.6302168369293213\n",
            "Train Cycle: 1715\n",
            "Loss Per Character: 1.4622700214385986\n",
            "Train Cycle: 1716\n",
            "Loss Per Character: 1.2562506198883057\n",
            "Train Cycle: 1717\n",
            "Loss Per Character: 1.7442359924316406\n",
            "Train Cycle: 1718\n",
            "Loss Per Character: 1.6181286573410034\n",
            "Train Cycle: 1719\n",
            "Loss Per Character: 1.7827174663543701\n",
            "Train Cycle: 1720\n",
            "Loss Per Character: 1.5597691535949707\n",
            "Train Cycle: 1721\n",
            "Loss Per Character: 1.4700440168380737\n",
            "Train Cycle: 1722\n",
            "Loss Per Character: 1.5726253986358643\n",
            "Train Cycle: 1723\n",
            "Loss Per Character: 1.8666232824325562\n",
            "Train Cycle: 1724\n",
            "Loss Per Character: 1.7530502080917358\n",
            "Train Cycle: 1725\n",
            "Loss Per Character: 1.4201408624649048\n",
            "Train Cycle: 1726\n",
            "Loss Per Character: 1.360520362854004\n",
            "Train Cycle: 1727\n",
            "Loss Per Character: 1.6664478778839111\n",
            "Train Cycle: 1728\n",
            "Loss Per Character: 1.4875966310501099\n",
            "Train Cycle: 1729\n",
            "Loss Per Character: 1.5903136730194092\n",
            "Train Cycle: 1730\n",
            "Loss Per Character: 1.6103804111480713\n",
            "Train Cycle: 1731\n",
            "Loss Per Character: 1.5795806646347046\n",
            "Train Cycle: 1732\n",
            "Loss Per Character: 1.3386484384536743\n",
            "Train Cycle: 1733\n",
            "Loss Per Character: 1.6247587203979492\n",
            "Train Cycle: 1734\n",
            "Loss Per Character: 1.4965593814849854\n",
            "Train Cycle: 1735\n",
            "Loss Per Character: 1.6105518341064453\n",
            "Train Cycle: 1736\n",
            "Loss Per Character: 1.5032671689987183\n",
            "Train Cycle: 1737\n",
            "Loss Per Character: 1.7057349681854248\n",
            "Train Cycle: 1738\n",
            "Loss Per Character: 1.4761075973510742\n",
            "Train Cycle: 1739\n",
            "Loss Per Character: 1.6525846719741821\n",
            "Train Cycle: 1740\n",
            "Loss Per Character: 1.4784833192825317\n",
            "Train Cycle: 1741\n",
            "Loss Per Character: 1.5274418592453003\n",
            "Train Cycle: 1742\n",
            "Loss Per Character: 1.3962444067001343\n",
            "Train Cycle: 1743\n",
            "Loss Per Character: 1.5621538162231445\n",
            "Train Cycle: 1744\n",
            "Loss Per Character: 1.5379749536514282\n",
            "Train Cycle: 1745\n",
            "Loss Per Character: 1.349092721939087\n",
            "Train Cycle: 1746\n",
            "Loss Per Character: 1.5568867921829224\n",
            "Train Cycle: 1747\n",
            "Loss Per Character: 1.4109934568405151\n",
            "Train Cycle: 1748\n",
            "Loss Per Character: 1.3455476760864258\n",
            "Train Cycle: 1749\n",
            "Loss Per Character: 1.300487995147705\n",
            "I'visling. They arous a porn ournotiou liked.<bl/> Toose anniese in tile. Things out the mise offer togaliest of tilies and the screeden timately to belief it worse they're. Trus is a coullet insull a coup stayshic watserious. In implosied by an and Train Cycle: 1750\n",
            "Loss Per Character: 1.1841492652893066\n",
            "Train Cycle: 1751\n",
            "Loss Per Character: 1.6089383363723755\n",
            "Train Cycle: 1752\n",
            "Loss Per Character: 1.2859959602355957\n",
            "Train Cycle: 1753\n",
            "Loss Per Character: 1.7638102769851685\n",
            "Train Cycle: 1754\n",
            "Loss Per Character: 1.420217752456665\n",
            "Train Cycle: 1755\n",
            "Loss Per Character: 1.5605612993240356\n",
            "Train Cycle: 1756\n",
            "Loss Per Character: 1.3137773275375366\n",
            "Train Cycle: 1757\n",
            "Loss Per Character: 1.4388808012008667\n",
            "Train Cycle: 1758\n",
            "Loss Per Character: 1.389754056930542\n",
            "Train Cycle: 1759\n",
            "Loss Per Character: 1.4570146799087524\n",
            "Train Cycle: 1760\n",
            "Loss Per Character: 1.4444102048873901\n",
            "Train Cycle: 1761\n",
            "Loss Per Character: 1.284070372581482\n",
            "Train Cycle: 1762\n",
            "Loss Per Character: 1.5625839233398438\n",
            "Train Cycle: 1763\n",
            "Loss Per Character: 1.2703135013580322\n",
            "Train Cycle: 1764\n",
            "Loss Per Character: 1.3697230815887451\n",
            "Train Cycle: 1765\n",
            "Loss Per Character: 1.492858648300171\n",
            "Train Cycle: 1766\n",
            "Loss Per Character: 1.2501147985458374\n",
            "Train Cycle: 1767\n",
            "Loss Per Character: 1.4473657608032227\n",
            "Train Cycle: 1768\n",
            "Loss Per Character: 1.377097249031067\n",
            "Train Cycle: 1769\n",
            "Loss Per Character: 1.3628449440002441\n",
            "Train Cycle: 1770\n",
            "Loss Per Character: 1.4046980142593384\n",
            "Train Cycle: 1771\n",
            "Loss Per Character: 1.281128168106079\n",
            "Train Cycle: 1772\n",
            "Loss Per Character: 1.613391399383545\n",
            "Train Cycle: 1773\n",
            "Loss Per Character: 1.2219406366348267\n",
            "Train Cycle: 1774\n",
            "Loss Per Character: 1.4671794176101685\n",
            "Train Cycle: 1775\n",
            "Loss Per Character: 1.5591260194778442\n",
            "Train Cycle: 1776\n",
            "Loss Per Character: 1.7339879274368286\n",
            "Train Cycle: 1777\n",
            "Loss Per Character: 1.4478415250778198\n",
            "Train Cycle: 1778\n",
            "Loss Per Character: 1.7901419401168823\n",
            "Train Cycle: 1779\n",
            "Loss Per Character: 1.5913499593734741\n",
            "Train Cycle: 1780\n",
            "Loss Per Character: 1.4978684186935425\n",
            "Train Cycle: 1781\n",
            "Loss Per Character: 1.2775647640228271\n",
            "Train Cycle: 1782\n",
            "Loss Per Character: 1.386702060699463\n",
            "Train Cycle: 1783\n",
            "Loss Per Character: 1.36105477809906\n",
            "Train Cycle: 1784\n",
            "Loss Per Character: 1.3467541933059692\n",
            "Train Cycle: 1785\n",
            "Loss Per Character: 1.347558617591858\n",
            "Train Cycle: 1786\n",
            "Loss Per Character: 1.3008971214294434\n",
            "Train Cycle: 1787\n",
            "Loss Per Character: 1.6294523477554321\n",
            "Train Cycle: 1788\n",
            "Loss Per Character: 1.4993053674697876\n",
            "Train Cycle: 1789\n",
            "Loss Per Character: 1.5692497491836548\n",
            "Train Cycle: 1790\n",
            "Loss Per Character: 1.4504601955413818\n",
            "Train Cycle: 1791\n",
            "Loss Per Character: 1.3756264448165894\n",
            "Train Cycle: 1792\n",
            "Loss Per Character: 1.3045037984848022\n",
            "Train Cycle: 1793\n",
            "Loss Per Character: 1.5102155208587646\n",
            "Train Cycle: 1794\n",
            "Loss Per Character: 1.5089807510375977\n",
            "Train Cycle: 1795\n",
            "Loss Per Character: 1.6050504446029663\n",
            "Train Cycle: 1796\n",
            "Loss Per Character: 1.656696081161499\n",
            "Train Cycle: 1797\n",
            "Loss Per Character: 1.4675973653793335\n",
            "Train Cycle: 1798\n",
            "Loss Per Character: 1.1736886501312256\n",
            "Train Cycle: 1799\n",
            "Loss Per Character: 1.4007056951522827\n",
            "(Sanyen isnesses at anyone of... tense off terring.<br /><br... Tom an interesined too becends to be alowars in to shoots.<o was. It worth they'verape a chaceen askays it isn' betwee what is therough trass to terribleds in a bot thas it' and it wouldTrain Cycle: 1800\n",
            "Loss Per Character: 1.5008361339569092\n",
            "Train Cycle: 1801\n",
            "Loss Per Character: 1.5654488801956177\n",
            "Train Cycle: 1802\n",
            "Loss Per Character: 1.2966781854629517\n",
            "Train Cycle: 1803\n",
            "Loss Per Character: 1.5225183963775635\n",
            "Train Cycle: 1804\n",
            "Loss Per Character: 1.4630340337753296\n",
            "Train Cycle: 1805\n",
            "Loss Per Character: 1.5830771923065186\n",
            "Train Cycle: 1806\n",
            "Loss Per Character: 1.4615881443023682\n",
            "Train Cycle: 1807\n",
            "Loss Per Character: 1.4700167179107666\n",
            "Train Cycle: 1808\n",
            "Loss Per Character: 1.5062447786331177\n",
            "Train Cycle: 1809\n",
            "Loss Per Character: 1.5284425020217896\n",
            "Train Cycle: 1810\n",
            "Loss Per Character: 1.6155836582183838\n",
            "Train Cycle: 1811\n",
            "Loss Per Character: 1.3252438306808472\n",
            "Train Cycle: 1812\n",
            "Loss Per Character: 1.7323838472366333\n",
            "Train Cycle: 1813\n",
            "Loss Per Character: 1.5726374387741089\n",
            "Train Cycle: 1814\n",
            "Loss Per Character: 1.6376097202301025\n",
            "Train Cycle: 1815\n",
            "Loss Per Character: 1.5410116910934448\n",
            "Train Cycle: 1816\n",
            "Loss Per Character: 1.5026731491088867\n",
            "Train Cycle: 1817\n",
            "Loss Per Character: 1.4691195487976074\n",
            "Train Cycle: 1818\n",
            "Loss Per Character: 1.4070905447006226\n",
            "Train Cycle: 1819\n",
            "Loss Per Character: 1.4462835788726807\n",
            "Train Cycle: 1820\n",
            "Loss Per Character: 1.7941713333129883\n",
            "Train Cycle: 1821\n",
            "Loss Per Character: 1.5810984373092651\n",
            "Train Cycle: 1822\n",
            "Loss Per Character: 1.3478114604949951\n",
            "Train Cycle: 1823\n",
            "Loss Per Character: 1.5309803485870361\n",
            "Train Cycle: 1824\n",
            "Loss Per Character: 1.4490363597869873\n",
            "Train Cycle: 1825\n",
            "Loss Per Character: 1.4812562465667725\n",
            "Train Cycle: 1826\n",
            "Loss Per Character: 1.540558099746704\n",
            "Train Cycle: 1827\n",
            "Loss Per Character: 1.3540515899658203\n",
            "Train Cycle: 1828\n",
            "Loss Per Character: 1.527138352394104\n",
            "Train Cycle: 1829\n",
            "Loss Per Character: 1.4663668870925903\n",
            "Train Cycle: 1830\n",
            "Loss Per Character: 1.4147087335586548\n",
            "Train Cycle: 1831\n",
            "Loss Per Character: 1.334611177444458\n",
            "Train Cycle: 1832\n",
            "Loss Per Character: 1.569091558456421\n",
            "Train Cycle: 1833\n",
            "Loss Per Character: 1.4041281938552856\n",
            "Train Cycle: 1834\n",
            "Loss Per Character: 1.5046769380569458\n",
            "Train Cycle: 1835\n",
            "Loss Per Character: 1.720513105392456\n",
            "Train Cycle: 1836\n",
            "Loss Per Character: 1.5242723226547241\n",
            "Train Cycle: 1837\n",
            "Loss Per Character: 1.5080935955047607\n",
            "Train Cycle: 1838\n",
            "Loss Per Character: 1.537484884262085\n",
            "Train Cycle: 1839\n",
            "Loss Per Character: 1.5973238945007324\n",
            "Train Cycle: 1840\n",
            "Loss Per Character: 1.313015103340149\n",
            "Train Cycle: 1841\n",
            "Loss Per Character: 1.4905996322631836\n",
            "Train Cycle: 1842\n",
            "Loss Per Character: 1.301978588104248\n",
            "Train Cycle: 1843\n",
            "Loss Per Character: 1.5565695762634277\n",
            "Train Cycle: 1844\n",
            "Loss Per Character: 1.577250361442566\n",
            "Train Cycle: 1845\n",
            "Loss Per Character: 1.7172280550003052\n",
            "Train Cycle: 1846\n",
            "Loss Per Character: 1.398126244544983\n",
            "Train Cycle: 1847\n",
            "Loss Per Character: 1.3961613178253174\n",
            "Train Cycle: 1848\n",
            "Loss Per Character: 1.463650107383728\n",
            "Train Cycle: 1849\n",
            "Loss Per Character: 1.3590989112854004\n",
            ". Think it's succees.. I such a shows are took the prescened ins the scames an anyone is trashed a catch ofter.... this movers's so a shape as.<bolits ofter the most ordedly also that. If yarran a finationall as try..<r than is there's the screwinessTrain Cycle: 1850\n",
            "Loss Per Character: 1.4821746349334717\n",
            "Train Cycle: 1851\n",
            "Loss Per Character: 1.623852252960205\n",
            "Train Cycle: 1852\n",
            "Loss Per Character: 1.6173534393310547\n",
            "Train Cycle: 1853\n",
            "Loss Per Character: 1.294692873954773\n",
            "Train Cycle: 1854\n",
            "Loss Per Character: 1.4512684345245361\n",
            "Train Cycle: 1855\n",
            "Loss Per Character: 1.5184860229492188\n",
            "Train Cycle: 1856\n",
            "Loss Per Character: 1.377651572227478\n",
            "Train Cycle: 1857\n",
            "Loss Per Character: 1.3169279098510742\n",
            "Train Cycle: 1858\n",
            "Loss Per Character: 1.5027045011520386\n",
            "Train Cycle: 1859\n",
            "Loss Per Character: 1.4852374792099\n",
            "Train Cycle: 1860\n",
            "Loss Per Character: 1.2554799318313599\n",
            "Train Cycle: 1861\n",
            "Loss Per Character: 1.4839264154434204\n",
            "Train Cycle: 1862\n",
            "Loss Per Character: 1.8416961431503296\n",
            "Train Cycle: 1863\n",
            "Loss Per Character: 1.2389254570007324\n",
            "Train Cycle: 1864\n",
            "Loss Per Character: 1.3803324699401855\n",
            "Train Cycle: 1865\n",
            "Loss Per Character: 1.6467336416244507\n",
            "Train Cycle: 1866\n",
            "Loss Per Character: 1.5119937658309937\n",
            "Train Cycle: 1867\n",
            "Loss Per Character: 1.5659719705581665\n",
            "Train Cycle: 1868\n",
            "Loss Per Character: 1.2788662910461426\n",
            "Train Cycle: 1869\n",
            "Loss Per Character: 1.4628874063491821\n",
            "Train Cycle: 1870\n",
            "Loss Per Character: 1.3244647979736328\n",
            "Train Cycle: 1871\n",
            "Loss Per Character: 1.210343837738037\n",
            "Train Cycle: 1872\n",
            "Loss Per Character: 1.4053912162780762\n",
            "Train Cycle: 1873\n",
            "Loss Per Character: 1.3054044246673584\n",
            "Train Cycle: 1874\n",
            "Loss Per Character: 1.4139975309371948\n",
            "Train Cycle: 1875\n",
            "Loss Per Character: 1.5472544431686401\n",
            "Train Cycle: 1876\n",
            "Loss Per Character: 1.5262789726257324\n",
            "Train Cycle: 1877\n",
            "Loss Per Character: 1.423337697982788\n",
            "Train Cycle: 1878\n",
            "Loss Per Character: 1.274506688117981\n",
            "Train Cycle: 1879\n",
            "Loss Per Character: 1.4598183631896973\n",
            "Train Cycle: 1880\n",
            "Loss Per Character: 1.3611797094345093\n",
            "Train Cycle: 1881\n",
            "Loss Per Character: 1.1279863119125366\n",
            "Train Cycle: 1882\n",
            "Loss Per Character: 1.3575280904769897\n",
            "Train Cycle: 1883\n",
            "Loss Per Character: 1.379076361656189\n",
            "Train Cycle: 1884\n",
            "Loss Per Character: 1.3044838905334473\n",
            "Train Cycle: 1885\n",
            "Loss Per Character: 1.4229495525360107\n",
            "Train Cycle: 1886\n",
            "Loss Per Character: 1.3703736066818237\n",
            "Train Cycle: 1887\n",
            "Loss Per Character: 1.112811803817749\n",
            "Train Cycle: 1888\n",
            "Loss Per Character: 1.3834184408187866\n",
            "Train Cycle: 1889\n",
            "Loss Per Character: 1.526770830154419\n",
            "Train Cycle: 1890\n",
            "Loss Per Character: 1.4250410795211792\n",
            "Train Cycle: 1891\n",
            "Loss Per Character: 1.5501466989517212\n",
            "Train Cycle: 1892\n",
            "Loss Per Character: 1.592715859413147\n",
            "Train Cycle: 1893\n",
            "Loss Per Character: 1.3844534158706665\n",
            "Train Cycle: 1894\n",
            "Loss Per Character: 1.408761978149414\n",
            "Train Cycle: 1895\n",
            "Loss Per Character: 1.748522400856018\n",
            "Train Cycle: 1896\n",
            "Loss Per Character: 1.629374384880066\n",
            "Train Cycle: 1897\n",
            "Loss Per Character: 1.7067912817001343\n",
            "Train Cycle: 1898\n",
            "Loss Per Character: 1.564577579498291\n",
            "Train Cycle: 1899\n",
            "Loss Per Character: 1.5714592933654785\n",
            "rig four out took of andisted\".<orrough.<r /><but triest trist at lear souldn't.<br and tryings offecid thanks a serray.<rroody anding tirestort to becent footally see it want at a believious too bace thandstoped truly..<bl /> I'med top then. It's abTrain Cycle: 1900\n",
            "Loss Per Character: 1.5180284976959229\n",
            "Train Cycle: 1901\n",
            "Loss Per Character: 1.823138952255249\n",
            "Train Cycle: 1902\n",
            "Loss Per Character: 1.4956414699554443\n",
            "Train Cycle: 1903\n",
            "Loss Per Character: 1.5144546031951904\n",
            "Train Cycle: 1904\n",
            "Loss Per Character: 1.6496379375457764\n",
            "Train Cycle: 1905\n",
            "Loss Per Character: 1.5233124494552612\n",
            "Train Cycle: 1906\n",
            "Loss Per Character: 1.624573826789856\n",
            "Train Cycle: 1907\n",
            "Loss Per Character: 1.507585048675537\n",
            "Train Cycle: 1908\n",
            "Loss Per Character: 1.6662302017211914\n",
            "Train Cycle: 1909\n",
            "Loss Per Character: 1.42879056930542\n",
            "Train Cycle: 1910\n",
            "Loss Per Character: 1.2649999856948853\n",
            "Train Cycle: 1911\n",
            "Loss Per Character: 1.5997503995895386\n",
            "Train Cycle: 1912\n",
            "Loss Per Character: 1.4813793897628784\n",
            "Train Cycle: 1913\n",
            "Loss Per Character: 1.7524150609970093\n",
            "Train Cycle: 1914\n",
            "Loss Per Character: 1.4488449096679688\n",
            "Train Cycle: 1915\n",
            "Loss Per Character: 1.5186001062393188\n",
            "Train Cycle: 1916\n",
            "Loss Per Character: 1.484099268913269\n",
            "Train Cycle: 1917\n",
            "Loss Per Character: 1.3823182582855225\n",
            "Train Cycle: 1918\n",
            "Loss Per Character: 1.4981498718261719\n",
            "Train Cycle: 1919\n",
            "Loss Per Character: 1.289223551750183\n",
            "Train Cycle: 1920\n",
            "Loss Per Character: 1.3608479499816895\n",
            "Train Cycle: 1921\n",
            "Loss Per Character: 1.3608307838439941\n",
            "Train Cycle: 1922\n",
            "Loss Per Character: 1.2344845533370972\n",
            "Train Cycle: 1923\n",
            "Loss Per Character: 1.2632676362991333\n",
            "Train Cycle: 1924\n",
            "Loss Per Character: 1.5460567474365234\n",
            "Train Cycle: 1925\n",
            "Loss Per Character: 1.3420383930206299\n",
            "Train Cycle: 1926\n",
            "Loss Per Character: 1.589609980583191\n",
            "Train Cycle: 1927\n",
            "Loss Per Character: 1.3752084970474243\n",
            "Train Cycle: 1928\n",
            "Loss Per Character: 1.4909913539886475\n",
            "Train Cycle: 1929\n",
            "Loss Per Character: 1.3129749298095703\n",
            "Train Cycle: 1930\n",
            "Loss Per Character: 1.4023776054382324\n",
            "Train Cycle: 1931\n",
            "Loss Per Character: 1.3148070573806763\n",
            "Train Cycle: 1932\n",
            "Loss Per Character: 1.2648258209228516\n",
            "Train Cycle: 1933\n",
            "Loss Per Character: 1.3025047779083252\n",
            "Train Cycle: 1934\n",
            "Loss Per Character: 1.335856318473816\n",
            "Train Cycle: 1935\n",
            "Loss Per Character: 1.4325658082962036\n",
            "Train Cycle: 1936\n",
            "Loss Per Character: 1.4052814245224\n",
            "Train Cycle: 1937\n",
            "Loss Per Character: 1.368372917175293\n",
            "Train Cycle: 1938\n",
            "Loss Per Character: 1.5849796533584595\n",
            "Train Cycle: 1939\n",
            "Loss Per Character: 1.5409976243972778\n",
            "Train Cycle: 1940\n",
            "Loss Per Character: 1.4297335147857666\n",
            "Train Cycle: 1941\n",
            "Loss Per Character: 1.4630640745162964\n",
            "Train Cycle: 1942\n",
            "Loss Per Character: 1.3966830968856812\n",
            "Train Cycle: 1943\n",
            "Loss Per Character: 1.3438832759857178\n",
            "Train Cycle: 1944\n",
            "Loss Per Character: 1.5790435075759888\n",
            "Train Cycle: 1945\n",
            "Loss Per Character: 1.459041714668274\n",
            "Train Cycle: 1946\n",
            "Loss Per Character: 1.3068249225616455\n",
            "Train Cycle: 1947\n",
            "Loss Per Character: 1.8165874481201172\n",
            "Train Cycle: 1948\n",
            "Loss Per Character: 1.6403475999832153\n",
            "Train Cycle: 1949\n",
            "Loss Per Character: 1.4898056983947754\n",
            "wit thantonis on so wher to thinkis she deselt... thant one. If to seriols too movie whethich in this!!!<bo this! It was all teacksterion there've evelt sen to sam and. It woully's so that...<br/</><br />The perials try one whow to see interailed! ToTrain Cycle: 1950\n",
            "Loss Per Character: 1.651082158088684\n",
            "Train Cycle: 1951\n",
            "Loss Per Character: 1.6411592960357666\n",
            "Train Cycle: 1952\n",
            "Loss Per Character: 1.4772014617919922\n",
            "Train Cycle: 1953\n",
            "Loss Per Character: 1.5507638454437256\n",
            "Train Cycle: 1954\n",
            "Loss Per Character: 1.4669181108474731\n",
            "Train Cycle: 1955\n",
            "Loss Per Character: 1.437593698501587\n",
            "Train Cycle: 1956\n",
            "Loss Per Character: 1.4990932941436768\n",
            "Train Cycle: 1957\n",
            "Loss Per Character: 1.4711322784423828\n",
            "Train Cycle: 1958\n",
            "Loss Per Character: 1.3298052549362183\n",
            "Train Cycle: 1959\n",
            "Loss Per Character: 1.4851269721984863\n",
            "Train Cycle: 1960\n",
            "Loss Per Character: 1.418552279472351\n",
            "Train Cycle: 1961\n",
            "Loss Per Character: 1.4453482627868652\n",
            "Train Cycle: 1962\n",
            "Loss Per Character: 1.4043396711349487\n",
            "Train Cycle: 1963\n",
            "Loss Per Character: 1.2814313173294067\n",
            "Train Cycle: 1964\n",
            "Loss Per Character: 1.3201706409454346\n",
            "Train Cycle: 1965\n",
            "Loss Per Character: 1.382657527923584\n",
            "Train Cycle: 1966\n",
            "Loss Per Character: 1.4643272161483765\n",
            "Train Cycle: 1967\n",
            "Loss Per Character: 1.680925726890564\n",
            "Train Cycle: 1968\n",
            "Loss Per Character: 1.338364601135254\n",
            "Train Cycle: 1969\n",
            "Loss Per Character: 1.9364453554153442\n",
            "Train Cycle: 1970\n",
            "Loss Per Character: 1.4611316919326782\n",
            "Train Cycle: 1971\n",
            "Loss Per Character: 1.4021834135055542\n",
            "Train Cycle: 1972\n",
            "Loss Per Character: 1.5862654447555542\n",
            "Train Cycle: 1973\n",
            "Loss Per Character: 1.3922560214996338\n",
            "Train Cycle: 1974\n",
            "Loss Per Character: 1.2896981239318848\n",
            "Train Cycle: 1975\n",
            "Loss Per Character: 1.562376618385315\n",
            "Train Cycle: 1976\n",
            "Loss Per Character: 1.416472315788269\n",
            "Train Cycle: 1977\n",
            "Loss Per Character: 1.5195399522781372\n",
            "Train Cycle: 1978\n",
            "Loss Per Character: 1.5272959470748901\n",
            "Train Cycle: 1979\n",
            "Loss Per Character: 1.3711785078048706\n",
            "Train Cycle: 1980\n",
            "Loss Per Character: 1.617963433265686\n",
            "Train Cycle: 1981\n",
            "Loss Per Character: 1.2357937097549438\n",
            "Train Cycle: 1982\n",
            "Loss Per Character: 1.3057295083999634\n",
            "Train Cycle: 1983\n",
            "Loss Per Character: 1.425061583518982\n",
            "Train Cycle: 1984\n",
            "Loss Per Character: 1.43205726146698\n",
            "Train Cycle: 1985\n",
            "Loss Per Character: 1.4633949995040894\n",
            "Train Cycle: 1986\n",
            "Loss Per Character: 1.3769451379776\n",
            "Train Cycle: 1987\n",
            "Loss Per Character: 1.5873942375183105\n",
            "Train Cycle: 1988\n",
            "Loss Per Character: 1.5532640218734741\n",
            "Train Cycle: 1989\n",
            "Loss Per Character: 1.4559167623519897\n",
            "Train Cycle: 1990\n",
            "Loss Per Character: 1.2837615013122559\n",
            "Train Cycle: 1991\n",
            "Loss Per Character: 1.4377127885818481\n",
            "Train Cycle: 1992\n",
            "Loss Per Character: 1.1266250610351562\n",
            "Train Cycle: 1993\n",
            "Loss Per Character: 1.38534414768219\n",
            "Train Cycle: 1994\n",
            "Loss Per Character: 1.4653465747833252\n",
            "Train Cycle: 1995\n",
            "Loss Per Character: 1.711104393005371\n",
            "Train Cycle: 1996\n",
            "Loss Per Character: 1.4950339794158936\n",
            "Train Cycle: 1997\n",
            "Loss Per Character: 1.421774983406067\n",
            "Train Cycle: 1998\n",
            "Loss Per Character: 1.469126582145691\n",
            "Train Cycle: 1999\n",
            "Loss Per Character: 1.33363676071167\n",
            "Nething it in alsons totack tites. All a bill of.. too sayin white him to the word. Than alop or stired and acridities take a going thirs.<bl accossiors accurated.<bl/<bl /><ob />Thir sceernall we him westent and...<ould ser to bad....It incrip takinTrain Cycle: 2000\n",
            "Loss Per Character: 1.561315894126892\n",
            "Train Cycle: 2001\n",
            "Loss Per Character: 1.5157272815704346\n",
            "Train Cycle: 2002\n",
            "Loss Per Character: 1.4544296264648438\n",
            "Train Cycle: 2003\n",
            "Loss Per Character: 1.5139728784561157\n",
            "Train Cycle: 2004\n",
            "Loss Per Character: 1.4268909692764282\n",
            "Train Cycle: 2005\n",
            "Loss Per Character: 1.643298625946045\n",
            "Train Cycle: 2006\n",
            "Loss Per Character: 1.2837659120559692\n",
            "Train Cycle: 2007\n",
            "Loss Per Character: 1.296655297279358\n",
            "Train Cycle: 2008\n",
            "Loss Per Character: 1.62513267993927\n",
            "Train Cycle: 2009\n",
            "Loss Per Character: 1.3542752265930176\n",
            "Train Cycle: 2010\n",
            "Loss Per Character: 1.327420711517334\n",
            "Train Cycle: 2011\n",
            "Loss Per Character: 1.3874305486679077\n",
            "Train Cycle: 2012\n",
            "Loss Per Character: 1.3688374757766724\n",
            "Train Cycle: 2013\n",
            "Loss Per Character: 1.3437788486480713\n",
            "Train Cycle: 2014\n",
            "Loss Per Character: 1.4129072427749634\n",
            "Train Cycle: 2015\n",
            "Loss Per Character: 1.5477137565612793\n",
            "Train Cycle: 2016\n",
            "Loss Per Character: 1.4533758163452148\n",
            "Train Cycle: 2017\n",
            "Loss Per Character: 1.5169532299041748\n",
            "Train Cycle: 2018\n",
            "Loss Per Character: 1.7563470602035522\n",
            "Train Cycle: 2019\n",
            "Loss Per Character: 1.3797434568405151\n",
            "Train Cycle: 2020\n",
            "Loss Per Character: 1.5691522359848022\n",
            "Train Cycle: 2021\n",
            "Loss Per Character: 1.336726427078247\n",
            "Train Cycle: 2022\n",
            "Loss Per Character: 1.432902455329895\n",
            "Train Cycle: 2023\n",
            "Loss Per Character: 1.5860244035720825\n",
            "Train Cycle: 2024\n",
            "Loss Per Character: 1.4311579465866089\n",
            "Train Cycle: 2025\n",
            "Loss Per Character: 1.4015319347381592\n",
            "Train Cycle: 2026\n",
            "Loss Per Character: 1.437269687652588\n",
            "Train Cycle: 2027\n",
            "Loss Per Character: 1.4334027767181396\n",
            "Train Cycle: 2028\n",
            "Loss Per Character: 1.4250545501708984\n",
            "Train Cycle: 2029\n",
            "Loss Per Character: 1.3859338760375977\n",
            "Train Cycle: 2030\n",
            "Loss Per Character: 1.732874870300293\n",
            "Train Cycle: 2031\n",
            "Loss Per Character: 1.56558096408844\n",
            "Train Cycle: 2032\n",
            "Loss Per Character: 1.3854196071624756\n",
            "Train Cycle: 2033\n",
            "Loss Per Character: 1.4871275424957275\n",
            "Train Cycle: 2034\n",
            "Loss Per Character: 1.6565310955047607\n",
            "Train Cycle: 2035\n",
            "Loss Per Character: 1.5634496212005615\n",
            "Train Cycle: 2036\n",
            "Loss Per Character: 1.5185275077819824\n",
            "Train Cycle: 2037\n",
            "Loss Per Character: 1.4234495162963867\n",
            "Train Cycle: 2038\n",
            "Loss Per Character: 1.601923942565918\n",
            "Train Cycle: 2039\n",
            "Loss Per Character: 1.5872797966003418\n",
            "Train Cycle: 2040\n",
            "Loss Per Character: 1.317253589630127\n",
            "Train Cycle: 2041\n",
            "Loss Per Character: 1.537826657295227\n",
            "Train Cycle: 2042\n",
            "Loss Per Character: 1.4586659669876099\n",
            "Train Cycle: 2043\n",
            "Loss Per Character: 1.6018911600112915\n",
            "Train Cycle: 2044\n",
            "Loss Per Character: 1.4564179182052612\n",
            "Train Cycle: 2045\n",
            "Loss Per Character: 1.5144016742706299\n",
            "Train Cycle: 2046\n",
            "Loss Per Character: 1.4229156970977783\n",
            "Train Cycle: 2047\n",
            "Loss Per Character: 1.5862902402877808\n",
            "Train Cycle: 2048\n",
            "Loss Per Character: 1.3158369064331055\n",
            "Train Cycle: 2049\n",
            "Loss Per Character: 1.714127540588379\n",
            "! Thats aren it's!!!! A stop on solvions. Thats a bittlem of thas..<br/><br The movie it's. The chanchilar they. Alson into thim of case. They wend as tellivalish.<round while seemed to belines a man checks.. They. To maki mades toother thing. Als trTrain Cycle: 2050\n",
            "Loss Per Character: 1.4174555540084839\n",
            "Train Cycle: 2051\n",
            "Loss Per Character: 1.5187056064605713\n",
            "Train Cycle: 2052\n",
            "Loss Per Character: 1.5826561450958252\n",
            "Train Cycle: 2053\n",
            "Loss Per Character: 1.6035948991775513\n",
            "Train Cycle: 2054\n",
            "Loss Per Character: 1.6142542362213135\n",
            "Train Cycle: 2055\n",
            "Loss Per Character: 1.5651015043258667\n",
            "Train Cycle: 2056\n",
            "Loss Per Character: 1.2468175888061523\n",
            "Train Cycle: 2057\n",
            "Loss Per Character: 1.3860845565795898\n",
            "Train Cycle: 2058\n",
            "Loss Per Character: 1.6107151508331299\n",
            "Train Cycle: 2059\n",
            "Loss Per Character: 1.2470225095748901\n",
            "Train Cycle: 2060\n",
            "Loss Per Character: 1.513188123703003\n",
            "Train Cycle: 2061\n",
            "Loss Per Character: 1.4831408262252808\n",
            "Train Cycle: 2062\n",
            "Loss Per Character: 1.56991708278656\n",
            "Train Cycle: 2063\n",
            "Loss Per Character: 1.496951699256897\n",
            "Train Cycle: 2064\n",
            "Loss Per Character: 1.485994815826416\n",
            "Train Cycle: 2065\n",
            "Loss Per Character: 1.45485520362854\n",
            "Train Cycle: 2066\n",
            "Loss Per Character: 1.453108787536621\n",
            "Train Cycle: 2067\n",
            "Loss Per Character: 1.5903480052947998\n",
            "Train Cycle: 2068\n",
            "Loss Per Character: 1.2689262628555298\n",
            "Train Cycle: 2069\n",
            "Loss Per Character: 1.5853512287139893\n",
            "Train Cycle: 2070\n",
            "Loss Per Character: 1.5339277982711792\n",
            "Train Cycle: 2071\n",
            "Loss Per Character: 1.684921383857727\n",
            "Train Cycle: 2072\n",
            "Loss Per Character: 1.479077935218811\n",
            "Train Cycle: 2073\n",
            "Loss Per Character: 1.5600571632385254\n",
            "Train Cycle: 2074\n",
            "Loss Per Character: 1.4367603063583374\n",
            "Train Cycle: 2075\n",
            "Loss Per Character: 1.4821885824203491\n",
            "Train Cycle: 2076\n",
            "Loss Per Character: 1.525694727897644\n",
            "Train Cycle: 2077\n",
            "Loss Per Character: 1.3529289960861206\n",
            "Train Cycle: 2078\n",
            "Loss Per Character: 1.4490506649017334\n",
            "Train Cycle: 2079\n",
            "Loss Per Character: 1.3677353858947754\n",
            "Train Cycle: 2080\n",
            "Loss Per Character: 1.1760120391845703\n",
            "Train Cycle: 2081\n",
            "Loss Per Character: 1.2795071601867676\n",
            "Train Cycle: 2082\n",
            "Loss Per Character: 1.266480803489685\n",
            "Train Cycle: 2083\n",
            "Loss Per Character: 1.2090011835098267\n",
            "Train Cycle: 2084\n",
            "Loss Per Character: 1.4968465566635132\n",
            "Train Cycle: 2085\n",
            "Loss Per Character: 1.4885634183883667\n",
            "Train Cycle: 2086\n",
            "Loss Per Character: 1.5495489835739136\n",
            "Train Cycle: 2087\n",
            "Loss Per Character: 1.4358243942260742\n",
            "Train Cycle: 2088\n",
            "Loss Per Character: 1.3182820081710815\n",
            "Train Cycle: 2089\n",
            "Loss Per Character: 1.4596120119094849\n",
            "Train Cycle: 2090\n",
            "Loss Per Character: 1.3379125595092773\n",
            "Train Cycle: 2091\n",
            "Loss Per Character: 1.5768544673919678\n",
            "Train Cycle: 2092\n",
            "Loss Per Character: 1.3651732206344604\n",
            "Train Cycle: 2093\n",
            "Loss Per Character: 1.3763320446014404\n",
            "Train Cycle: 2094\n",
            "Loss Per Character: 1.3930915594100952\n",
            "Train Cycle: 2095\n",
            "Loss Per Character: 1.5218582153320312\n",
            "Train Cycle: 2096\n",
            "Loss Per Character: 1.3969929218292236\n",
            "Train Cycle: 2097\n",
            "Loss Per Character: 1.3836463689804077\n",
            "Train Cycle: 2098\n",
            "Loss Per Character: 1.3264122009277344\n",
            "Train Cycle: 2099\n",
            "Loss Per Character: 1.3994359970092773\n",
            "brella of thin off oring this ones of the movy is new that. That thirks a longs. I watckity too but. If it.<bo.<br /><blow is that. Treating is need a mile offinings..<br Too manices and seem and. I watch it's a poin of tirn or seem andody. It see thTrain Cycle: 2100\n",
            "Loss Per Character: 1.3350824117660522\n",
            "Train Cycle: 2101\n",
            "Loss Per Character: 1.2636277675628662\n",
            "Train Cycle: 2102\n",
            "Loss Per Character: 1.5693765878677368\n",
            "Train Cycle: 2103\n",
            "Loss Per Character: 1.462020754814148\n",
            "Train Cycle: 2104\n",
            "Loss Per Character: 1.4612329006195068\n",
            "Train Cycle: 2105\n",
            "Loss Per Character: 1.4386146068572998\n",
            "Train Cycle: 2106\n",
            "Loss Per Character: 1.369654893875122\n",
            "Train Cycle: 2107\n",
            "Loss Per Character: 1.4123497009277344\n",
            "Train Cycle: 2108\n",
            "Loss Per Character: 1.7825043201446533\n",
            "Train Cycle: 2109\n",
            "Loss Per Character: 1.736930012702942\n",
            "Train Cycle: 2110\n",
            "Loss Per Character: 1.8785873651504517\n",
            "Train Cycle: 2111\n",
            "Loss Per Character: 1.4845185279846191\n",
            "Train Cycle: 2112\n",
            "Loss Per Character: 1.4728094339370728\n",
            "Train Cycle: 2113\n",
            "Loss Per Character: 1.6525251865386963\n",
            "Train Cycle: 2114\n",
            "Loss Per Character: 1.5287901163101196\n",
            "Train Cycle: 2115\n",
            "Loss Per Character: 1.5347485542297363\n",
            "Train Cycle: 2116\n",
            "Loss Per Character: 1.519256353378296\n",
            "Train Cycle: 2117\n",
            "Loss Per Character: 1.473344326019287\n",
            "Train Cycle: 2118\n",
            "Loss Per Character: 1.2434659004211426\n",
            "Train Cycle: 2119\n",
            "Loss Per Character: 1.3970154523849487\n",
            "Train Cycle: 2120\n",
            "Loss Per Character: 1.517171025276184\n",
            "Train Cycle: 2121\n",
            "Loss Per Character: 1.3702441453933716\n",
            "Train Cycle: 2122\n",
            "Loss Per Character: 1.5606082677841187\n",
            "Train Cycle: 2123\n",
            "Loss Per Character: 1.6508305072784424\n",
            "Train Cycle: 2124\n",
            "Loss Per Character: 1.4879952669143677\n",
            "Train Cycle: 2125\n",
            "Loss Per Character: 1.547668695449829\n",
            "Train Cycle: 2126\n",
            "Loss Per Character: 1.4634854793548584\n",
            "Train Cycle: 2127\n",
            "Loss Per Character: 1.4455915689468384\n",
            "Train Cycle: 2128\n",
            "Loss Per Character: 1.5373218059539795\n",
            "Train Cycle: 2129\n",
            "Loss Per Character: 1.496846318244934\n",
            "Train Cycle: 2130\n",
            "Loss Per Character: 1.5245712995529175\n",
            "Train Cycle: 2131\n",
            "Loss Per Character: 1.5128425359725952\n",
            "Train Cycle: 2132\n",
            "Loss Per Character: 1.613205075263977\n",
            "Train Cycle: 2133\n",
            "Loss Per Character: 1.362002968788147\n",
            "Train Cycle: 2134\n",
            "Loss Per Character: 1.5074794292449951\n",
            "Train Cycle: 2135\n",
            "Loss Per Character: 1.4121336936950684\n",
            "Train Cycle: 2136\n",
            "Loss Per Character: 1.3678878545761108\n",
            "Train Cycle: 2137\n",
            "Loss Per Character: 1.4079525470733643\n",
            "Train Cycle: 2138\n",
            "Loss Per Character: 1.6170529127120972\n",
            "Train Cycle: 2139\n",
            "Loss Per Character: 1.3383933305740356\n",
            "Train Cycle: 2140\n",
            "Loss Per Character: 1.4545997381210327\n",
            "Train Cycle: 2141\n",
            "Loss Per Character: 1.5685243606567383\n",
            "Train Cycle: 2142\n",
            "Loss Per Character: 1.3999402523040771\n",
            "Train Cycle: 2143\n",
            "Loss Per Character: 1.3621890544891357\n",
            "Train Cycle: 2144\n",
            "Loss Per Character: 1.7345902919769287\n",
            "Train Cycle: 2145\n",
            "Loss Per Character: 1.6536177396774292\n",
            "Train Cycle: 2146\n",
            "Loss Per Character: 1.317570447921753\n",
            "Train Cycle: 2147\n",
            "Loss Per Character: 1.31789231300354\n",
            "Train Cycle: 2148\n",
            "Loss Per Character: 1.2734383344650269\n",
            "Train Cycle: 2149\n",
            "Loss Per Character: 1.2958086729049683\n",
            "Revath as the made.< /.<br /><orred. We well. The film waste oun a seriestal fine aspict and terry.<br This isn't talent forchine and when.. Theroun to takes off in termal seemed an evely sex somine. Will and terribles. Thirk watchinesthild thantsineTrain Cycle: 2150\n",
            "Loss Per Character: 1.2879667282104492\n",
            "Train Cycle: 2151\n",
            "Loss Per Character: 1.443011999130249\n",
            "Train Cycle: 2152\n",
            "Loss Per Character: 1.208419680595398\n",
            "Train Cycle: 2153\n",
            "Loss Per Character: 1.3879387378692627\n",
            "Train Cycle: 2154\n",
            "Loss Per Character: 1.5848841667175293\n",
            "Train Cycle: 2155\n",
            "Loss Per Character: 1.5167388916015625\n",
            "Train Cycle: 2156\n",
            "Loss Per Character: 1.332067847251892\n",
            "Train Cycle: 2157\n",
            "Loss Per Character: 1.4121527671813965\n",
            "Train Cycle: 2158\n",
            "Loss Per Character: 1.7933131456375122\n",
            "Train Cycle: 2159\n",
            "Loss Per Character: 1.6882818937301636\n",
            "Train Cycle: 2160\n",
            "Loss Per Character: 1.4579411745071411\n",
            "Train Cycle: 2161\n",
            "Loss Per Character: 1.4066758155822754\n",
            "Train Cycle: 2162\n",
            "Loss Per Character: 1.5383459329605103\n",
            "Train Cycle: 2163\n",
            "Loss Per Character: 1.3748866319656372\n",
            "Train Cycle: 2164\n",
            "Loss Per Character: 1.5929025411605835\n",
            "Train Cycle: 2165\n",
            "Loss Per Character: 1.7408725023269653\n",
            "Train Cycle: 2166\n",
            "Loss Per Character: 1.2255877256393433\n",
            "Train Cycle: 2167\n",
            "Loss Per Character: 1.3284245729446411\n",
            "Train Cycle: 2168\n",
            "Loss Per Character: 1.5074503421783447\n",
            "Train Cycle: 2169\n",
            "Loss Per Character: 1.5220286846160889\n",
            "Train Cycle: 2170\n",
            "Loss Per Character: 1.3458874225616455\n",
            "Train Cycle: 2171\n",
            "Loss Per Character: 1.5313153266906738\n",
            "Train Cycle: 2172\n",
            "Loss Per Character: 1.2679884433746338\n",
            "Train Cycle: 2173\n",
            "Loss Per Character: 1.3829026222229004\n",
            "Train Cycle: 2174\n",
            "Loss Per Character: 1.4754900932312012\n",
            "Train Cycle: 2175\n",
            "Loss Per Character: 1.3875410556793213\n",
            "Train Cycle: 2176\n",
            "Loss Per Character: 1.278931975364685\n",
            "Train Cycle: 2177\n",
            "Loss Per Character: 1.523892879486084\n",
            "Train Cycle: 2178\n",
            "Loss Per Character: 1.812406063079834\n",
            "Train Cycle: 2179\n",
            "Loss Per Character: 1.4311693906784058\n",
            "Train Cycle: 2180\n",
            "Loss Per Character: 1.651200771331787\n",
            "Train Cycle: 2181\n",
            "Loss Per Character: 1.60459566116333\n",
            "Train Cycle: 2182\n",
            "Loss Per Character: 1.6533868312835693\n",
            "Train Cycle: 2183\n",
            "Loss Per Character: 1.596798300743103\n",
            "Train Cycle: 2184\n",
            "Loss Per Character: 1.4467846155166626\n",
            "Train Cycle: 2185\n",
            "Loss Per Character: 1.5662598609924316\n",
            "Train Cycle: 2186\n",
            "Loss Per Character: 1.5412317514419556\n",
            "Train Cycle: 2187\n",
            "Loss Per Character: 1.4753642082214355\n",
            "Train Cycle: 2188\n",
            "Loss Per Character: 1.365211009979248\n",
            "Train Cycle: 2189\n",
            "Loss Per Character: 1.6718299388885498\n",
            "Train Cycle: 2190\n",
            "Loss Per Character: 1.6470191478729248\n",
            "Train Cycle: 2191\n",
            "Loss Per Character: 1.533485770225525\n",
            "Train Cycle: 2192\n",
            "Loss Per Character: 1.585860252380371\n",
            "Train Cycle: 2193\n",
            "Loss Per Character: 1.4508020877838135\n",
            "Train Cycle: 2194\n",
            "Loss Per Character: 1.4073352813720703\n",
            "Train Cycle: 2195\n",
            "Loss Per Character: 1.403948187828064\n",
            "Train Cycle: 2196\n",
            "Loss Per Character: 1.5173991918563843\n",
            "Train Cycle: 2197\n",
            "Loss Per Character: 1.4834836721420288\n",
            "Train Cycle: 2198\n",
            "Loss Per Character: 1.9309089183807373\n",
            "Train Cycle: 2199\n",
            "Loss Per Character: 1.6626428365707397\n",
            "Judosts. It watching hig sham that are thar tooks like a fails of this maki who'vangin an one who werk ofthen wife.<br.... The makeraser...<ririty tripedly).<rrow.. This isn't a filled)and it andong andomic in he we are supre the shated in a politionTrain Cycle: 2200\n",
            "Loss Per Character: 1.6925249099731445\n",
            "Train Cycle: 2201\n",
            "Loss Per Character: 1.480040431022644\n",
            "Train Cycle: 2202\n",
            "Loss Per Character: 1.5034992694854736\n",
            "Train Cycle: 2203\n",
            "Loss Per Character: 1.5584118366241455\n",
            "Train Cycle: 2204\n",
            "Loss Per Character: 1.6891300678253174\n",
            "Train Cycle: 2205\n",
            "Loss Per Character: 1.631873607635498\n",
            "Train Cycle: 2206\n",
            "Loss Per Character: 1.504254937171936\n",
            "Train Cycle: 2207\n",
            "Loss Per Character: 1.1834207773208618\n",
            "Train Cycle: 2208\n",
            "Loss Per Character: 1.5746527910232544\n",
            "Train Cycle: 2209\n",
            "Loss Per Character: 1.48802649974823\n",
            "Train Cycle: 2210\n",
            "Loss Per Character: 1.2462677955627441\n",
            "Train Cycle: 2211\n",
            "Loss Per Character: 1.4647451639175415\n",
            "Train Cycle: 2212\n",
            "Loss Per Character: 1.4842581748962402\n",
            "Train Cycle: 2213\n",
            "Loss Per Character: 1.1149221658706665\n",
            "Train Cycle: 2214\n",
            "Loss Per Character: 1.3830487728118896\n",
            "Train Cycle: 2215\n",
            "Loss Per Character: 1.408211588859558\n",
            "Train Cycle: 2216\n",
            "Loss Per Character: 1.4205982685089111\n",
            "Train Cycle: 2217\n",
            "Loss Per Character: 1.4958982467651367\n",
            "Train Cycle: 2218\n",
            "Loss Per Character: 1.4132097959518433\n",
            "Train Cycle: 2219\n",
            "Loss Per Character: 1.6439964771270752\n",
            "Train Cycle: 2220\n",
            "Loss Per Character: 1.5083894729614258\n",
            "Train Cycle: 2221\n",
            "Loss Per Character: 1.3371614217758179\n",
            "Train Cycle: 2222\n",
            "Loss Per Character: 1.4965059757232666\n",
            "Train Cycle: 2223\n",
            "Loss Per Character: 1.3954938650131226\n",
            "Train Cycle: 2224\n",
            "Loss Per Character: 1.3459583520889282\n",
            "Train Cycle: 2225\n",
            "Loss Per Character: 1.56354820728302\n",
            "Train Cycle: 2226\n",
            "Loss Per Character: 1.3209537267684937\n",
            "Train Cycle: 2227\n",
            "Loss Per Character: 1.5582809448242188\n",
            "Train Cycle: 2228\n",
            "Loss Per Character: 1.3277260065078735\n",
            "Train Cycle: 2229\n",
            "Loss Per Character: 1.3374221324920654\n",
            "Train Cycle: 2230\n",
            "Loss Per Character: 1.595836877822876\n",
            "Train Cycle: 2231\n",
            "Loss Per Character: 1.2447915077209473\n",
            "Train Cycle: 2232\n",
            "Loss Per Character: 1.5175200700759888\n",
            "Train Cycle: 2233\n",
            "Loss Per Character: 1.5327714681625366\n",
            "Train Cycle: 2234\n",
            "Loss Per Character: 1.4799363613128662\n",
            "Train Cycle: 2235\n",
            "Loss Per Character: 1.3509701490402222\n",
            "Train Cycle: 2236\n",
            "Loss Per Character: 1.5254335403442383\n",
            "Train Cycle: 2237\n",
            "Loss Per Character: 1.3705233335494995\n",
            "Train Cycle: 2238\n",
            "Loss Per Character: 1.4045978784561157\n",
            "Train Cycle: 2239\n",
            "Loss Per Character: 1.4365763664245605\n",
            "Train Cycle: 2240\n",
            "Loss Per Character: 1.550711989402771\n",
            "Train Cycle: 2241\n",
            "Loss Per Character: 1.4250664710998535\n",
            "Train Cycle: 2242\n",
            "Loss Per Character: 1.3634729385375977\n",
            "Train Cycle: 2243\n",
            "Loss Per Character: 1.4977807998657227\n",
            "Train Cycle: 2244\n",
            "Loss Per Character: 1.176735281944275\n",
            "Train Cycle: 2245\n",
            "Loss Per Character: 1.2723100185394287\n",
            "Train Cycle: 2246\n",
            "Loss Per Character: 1.3658775091171265\n",
            "Train Cycle: 2247\n",
            "Loss Per Character: 1.4286226034164429\n",
            "Train Cycle: 2248\n",
            "Loss Per Character: 1.3130731582641602\n",
            "Train Cycle: 2249\n",
            "Loss Per Character: 1.5039587020874023\n",
            "juddy.<bu to bat time.<ride isn't)<br //>I his funny'.<r THE. If it wasn'tentingle to ser about! Shot an admoting. Stars arounter as thing a fantal.. The first music.... ter story is. Streisand that is not..<ri/chood's comed as it's need theilerst toTrain Cycle: 2250\n",
            "Loss Per Character: 1.4576330184936523\n",
            "Train Cycle: 2251\n",
            "Loss Per Character: 1.2859413623809814\n",
            "Train Cycle: 2252\n",
            "Loss Per Character: 1.6653962135314941\n",
            "Train Cycle: 2253\n",
            "Loss Per Character: 1.6935784816741943\n",
            "Train Cycle: 2254\n",
            "Loss Per Character: 1.5786975622177124\n",
            "Train Cycle: 2255\n",
            "Loss Per Character: 1.6036127805709839\n",
            "Train Cycle: 2256\n",
            "Loss Per Character: 1.3218764066696167\n",
            "Train Cycle: 2257\n",
            "Loss Per Character: 1.4591871500015259\n",
            "Train Cycle: 2258\n",
            "Loss Per Character: 1.5455368757247925\n",
            "Train Cycle: 2259\n",
            "Loss Per Character: 1.3175806999206543\n",
            "Train Cycle: 2260\n",
            "Loss Per Character: 1.5514110326766968\n",
            "Train Cycle: 2261\n",
            "Loss Per Character: 1.3783984184265137\n",
            "Train Cycle: 2262\n",
            "Loss Per Character: 1.357431173324585\n",
            "Train Cycle: 2263\n",
            "Loss Per Character: 1.480111837387085\n",
            "Train Cycle: 2264\n",
            "Loss Per Character: 1.375181794166565\n",
            "Train Cycle: 2265\n",
            "Loss Per Character: 1.4964386224746704\n",
            "Train Cycle: 2266\n",
            "Loss Per Character: 1.4462878704071045\n",
            "Train Cycle: 2267\n",
            "Loss Per Character: 1.5389946699142456\n",
            "Train Cycle: 2268\n",
            "Loss Per Character: 1.664190649986267\n",
            "Train Cycle: 2269\n",
            "Loss Per Character: 1.4530049562454224\n",
            "Train Cycle: 2270\n",
            "Loss Per Character: 1.4054163694381714\n",
            "Train Cycle: 2271\n",
            "Loss Per Character: 1.6902427673339844\n",
            "Train Cycle: 2272\n",
            "Loss Per Character: 1.6995549201965332\n",
            "Train Cycle: 2273\n",
            "Loss Per Character: 1.508742094039917\n",
            "Train Cycle: 2274\n",
            "Loss Per Character: 1.47737455368042\n",
            "Train Cycle: 2275\n",
            "Loss Per Character: 1.4118658304214478\n",
            "Train Cycle: 2276\n",
            "Loss Per Character: 1.3591811656951904\n",
            "Train Cycle: 2277\n",
            "Loss Per Character: 1.4732158184051514\n",
            "Train Cycle: 2278\n",
            "Loss Per Character: 1.2278926372528076\n",
            "Train Cycle: 2279\n",
            "Loss Per Character: 1.5822654962539673\n",
            "Train Cycle: 2280\n",
            "Loss Per Character: 1.6402479410171509\n",
            "Train Cycle: 2281\n",
            "Loss Per Character: 1.4227538108825684\n",
            "Train Cycle: 2282\n",
            "Loss Per Character: 1.4879910945892334\n",
            "Train Cycle: 2283\n",
            "Loss Per Character: 1.5041937828063965\n",
            "Train Cycle: 2284\n",
            "Loss Per Character: 1.4257375001907349\n",
            "Train Cycle: 2285\n",
            "Loss Per Character: 1.5064884424209595\n",
            "Train Cycle: 2286\n",
            "Loss Per Character: 1.4267901182174683\n",
            "Train Cycle: 2287\n",
            "Loss Per Character: 1.296480417251587\n",
            "Train Cycle: 2288\n",
            "Loss Per Character: 1.4315321445465088\n",
            "Train Cycle: 2289\n",
            "Loss Per Character: 1.4564638137817383\n",
            "Train Cycle: 2290\n",
            "Loss Per Character: 1.7373909950256348\n",
            "Train Cycle: 2291\n",
            "Loss Per Character: 1.5082498788833618\n",
            "Train Cycle: 2292\n",
            "Loss Per Character: 1.541528582572937\n",
            "Train Cycle: 2293\n",
            "Loss Per Character: 1.3687292337417603\n",
            "Train Cycle: 2294\n",
            "Loss Per Character: 1.357325792312622\n",
            "Train Cycle: 2295\n",
            "Loss Per Character: 1.5220481157302856\n",
            "Train Cycle: 2296\n",
            "Loss Per Character: 1.3242930173873901\n",
            "Train Cycle: 2297\n",
            "Loss Per Character: 1.2167733907699585\n",
            "Train Cycle: 2298\n",
            "Loss Per Character: 1.3893578052520752\n",
            "Train Cycle: 2299\n",
            "Loss Per Character: 1.2366862297058105\n",
            "kyoughablish. Think in an isnead. All anocestencipate anyond to terribin then seemed! Tooded out trists). Altoo this:\"... This movin's armanizabit store. Iffects) and a serioly.. to seat and along and therouldn't had sentine triely. The mad and.<br /Train Cycle: 2300\n",
            "Loss Per Character: 1.6650805473327637\n",
            "Train Cycle: 2301\n",
            "Loss Per Character: 1.3968430757522583\n",
            "Train Cycle: 2302\n",
            "Loss Per Character: 1.3915036916732788\n",
            "Train Cycle: 2303\n",
            "Loss Per Character: 1.5348726511001587\n",
            "Train Cycle: 2304\n",
            "Loss Per Character: 1.3817896842956543\n",
            "Train Cycle: 2305\n",
            "Loss Per Character: 1.5248039960861206\n",
            "Train Cycle: 2306\n",
            "Loss Per Character: 1.5825797319412231\n",
            "Train Cycle: 2307\n",
            "Loss Per Character: 1.5830713510513306\n",
            "Train Cycle: 2308\n",
            "Loss Per Character: 1.3070249557495117\n",
            "Train Cycle: 2309\n",
            "Loss Per Character: 1.4421358108520508\n",
            "Train Cycle: 2310\n",
            "Loss Per Character: 1.3769153356552124\n",
            "Train Cycle: 2311\n",
            "Loss Per Character: 1.4626671075820923\n",
            "Train Cycle: 2312\n",
            "Loss Per Character: 1.493853211402893\n",
            "Train Cycle: 2313\n",
            "Loss Per Character: 1.2676352262496948\n",
            "Train Cycle: 2314\n",
            "Loss Per Character: 1.1650934219360352\n",
            "Train Cycle: 2315\n",
            "Loss Per Character: 1.4385992288589478\n",
            "Train Cycle: 2316\n",
            "Loss Per Character: 1.4527554512023926\n",
            "Train Cycle: 2317\n",
            "Loss Per Character: 1.2844902276992798\n",
            "Train Cycle: 2318\n",
            "Loss Per Character: 1.279267430305481\n",
            "Train Cycle: 2319\n",
            "Loss Per Character: 1.4727766513824463\n",
            "Train Cycle: 2320\n",
            "Loss Per Character: 1.3610528707504272\n",
            "Train Cycle: 2321\n",
            "Loss Per Character: 1.4855047464370728\n",
            "Train Cycle: 2322\n",
            "Loss Per Character: 1.4036086797714233\n",
            "Train Cycle: 2323\n",
            "Loss Per Character: 1.356473684310913\n",
            "Train Cycle: 2324\n",
            "Loss Per Character: 1.3242570161819458\n",
            "Train Cycle: 2325\n",
            "Loss Per Character: 1.4644829034805298\n",
            "Train Cycle: 2326\n",
            "Loss Per Character: 1.4262670278549194\n",
            "Train Cycle: 2327\n",
            "Loss Per Character: 1.3582040071487427\n",
            "Train Cycle: 2328\n",
            "Loss Per Character: 1.1849759817123413\n",
            "Train Cycle: 2329\n",
            "Loss Per Character: 1.399551510810852\n",
            "Train Cycle: 2330\n",
            "Loss Per Character: 1.636716604232788\n",
            "Train Cycle: 2331\n",
            "Loss Per Character: 1.7420941591262817\n",
            "Train Cycle: 2332\n",
            "Loss Per Character: 1.4947963953018188\n",
            "Train Cycle: 2333\n",
            "Loss Per Character: 1.4761357307434082\n",
            "Train Cycle: 2334\n",
            "Loss Per Character: 1.7034024000167847\n",
            "Train Cycle: 2335\n",
            "Loss Per Character: 1.5837974548339844\n",
            "Train Cycle: 2336\n",
            "Loss Per Character: 1.4937005043029785\n",
            "Train Cycle: 2337\n",
            "Loss Per Character: 1.275314450263977\n",
            "Train Cycle: 2338\n",
            "Loss Per Character: 1.4070202112197876\n",
            "Train Cycle: 2339\n",
            "Loss Per Character: 1.495343565940857\n",
            "Train Cycle: 2340\n",
            "Loss Per Character: 1.2875651121139526\n",
            "Train Cycle: 2341\n",
            "Loss Per Character: 1.5501865148544312\n",
            "Train Cycle: 2342\n",
            "Loss Per Character: 1.388782262802124\n",
            "Train Cycle: 2343\n",
            "Loss Per Character: 1.720289707183838\n",
            "Train Cycle: 2344\n",
            "Loss Per Character: 1.3857091665267944\n",
            "Train Cycle: 2345\n",
            "Loss Per Character: 1.5052194595336914\n",
            "Train Cycle: 2346\n",
            "Loss Per Character: 1.2551530599594116\n",
            "Train Cycle: 2347\n",
            "Loss Per Character: 1.7060602903366089\n",
            "Train Cycle: 2348\n",
            "Loss Per Character: 1.3426952362060547\n",
            "Train Cycle: 2349\n",
            "Loss Per Character: 1.2792370319366455\n",
            "\" instemencistatine... therowind to traise it' wasn't...<ou cast anyoutio servion. A finds.<bl tograles. I have. An it intone toot than allow any miss are to belook the woods there's no way the misisures it's anow heresendent in helf its someof way tTrain Cycle: 2350\n",
            "Loss Per Character: 1.3118635416030884\n",
            "Train Cycle: 2351\n",
            "Loss Per Character: 1.453815221786499\n",
            "Train Cycle: 2352\n",
            "Loss Per Character: 1.744267225265503\n",
            "Train Cycle: 2353\n",
            "Loss Per Character: 1.3746302127838135\n",
            "Train Cycle: 2354\n",
            "Loss Per Character: 1.3381843566894531\n",
            "Train Cycle: 2355\n",
            "Loss Per Character: 1.200077772140503\n",
            "Train Cycle: 2356\n",
            "Loss Per Character: 1.1078486442565918\n",
            "Train Cycle: 2357\n",
            "Loss Per Character: 1.3266721963882446\n",
            "Train Cycle: 2358\n",
            "Loss Per Character: 1.4169760942459106\n",
            "Train Cycle: 2359\n",
            "Loss Per Character: 1.3987452983856201\n",
            "Train Cycle: 2360\n",
            "Loss Per Character: 1.4849302768707275\n",
            "Train Cycle: 2361\n",
            "Loss Per Character: 1.4642318487167358\n",
            "Train Cycle: 2362\n",
            "Loss Per Character: 1.4013205766677856\n",
            "Train Cycle: 2363\n",
            "Loss Per Character: 1.5810319185256958\n",
            "Train Cycle: 2364\n",
            "Loss Per Character: 1.523356556892395\n",
            "Train Cycle: 2365\n",
            "Loss Per Character: 1.473913550376892\n",
            "Train Cycle: 2366\n",
            "Loss Per Character: 1.4561549425125122\n",
            "Train Cycle: 2367\n",
            "Loss Per Character: 1.5672760009765625\n",
            "Train Cycle: 2368\n",
            "Loss Per Character: 1.6733697652816772\n",
            "Train Cycle: 2369\n",
            "Loss Per Character: 1.1293764114379883\n",
            "Train Cycle: 2370\n",
            "Loss Per Character: 1.2407509088516235\n",
            "Train Cycle: 2371\n",
            "Loss Per Character: 1.2819510698318481\n",
            "Train Cycle: 2372\n",
            "Loss Per Character: 1.3737504482269287\n",
            "Train Cycle: 2373\n",
            "Loss Per Character: 1.2231111526489258\n",
            "Train Cycle: 2374\n",
            "Loss Per Character: 1.2081921100616455\n",
            "Train Cycle: 2375\n",
            "Loss Per Character: 1.248799204826355\n",
            "Train Cycle: 2376\n",
            "Loss Per Character: 1.333465337753296\n",
            "Train Cycle: 2377\n",
            "Loss Per Character: 1.4593772888183594\n",
            "Train Cycle: 2378\n",
            "Loss Per Character: 1.6640053987503052\n",
            "Train Cycle: 2379\n",
            "Loss Per Character: 1.3996491432189941\n",
            "Train Cycle: 2380\n",
            "Loss Per Character: 1.6150703430175781\n",
            "Train Cycle: 2381\n",
            "Loss Per Character: 1.400918960571289\n",
            "Train Cycle: 2382\n",
            "Loss Per Character: 1.4052128791809082\n",
            "Train Cycle: 2383\n",
            "Loss Per Character: 1.4733550548553467\n",
            "Train Cycle: 2384\n",
            "Loss Per Character: 1.5528162717819214\n",
            "Train Cycle: 2385\n",
            "Loss Per Character: 1.3879703283309937\n",
            "Train Cycle: 2386\n",
            "Loss Per Character: 1.2209445238113403\n",
            "Train Cycle: 2387\n",
            "Loss Per Character: 1.546427607536316\n",
            "Train Cycle: 2388\n",
            "Loss Per Character: 1.654876470565796\n",
            "Train Cycle: 2389\n",
            "Loss Per Character: 1.3137487173080444\n",
            "Train Cycle: 2390\n",
            "Loss Per Character: 1.5066287517547607\n",
            "Train Cycle: 2391\n",
            "Loss Per Character: 1.578200101852417\n",
            "Train Cycle: 2392\n",
            "Loss Per Character: 1.4720075130462646\n",
            "Train Cycle: 2393\n",
            "Loss Per Character: 1.4288543462753296\n",
            "Train Cycle: 2394\n",
            "Loss Per Character: 1.3147263526916504\n",
            "Train Cycle: 2395\n",
            "Loss Per Character: 1.3951283693313599\n",
            "Train Cycle: 2396\n",
            "Loss Per Character: 1.242028832435608\n",
            "Train Cycle: 2397\n",
            "Loss Per Character: 1.2740237712860107\n",
            "Train Cycle: 2398\n",
            "Loss Per Character: 1.5673505067825317\n",
            "Train Cycle: 2399\n",
            "Loss Per Character: 1.2946925163269043\n",
            "Xs and this.<br //> ous anyo must be states this isn't) thirdst terrestine....<ourserviest filt tite and stoppies are. Troub thing... To standatic aboriced at thir oriers at likes that in telical charactes terming stuped..<bl to the monster of any stTrain Cycle: 2400\n",
            "Loss Per Character: 1.3276619911193848\n",
            "Train Cycle: 2401\n",
            "Loss Per Character: 1.4764069318771362\n",
            "Train Cycle: 2402\n",
            "Loss Per Character: 1.7244181632995605\n",
            "Train Cycle: 2403\n",
            "Loss Per Character: 1.4558030366897583\n",
            "Train Cycle: 2404\n",
            "Loss Per Character: 1.3798998594284058\n",
            "Train Cycle: 2405\n",
            "Loss Per Character: 1.401947021484375\n",
            "Train Cycle: 2406\n",
            "Loss Per Character: 1.415536642074585\n",
            "Train Cycle: 2407\n",
            "Loss Per Character: 1.3786486387252808\n",
            "Train Cycle: 2408\n",
            "Loss Per Character: 1.2908121347427368\n",
            "Train Cycle: 2409\n",
            "Loss Per Character: 1.4919716119766235\n",
            "Train Cycle: 2410\n",
            "Loss Per Character: 1.5205806493759155\n",
            "Train Cycle: 2411\n",
            "Loss Per Character: 1.3723971843719482\n",
            "Train Cycle: 2412\n",
            "Loss Per Character: 1.3315000534057617\n",
            "Train Cycle: 2413\n",
            "Loss Per Character: 1.5616700649261475\n",
            "Train Cycle: 2414\n",
            "Loss Per Character: 1.5534303188323975\n",
            "Train Cycle: 2415\n",
            "Loss Per Character: 1.2851006984710693\n",
            "Train Cycle: 2416\n",
            "Loss Per Character: 1.4622353315353394\n",
            "Train Cycle: 2417\n",
            "Loss Per Character: 1.5051376819610596\n",
            "Train Cycle: 2418\n",
            "Loss Per Character: 1.551354169845581\n",
            "Train Cycle: 2419\n",
            "Loss Per Character: 1.3275463581085205\n",
            "Train Cycle: 2420\n",
            "Loss Per Character: 1.569573998451233\n",
            "Train Cycle: 2421\n",
            "Loss Per Character: 1.6023985147476196\n",
            "Train Cycle: 2422\n",
            "Loss Per Character: 1.6492069959640503\n",
            "Train Cycle: 2423\n",
            "Loss Per Character: 1.4821233749389648\n",
            "Train Cycle: 2424\n",
            "Loss Per Character: 1.1819026470184326\n",
            "Train Cycle: 2425\n",
            "Loss Per Character: 1.3226209878921509\n",
            "Train Cycle: 2426\n",
            "Loss Per Character: 1.4151462316513062\n",
            "Train Cycle: 2427\n",
            "Loss Per Character: 1.3730043172836304\n",
            "Train Cycle: 2428\n",
            "Loss Per Character: 1.5772219896316528\n",
            "Train Cycle: 2429\n",
            "Loss Per Character: 1.1633052825927734\n",
            "Train Cycle: 2430\n",
            "Loss Per Character: 1.2435860633850098\n",
            "Train Cycle: 2431\n",
            "Loss Per Character: 1.2789477109909058\n",
            "Train Cycle: 2432\n",
            "Loss Per Character: 1.5079426765441895\n",
            "Train Cycle: 2433\n",
            "Loss Per Character: 1.1580097675323486\n",
            "Train Cycle: 2434\n",
            "Loss Per Character: 1.3119064569473267\n",
            "Train Cycle: 2435\n",
            "Loss Per Character: 1.5432416200637817\n",
            "Train Cycle: 2436\n",
            "Loss Per Character: 1.280483603477478\n",
            "Train Cycle: 2437\n",
            "Loss Per Character: 1.2461259365081787\n",
            "Train Cycle: 2438\n",
            "Loss Per Character: 1.4199748039245605\n",
            "Train Cycle: 2439\n",
            "Loss Per Character: 1.2983450889587402\n",
            "Train Cycle: 2440\n",
            "Loss Per Character: 1.3534075021743774\n",
            "Train Cycle: 2441\n",
            "Loss Per Character: 1.380068302154541\n",
            "Train Cycle: 2442\n",
            "Loss Per Character: 1.2704355716705322\n",
            "Train Cycle: 2443\n",
            "Loss Per Character: 1.312618613243103\n",
            "Train Cycle: 2444\n",
            "Loss Per Character: 1.248813271522522\n",
            "Train Cycle: 2445\n",
            "Loss Per Character: 1.4196783304214478\n",
            "Train Cycle: 2446\n",
            "Loss Per Character: 1.338964581489563\n",
            "Train Cycle: 2447\n",
            "Loss Per Character: 1.3107069730758667\n",
            "Train Cycle: 2448\n",
            "Loss Per Character: 1.4488000869750977\n",
            "Train Cycle: 2449\n",
            "Loss Per Character: 1.501786470413208\n",
            "Don' oftal actors)<br / /><r stup isno an out. A considentally) ands a main orders an an and this is.<br that' alson and and they. The orice togen to takes\").<bl/> Thir one. And intritic burns... This!).<or / was a monest at. To betteres.<or taks..<bTrain Cycle: 2450\n",
            "Loss Per Character: 1.3927409648895264\n",
            "Train Cycle: 2451\n",
            "Loss Per Character: 1.2819229364395142\n",
            "Train Cycle: 2452\n",
            "Loss Per Character: 1.552960753440857\n",
            "Train Cycle: 2453\n",
            "Loss Per Character: 1.5166264772415161\n",
            "Train Cycle: 2454\n",
            "Loss Per Character: 1.5054688453674316\n",
            "Train Cycle: 2455\n",
            "Loss Per Character: 1.51347815990448\n",
            "Train Cycle: 2456\n",
            "Loss Per Character: 1.4547768831253052\n",
            "Train Cycle: 2457\n",
            "Loss Per Character: 1.5643898248672485\n",
            "Train Cycle: 2458\n",
            "Loss Per Character: 1.607717752456665\n",
            "Train Cycle: 2459\n",
            "Loss Per Character: 1.454014539718628\n",
            "Train Cycle: 2460\n",
            "Loss Per Character: 1.6790837049484253\n",
            "Train Cycle: 2461\n",
            "Loss Per Character: 1.487156867980957\n",
            "Train Cycle: 2462\n",
            "Loss Per Character: 1.5714060068130493\n",
            "Train Cycle: 2463\n",
            "Loss Per Character: 1.3580825328826904\n",
            "Train Cycle: 2464\n",
            "Loss Per Character: 1.4315587282180786\n",
            "Train Cycle: 2465\n",
            "Loss Per Character: 1.4128700494766235\n",
            "Train Cycle: 2466\n",
            "Loss Per Character: 1.5713117122650146\n",
            "Train Cycle: 2467\n",
            "Loss Per Character: 1.4580475091934204\n",
            "Train Cycle: 2468\n",
            "Loss Per Character: 1.3976935148239136\n",
            "Train Cycle: 2469\n",
            "Loss Per Character: 1.5812832117080688\n",
            "Train Cycle: 2470\n",
            "Loss Per Character: 1.2862865924835205\n",
            "Train Cycle: 2471\n",
            "Loss Per Character: 1.2878679037094116\n",
            "Train Cycle: 2472\n",
            "Loss Per Character: 1.3384310007095337\n",
            "Train Cycle: 2473\n",
            "Loss Per Character: 1.090358018875122\n",
            "Train Cycle: 2474\n",
            "Loss Per Character: 1.3126267194747925\n",
            "Train Cycle: 2475\n",
            "Loss Per Character: 1.5366909503936768\n",
            "Train Cycle: 2476\n",
            "Loss Per Character: 1.376556158065796\n",
            "Train Cycle: 2477\n",
            "Loss Per Character: 1.288176417350769\n",
            "Train Cycle: 2478\n",
            "Loss Per Character: 1.6670823097229004\n",
            "Train Cycle: 2479\n",
            "Loss Per Character: 1.3782827854156494\n",
            "Train Cycle: 2480\n",
            "Loss Per Character: 1.67387855052948\n",
            "Train Cycle: 2481\n",
            "Loss Per Character: 1.5574103593826294\n",
            "Train Cycle: 2482\n",
            "Loss Per Character: 1.4455089569091797\n",
            "Train Cycle: 2483\n",
            "Loss Per Character: 1.66764497756958\n",
            "Train Cycle: 2484\n",
            "Loss Per Character: 1.3096505403518677\n",
            "Train Cycle: 2485\n",
            "Loss Per Character: 1.3841190338134766\n",
            "Train Cycle: 2486\n",
            "Loss Per Character: 1.4921159744262695\n",
            "Train Cycle: 2487\n",
            "Loss Per Character: 1.494850516319275\n",
            "Train Cycle: 2488\n",
            "Loss Per Character: 1.6013530492782593\n",
            "Train Cycle: 2489\n",
            "Loss Per Character: 1.5396029949188232\n",
            "Train Cycle: 2490\n",
            "Loss Per Character: 1.456238865852356\n",
            "Train Cycle: 2491\n",
            "Loss Per Character: 1.488067388534546\n",
            "Train Cycle: 2492\n",
            "Loss Per Character: 1.4296658039093018\n",
            "Train Cycle: 2493\n",
            "Loss Per Character: 1.479071855545044\n",
            "Train Cycle: 2494\n",
            "Loss Per Character: 1.4589399099349976\n",
            "Train Cycle: 2495\n",
            "Loss Per Character: 1.3144264221191406\n",
            "Train Cycle: 2496\n",
            "Loss Per Character: 1.4238438606262207\n",
            "Train Cycle: 2497\n",
            "Loss Per Character: 1.3043365478515625\n",
            "Train Cycle: 2498\n",
            "Loss Per Character: 1.3377467393875122\n",
            "Train Cycle: 2499\n",
            "Loss Per Character: 1.4503841400146484\n",
            "Get anything....<row is somatents)and. An all on that. An its succast tookshin is so much of hig basily bet the serry' supprises and supe on and...<b /><or/>I wis much. Tries. The othens. If tell thing this is that that a gots off. A fains... It'llooTrain Cycle: 2500\n",
            "Loss Per Character: 1.5499112606048584\n",
            "Train Cycle: 2501\n",
            "Loss Per Character: 1.2397847175598145\n",
            "Train Cycle: 2502\n",
            "Loss Per Character: 1.4169336557388306\n",
            "Train Cycle: 2503\n",
            "Loss Per Character: 1.5577497482299805\n",
            "Train Cycle: 2504\n",
            "Loss Per Character: 1.575664758682251\n",
            "Train Cycle: 2505\n",
            "Loss Per Character: 1.7011393308639526\n",
            "Train Cycle: 2506\n",
            "Loss Per Character: 1.9023730754852295\n",
            "Train Cycle: 2507\n",
            "Loss Per Character: 1.4643545150756836\n",
            "Train Cycle: 2508\n",
            "Loss Per Character: 1.4368958473205566\n",
            "Train Cycle: 2509\n",
            "Loss Per Character: 1.6639708280563354\n",
            "Train Cycle: 2510\n",
            "Loss Per Character: 1.5090612173080444\n",
            "Train Cycle: 2511\n",
            "Loss Per Character: 1.3565117120742798\n",
            "Train Cycle: 2512\n",
            "Loss Per Character: 1.3550907373428345\n",
            "Train Cycle: 2513\n",
            "Loss Per Character: 1.5766706466674805\n",
            "Train Cycle: 2514\n",
            "Loss Per Character: 1.2783253192901611\n",
            "Train Cycle: 2515\n",
            "Loss Per Character: 1.3014639616012573\n",
            "Train Cycle: 2516\n",
            "Loss Per Character: 1.5299975872039795\n",
            "Train Cycle: 2517\n",
            "Loss Per Character: 1.3333070278167725\n",
            "Train Cycle: 2518\n",
            "Loss Per Character: 1.205940842628479\n",
            "Train Cycle: 2519\n",
            "Loss Per Character: 1.359930157661438\n",
            "Train Cycle: 2520\n",
            "Loss Per Character: 1.1900362968444824\n",
            "Train Cycle: 2521\n",
            "Loss Per Character: 1.2821688652038574\n",
            "Train Cycle: 2522\n",
            "Loss Per Character: 1.4027918577194214\n",
            "Train Cycle: 2523\n",
            "Loss Per Character: 1.335734248161316\n",
            "Train Cycle: 2524\n",
            "Loss Per Character: 1.4139964580535889\n",
            "Train Cycle: 2525\n",
            "Loss Per Character: 1.2912981510162354\n",
            "Train Cycle: 2526\n",
            "Loss Per Character: 1.5700246095657349\n",
            "Train Cycle: 2527\n",
            "Loss Per Character: 1.2091115713119507\n",
            "Train Cycle: 2528\n",
            "Loss Per Character: 1.6408103704452515\n",
            "Train Cycle: 2529\n",
            "Loss Per Character: 1.2558956146240234\n",
            "Train Cycle: 2530\n",
            "Loss Per Character: 1.1850584745407104\n",
            "Train Cycle: 2531\n",
            "Loss Per Character: 1.4788366556167603\n",
            "Train Cycle: 2532\n",
            "Loss Per Character: 1.3227894306182861\n",
            "Train Cycle: 2533\n",
            "Loss Per Character: 1.4273382425308228\n",
            "Train Cycle: 2534\n",
            "Loss Per Character: 1.5593582391738892\n",
            "Train Cycle: 2535\n",
            "Loss Per Character: 1.1797200441360474\n",
            "Train Cycle: 2536\n",
            "Loss Per Character: 1.4957016706466675\n",
            "Train Cycle: 2537\n",
            "Loss Per Character: 1.2621421813964844\n",
            "Train Cycle: 2538\n",
            "Loss Per Character: 1.3890495300292969\n",
            "Train Cycle: 2539\n",
            "Loss Per Character: 1.1345518827438354\n",
            "Train Cycle: 2540\n",
            "Loss Per Character: 1.3085389137268066\n",
            "Train Cycle: 2541\n",
            "Loss Per Character: 1.3767178058624268\n",
            "Train Cycle: 2542\n",
            "Loss Per Character: 1.1689023971557617\n",
            "Train Cycle: 2543\n",
            "Loss Per Character: 1.2809709310531616\n",
            "Train Cycle: 2544\n",
            "Loss Per Character: 1.5824596881866455\n",
            "Train Cycle: 2545\n",
            "Loss Per Character: 1.4282307624816895\n",
            "Train Cycle: 2546\n",
            "Loss Per Character: 1.8820935487747192\n",
            "Train Cycle: 2547\n",
            "Loss Per Character: 1.2190954685211182\n",
            "Train Cycle: 2548\n",
            "Loss Per Character: 1.572540044784546\n",
            "Train Cycle: 2549\n",
            "Loss Per Character: 1.5244678258895874\n",
            "kwere. Anyogage to tell. Allowis moving. There is.<but I'mestedioct that top. Anot of tillines a but I won't believe insteme of coull an ame a bad made an ameant atto trasticatold... It serior of. And I firsh.<br / acreary) aren's trashed the filthe Train Cycle: 2550\n",
            "Loss Per Character: 1.4388233423233032\n",
            "Train Cycle: 2551\n",
            "Loss Per Character: 1.5275343656539917\n",
            "Train Cycle: 2552\n",
            "Loss Per Character: 1.4768050909042358\n",
            "Train Cycle: 2553\n",
            "Loss Per Character: 1.566055417060852\n",
            "Train Cycle: 2554\n",
            "Loss Per Character: 1.4938721656799316\n",
            "Train Cycle: 2555\n",
            "Loss Per Character: 1.4375410079956055\n",
            "Train Cycle: 2556\n",
            "Loss Per Character: 1.538940668106079\n",
            "Train Cycle: 2557\n",
            "Loss Per Character: 1.2786643505096436\n",
            "Train Cycle: 2558\n",
            "Loss Per Character: 1.319854497909546\n",
            "Train Cycle: 2559\n",
            "Loss Per Character: 1.5258264541625977\n",
            "Train Cycle: 2560\n",
            "Loss Per Character: 1.25995671749115\n",
            "Train Cycle: 2561\n",
            "Loss Per Character: 1.895530343055725\n",
            "Train Cycle: 2562\n",
            "Loss Per Character: 1.474851369857788\n",
            "Train Cycle: 2563\n",
            "Loss Per Character: 1.4980885982513428\n",
            "Train Cycle: 2564\n",
            "Loss Per Character: 1.370383620262146\n",
            "Train Cycle: 2565\n",
            "Loss Per Character: 1.6290862560272217\n",
            "Train Cycle: 2566\n",
            "Loss Per Character: 1.207666039466858\n",
            "Train Cycle: 2567\n",
            "Loss Per Character: 1.0498820543289185\n",
            "Train Cycle: 2568\n",
            "Loss Per Character: 1.496924877166748\n",
            "Train Cycle: 2569\n",
            "Loss Per Character: 1.3254140615463257\n",
            "Train Cycle: 2570\n",
            "Loss Per Character: 1.6828724145889282\n",
            "Train Cycle: 2571\n",
            "Loss Per Character: 1.314171314239502\n",
            "Train Cycle: 2572\n",
            "Loss Per Character: 1.7488939762115479\n",
            "Train Cycle: 2573\n",
            "Loss Per Character: 1.139496922492981\n",
            "Train Cycle: 2574\n",
            "Loss Per Character: 1.6687785387039185\n",
            "Train Cycle: 2575\n",
            "Loss Per Character: 1.448256254196167\n",
            "Train Cycle: 2576\n",
            "Loss Per Character: 1.336309552192688\n",
            "Train Cycle: 2577\n",
            "Loss Per Character: 1.7483521699905396\n",
            "Train Cycle: 2578\n",
            "Loss Per Character: 1.5616798400878906\n",
            "Train Cycle: 2579\n",
            "Loss Per Character: 1.4291441440582275\n",
            "Train Cycle: 2580\n",
            "Loss Per Character: 1.2587106227874756\n",
            "Train Cycle: 2581\n",
            "Loss Per Character: 1.384974718093872\n",
            "Train Cycle: 2582\n",
            "Loss Per Character: 1.433573603630066\n",
            "Train Cycle: 2583\n",
            "Loss Per Character: 1.3704323768615723\n",
            "Train Cycle: 2584\n",
            "Loss Per Character: 1.719191551208496\n",
            "Train Cycle: 2585\n",
            "Loss Per Character: 1.4718260765075684\n",
            "Train Cycle: 2586\n",
            "Loss Per Character: 1.6937260627746582\n",
            "Train Cycle: 2587\n",
            "Loss Per Character: 1.4301347732543945\n",
            "Train Cycle: 2588\n",
            "Loss Per Character: 1.4505232572555542\n",
            "Train Cycle: 2589\n",
            "Loss Per Character: 1.3291913270950317\n",
            "Train Cycle: 2590\n",
            "Loss Per Character: 1.6055059432983398\n",
            "Train Cycle: 2591\n",
            "Loss Per Character: 1.3463871479034424\n",
            "Train Cycle: 2592\n",
            "Loss Per Character: 1.3563885688781738\n",
            "Train Cycle: 2593\n",
            "Loss Per Character: 1.503836750984192\n",
            "Train Cycle: 2594\n",
            "Loss Per Character: 1.2565138339996338\n",
            "Train Cycle: 2595\n",
            "Loss Per Character: 1.3580398559570312\n",
            "Train Cycle: 2596\n",
            "Loss Per Character: 1.41561758518219\n",
            "Train Cycle: 2597\n",
            "Loss Per Character: 1.2026046514511108\n",
            "Train Cycle: 2598\n",
            "Loss Per Character: 1.3809648752212524\n",
            "Train Cycle: 2599\n",
            "Loss Per Character: 1.275979995727539\n",
            ":</>If yar terribely so trieds..<but there'vies to stoppy. I wanted it would stand iffo televis as introdue offerstant to belief. I was so mone on herois therrings whas arriedd anony. Thinsters who'vide it wonderedline to badles.. a doubly welkind teTrain Cycle: 2600\n",
            "Loss Per Character: 1.4280823469161987\n",
            "Train Cycle: 2601\n",
            "Loss Per Character: 1.525515079498291\n",
            "Train Cycle: 2602\n",
            "Loss Per Character: 1.544746994972229\n",
            "Train Cycle: 2603\n",
            "Loss Per Character: 1.4133726358413696\n",
            "Train Cycle: 2604\n",
            "Loss Per Character: 1.4791849851608276\n",
            "Train Cycle: 2605\n",
            "Loss Per Character: 1.4054884910583496\n",
            "Train Cycle: 2606\n",
            "Loss Per Character: 1.318543553352356\n",
            "Train Cycle: 2607\n",
            "Loss Per Character: 1.4320662021636963\n",
            "Train Cycle: 2608\n",
            "Loss Per Character: 1.5972975492477417\n",
            "Train Cycle: 2609\n",
            "Loss Per Character: 1.3636502027511597\n",
            "Train Cycle: 2610\n",
            "Loss Per Character: 1.3114914894104004\n",
            "Train Cycle: 2611\n",
            "Loss Per Character: 1.3426698446273804\n",
            "Train Cycle: 2612\n",
            "Loss Per Character: 1.4810484647750854\n",
            "Train Cycle: 2613\n",
            "Loss Per Character: 1.3817660808563232\n",
            "Train Cycle: 2614\n",
            "Loss Per Character: 1.3253103494644165\n",
            "Train Cycle: 2615\n",
            "Loss Per Character: 1.2340290546417236\n",
            "Train Cycle: 2616\n",
            "Loss Per Character: 1.4672406911849976\n",
            "Train Cycle: 2617\n",
            "Loss Per Character: 1.5226480960845947\n",
            "Train Cycle: 2618\n",
            "Loss Per Character: 1.5260316133499146\n",
            "Train Cycle: 2619\n",
            "Loss Per Character: 1.5074043273925781\n",
            "Train Cycle: 2620\n",
            "Loss Per Character: 1.4272961616516113\n",
            "Train Cycle: 2621\n",
            "Loss Per Character: 1.455001950263977\n",
            "Train Cycle: 2622\n",
            "Loss Per Character: 1.2376291751861572\n",
            "Train Cycle: 2623\n",
            "Loss Per Character: 1.5203526020050049\n",
            "Train Cycle: 2624\n",
            "Loss Per Character: 1.4860575199127197\n",
            "Train Cycle: 2625\n",
            "Loss Per Character: 1.5338664054870605\n",
            "Train Cycle: 2626\n",
            "Loss Per Character: 1.6434729099273682\n",
            "Train Cycle: 2627\n",
            "Loss Per Character: 1.3243895769119263\n",
            "Train Cycle: 2628\n",
            "Loss Per Character: 1.3724757432937622\n",
            "Train Cycle: 2629\n",
            "Loss Per Character: 1.5599976778030396\n",
            "Train Cycle: 2630\n",
            "Loss Per Character: 1.6169724464416504\n",
            "Train Cycle: 2631\n",
            "Loss Per Character: 1.4436447620391846\n",
            "Train Cycle: 2632\n",
            "Loss Per Character: 1.4327670335769653\n",
            "Train Cycle: 2633\n",
            "Loss Per Character: 1.3029190301895142\n",
            "Train Cycle: 2634\n",
            "Loss Per Character: 1.5619401931762695\n",
            "Train Cycle: 2635\n",
            "Loss Per Character: 1.469142198562622\n",
            "Train Cycle: 2636\n",
            "Loss Per Character: 1.429758071899414\n",
            "Train Cycle: 2637\n",
            "Loss Per Character: 1.3210352659225464\n",
            "Train Cycle: 2638\n",
            "Loss Per Character: 1.4342443943023682\n",
            "Train Cycle: 2639\n",
            "Loss Per Character: 1.378703236579895\n",
            "Train Cycle: 2640\n",
            "Loss Per Character: 1.1678863763809204\n",
            "Train Cycle: 2641\n",
            "Loss Per Character: 1.263346552848816\n",
            "Train Cycle: 2642\n",
            "Loss Per Character: 1.4938267469406128\n",
            "Train Cycle: 2643\n",
            "Loss Per Character: 1.5961188077926636\n",
            "Train Cycle: 2644\n",
            "Loss Per Character: 1.4048936367034912\n",
            "Train Cycle: 2645\n",
            "Loss Per Character: 1.335662841796875\n",
            "Train Cycle: 2646\n",
            "Loss Per Character: 1.613953948020935\n",
            "Train Cycle: 2647\n",
            "Loss Per Character: 1.2775925397872925\n",
            "Train Cycle: 2648\n",
            "Loss Per Character: 1.2071853876113892\n",
            "Train Cycle: 2649\n",
            "Loss Per Character: 1.1470158100128174\n",
            "Dogennated and\" in too. Anothe she idiat to see any sequaltimery. A strothing towno sequinedly. If.<r /><br/Treen't startes. If too stopy to too be a fine. Thask's.<br Delland..<burder actons. It'sn't'll. To breet..<br /><ou heads a describely) ands.Train Cycle: 2650\n",
            "Loss Per Character: 1.3095414638519287\n",
            "Train Cycle: 2651\n",
            "Loss Per Character: 1.296668529510498\n",
            "Train Cycle: 2652\n",
            "Loss Per Character: 1.4366693496704102\n",
            "Train Cycle: 2653\n",
            "Loss Per Character: 1.2580623626708984\n",
            "Train Cycle: 2654\n",
            "Loss Per Character: 1.2914453744888306\n",
            "Train Cycle: 2655\n",
            "Loss Per Character: 1.3613381385803223\n",
            "Train Cycle: 2656\n",
            "Loss Per Character: 1.3769431114196777\n",
            "Train Cycle: 2657\n",
            "Loss Per Character: 1.2936147451400757\n",
            "Train Cycle: 2658\n",
            "Loss Per Character: 1.3807519674301147\n",
            "Train Cycle: 2659\n",
            "Loss Per Character: 1.3399802446365356\n",
            "Train Cycle: 2660\n",
            "Loss Per Character: 1.8654118776321411\n",
            "Train Cycle: 2661\n",
            "Loss Per Character: 1.4135856628417969\n",
            "Train Cycle: 2662\n",
            "Loss Per Character: 1.3166017532348633\n",
            "Train Cycle: 2663\n",
            "Loss Per Character: 1.0319931507110596\n",
            "Train Cycle: 2664\n",
            "Loss Per Character: 1.3927335739135742\n",
            "Train Cycle: 2665\n",
            "Loss Per Character: 1.1135300397872925\n",
            "Train Cycle: 2666\n",
            "Loss Per Character: 1.2646342515945435\n",
            "Train Cycle: 2667\n",
            "Loss Per Character: 1.2874912023544312\n",
            "Train Cycle: 2668\n",
            "Loss Per Character: 1.2402777671813965\n",
            "Train Cycle: 2669\n",
            "Loss Per Character: 1.162682294845581\n",
            "Train Cycle: 2670\n",
            "Loss Per Character: 1.1766467094421387\n",
            "Train Cycle: 2671\n",
            "Loss Per Character: 1.4959379434585571\n",
            "Train Cycle: 2672\n",
            "Loss Per Character: 1.4618916511535645\n",
            "Train Cycle: 2673\n",
            "Loss Per Character: 1.491705298423767\n",
            "Train Cycle: 2674\n",
            "Loss Per Character: 1.3680226802825928\n",
            "Train Cycle: 2675\n",
            "Loss Per Character: 1.2276767492294312\n",
            "Train Cycle: 2676\n",
            "Loss Per Character: 1.4161863327026367\n",
            "Train Cycle: 2677\n",
            "Loss Per Character: 1.2184933423995972\n",
            "Train Cycle: 2678\n",
            "Loss Per Character: 1.269461750984192\n",
            "Train Cycle: 2679\n",
            "Loss Per Character: 1.6451303958892822\n",
            "Train Cycle: 2680\n",
            "Loss Per Character: 1.3767906427383423\n",
            "Train Cycle: 2681\n",
            "Loss Per Character: 1.5850509405136108\n",
            "Train Cycle: 2682\n",
            "Loss Per Character: 1.3897291421890259\n",
            "Train Cycle: 2683\n",
            "Loss Per Character: 1.3285332918167114\n",
            "Train Cycle: 2684\n",
            "Loss Per Character: 1.348192811012268\n",
            "Train Cycle: 2685\n",
            "Loss Per Character: 1.340012788772583\n",
            "Train Cycle: 2686\n",
            "Loss Per Character: 1.3269078731536865\n",
            "Train Cycle: 2687\n",
            "Loss Per Character: 1.4710047245025635\n",
            "Train Cycle: 2688\n",
            "Loss Per Character: 1.196288824081421\n",
            "Train Cycle: 2689\n",
            "Loss Per Character: 1.438908338546753\n",
            "Train Cycle: 2690\n",
            "Loss Per Character: 1.4761840105056763\n",
            "Train Cycle: 2691\n",
            "Loss Per Character: 1.3839178085327148\n",
            "Train Cycle: 2692\n",
            "Loss Per Character: 1.3325543403625488\n",
            "Train Cycle: 2693\n",
            "Loss Per Character: 1.3323825597763062\n",
            "Train Cycle: 2694\n",
            "Loss Per Character: 1.4753786325454712\n",
            "Train Cycle: 2695\n",
            "Loss Per Character: 1.4623842239379883\n",
            "Train Cycle: 2696\n",
            "Loss Per Character: 1.3925342559814453\n",
            "Train Cycle: 2697\n",
            "Loss Per Character: 1.3317049741744995\n",
            "Train Cycle: 2698\n",
            "Loss Per Character: 1.3849321603775024\n",
            "Train Cycle: 2699\n",
            "Loss Per Character: 1.687641978263855\n",
            "</Thangsticklat and and a seq as alment ousent. A lift an actival...<rrow here. All tryines than they. Than intended off termentine out of... I'vin an importatical.<br / I worked of. Trood. To bad. Almed isnan to they all... tot an and it's numitativTrain Cycle: 2700\n",
            "Loss Per Character: 1.359560489654541\n",
            "Train Cycle: 2701\n",
            "Loss Per Character: 1.280940294265747\n",
            "Train Cycle: 2702\n",
            "Loss Per Character: 1.3961119651794434\n",
            "Train Cycle: 2703\n",
            "Loss Per Character: 1.5159350633621216\n",
            "Train Cycle: 2704\n",
            "Loss Per Character: 1.3650935888290405\n",
            "Train Cycle: 2705\n",
            "Loss Per Character: 1.1891995668411255\n",
            "Train Cycle: 2706\n",
            "Loss Per Character: 1.2430500984191895\n",
            "Train Cycle: 2707\n",
            "Loss Per Character: 1.4088906049728394\n",
            "Train Cycle: 2708\n",
            "Loss Per Character: 1.2893251180648804\n",
            "Train Cycle: 2709\n",
            "Loss Per Character: 1.6391112804412842\n",
            "Train Cycle: 2710\n",
            "Loss Per Character: 1.2976762056350708\n",
            "Train Cycle: 2711\n",
            "Loss Per Character: 1.247916579246521\n",
            "Train Cycle: 2712\n",
            "Loss Per Character: 1.2732456922531128\n",
            "Train Cycle: 2713\n",
            "Loss Per Character: 1.3956414461135864\n",
            "Train Cycle: 2714\n",
            "Loss Per Character: 1.4768648147583008\n",
            "Train Cycle: 2715\n",
            "Loss Per Character: 1.2610927820205688\n",
            "Train Cycle: 2716\n",
            "Loss Per Character: 1.28534996509552\n",
            "Train Cycle: 2717\n",
            "Loss Per Character: 1.429563045501709\n",
            "Train Cycle: 2718\n",
            "Loss Per Character: 1.6952813863754272\n",
            "Train Cycle: 2719\n",
            "Loss Per Character: 1.422326922416687\n",
            "Train Cycle: 2720\n",
            "Loss Per Character: 1.4165266752243042\n",
            "Train Cycle: 2721\n",
            "Loss Per Character: 1.3408734798431396\n",
            "Train Cycle: 2722\n",
            "Loss Per Character: 1.2582000494003296\n",
            "Train Cycle: 2723\n",
            "Loss Per Character: 1.3716562986373901\n",
            "Train Cycle: 2724\n",
            "Loss Per Character: 1.6479398012161255\n",
            "Train Cycle: 2725\n",
            "Loss Per Character: 1.2479616403579712\n",
            "Train Cycle: 2726\n",
            "Loss Per Character: 1.3539499044418335\n",
            "Train Cycle: 2727\n",
            "Loss Per Character: 1.252855658531189\n",
            "Train Cycle: 2728\n",
            "Loss Per Character: 1.332180142402649\n",
            "Train Cycle: 2729\n",
            "Loss Per Character: 1.5446621179580688\n",
            "Train Cycle: 2730\n",
            "Loss Per Character: 1.45944082736969\n",
            "Train Cycle: 2731\n",
            "Loss Per Character: 1.3988462686538696\n",
            "Train Cycle: 2732\n",
            "Loss Per Character: 1.1256422996520996\n",
            "Train Cycle: 2733\n",
            "Loss Per Character: 1.3236889839172363\n",
            "Train Cycle: 2734\n",
            "Loss Per Character: 1.5994096994400024\n",
            "Train Cycle: 2735\n",
            "Loss Per Character: 1.3662439584732056\n",
            "Train Cycle: 2736\n",
            "Loss Per Character: 1.1084411144256592\n",
            "Train Cycle: 2737\n",
            "Loss Per Character: 1.3851406574249268\n",
            "Train Cycle: 2738\n",
            "Loss Per Character: 1.2456645965576172\n",
            "Train Cycle: 2739\n",
            "Loss Per Character: 1.255357265472412\n",
            "Train Cycle: 2740\n",
            "Loss Per Character: 1.3343340158462524\n",
            "Train Cycle: 2741\n",
            "Loss Per Character: 1.5962809324264526\n",
            "Train Cycle: 2742\n",
            "Loss Per Character: 1.3785808086395264\n",
            "Train Cycle: 2743\n",
            "Loss Per Character: 1.4544063806533813\n",
            "Train Cycle: 2744\n",
            "Loss Per Character: 1.5961120128631592\n",
            "Train Cycle: 2745\n",
            "Loss Per Character: 1.4584782123565674\n",
            "Train Cycle: 2746\n",
            "Loss Per Character: 1.2739843130111694\n",
            "Train Cycle: 2747\n",
            "Loss Per Character: 1.1647682189941406\n",
            "Train Cycle: 2748\n",
            "Loss Per Character: 1.428662657737732\n",
            "Train Cycle: 2749\n",
            "Loss Per Character: 1.390148401260376\n",
            "rashed.< off a lightimely...... and.. It seems) which..<ourstand in a scripped in my... I had. In thakes werhinds try the provortabiest.< I was a light bation offices on thinking. To triliant orialist way ofte and it. Anowart while stoppies too stayiTrain Cycle: 2750\n",
            "Loss Per Character: 1.2882753610610962\n",
            "Train Cycle: 2751\n",
            "Loss Per Character: 1.7285706996917725\n",
            "Train Cycle: 2752\n",
            "Loss Per Character: 1.2316941022872925\n",
            "Train Cycle: 2753\n",
            "Loss Per Character: 1.2893925905227661\n",
            "Train Cycle: 2754\n",
            "Loss Per Character: 1.4628701210021973\n",
            "Train Cycle: 2755\n",
            "Loss Per Character: 1.2276861667633057\n",
            "Train Cycle: 2756\n",
            "Loss Per Character: 1.3371293544769287\n",
            "Train Cycle: 2757\n",
            "Loss Per Character: 1.222761631011963\n",
            "Train Cycle: 2758\n",
            "Loss Per Character: 1.2485278844833374\n",
            "Train Cycle: 2759\n",
            "Loss Per Character: 2.0042693614959717\n",
            "Train Cycle: 2760\n",
            "Loss Per Character: 1.2130389213562012\n",
            "Train Cycle: 2761\n",
            "Loss Per Character: 1.2567654848098755\n",
            "Train Cycle: 2762\n",
            "Loss Per Character: 1.4200448989868164\n",
            "Train Cycle: 2763\n",
            "Loss Per Character: 1.5184216499328613\n",
            "Train Cycle: 2764\n",
            "Loss Per Character: 1.4483246803283691\n",
            "Train Cycle: 2765\n",
            "Loss Per Character: 1.2678512334823608\n",
            "Train Cycle: 2766\n",
            "Loss Per Character: 1.4047585725784302\n",
            "Train Cycle: 2767\n",
            "Loss Per Character: 1.26936936378479\n",
            "Train Cycle: 2768\n",
            "Loss Per Character: 1.4953821897506714\n",
            "Train Cycle: 2769\n",
            "Loss Per Character: 1.713366985321045\n",
            "Train Cycle: 2770\n",
            "Loss Per Character: 1.311903476715088\n",
            "Train Cycle: 2771\n",
            "Loss Per Character: 1.4144203662872314\n",
            "Train Cycle: 2772\n",
            "Loss Per Character: 1.5153741836547852\n",
            "Train Cycle: 2773\n",
            "Loss Per Character: 1.3425649404525757\n",
            "Train Cycle: 2774\n",
            "Loss Per Character: 1.4200844764709473\n",
            "Train Cycle: 2775\n",
            "Loss Per Character: 1.7355189323425293\n",
            "Train Cycle: 2776\n",
            "Loss Per Character: 1.69662606716156\n",
            "Train Cycle: 2777\n",
            "Loss Per Character: 1.5273562669754028\n",
            "Train Cycle: 2778\n",
            "Loss Per Character: 1.6279921531677246\n",
            "Train Cycle: 2779\n",
            "Loss Per Character: 1.8963946104049683\n",
            "Train Cycle: 2780\n",
            "Loss Per Character: 1.335817575454712\n",
            "Train Cycle: 2781\n",
            "Loss Per Character: 1.5993530750274658\n",
            "Train Cycle: 2782\n",
            "Loss Per Character: 1.447840929031372\n",
            "Train Cycle: 2783\n",
            "Loss Per Character: 1.3046000003814697\n",
            "Train Cycle: 2784\n",
            "Loss Per Character: 1.5381051301956177\n",
            "Train Cycle: 2785\n",
            "Loss Per Character: 1.575214147567749\n",
            "Train Cycle: 2786\n",
            "Loss Per Character: 1.4388092756271362\n",
            "Train Cycle: 2787\n",
            "Loss Per Character: 1.4908812046051025\n",
            "Train Cycle: 2788\n",
            "Loss Per Character: 1.5134273767471313\n",
            "Train Cycle: 2789\n",
            "Loss Per Character: 1.2896617650985718\n",
            "Train Cycle: 2790\n",
            "Loss Per Character: 1.7671966552734375\n",
            "Train Cycle: 2791\n",
            "Loss Per Character: 1.517665147781372\n",
            "Train Cycle: 2792\n",
            "Loss Per Character: 1.253442645072937\n",
            "Train Cycle: 2793\n",
            "Loss Per Character: 1.4270180463790894\n",
            "Train Cycle: 2794\n",
            "Loss Per Character: 1.4658745527267456\n",
            "Train Cycle: 2795\n",
            "Loss Per Character: 1.2259358167648315\n",
            "Train Cycle: 2796\n",
            "Loss Per Character: 1.5872159004211426\n",
            "Train Cycle: 2797\n",
            "Loss Per Character: 1.506495475769043\n",
            "Train Cycle: 2798\n",
            "Loss Per Character: 1.4670517444610596\n",
            "Train Cycle: 2799\n",
            "Loss Per Character: 1.380691409111023\n",
            "I watten tim in the stagant... I won't enjuaringly askitys are sold attembers trusingly as thinst oftelf thase too secomments) tood as anytimal actings)< whold that it and secried as sucks.<b Tharks\" watces of thoses. Thand titlish aspage this miggesTrain Cycle: 2800\n",
            "Loss Per Character: 1.3784679174423218\n",
            "Train Cycle: 2801\n",
            "Loss Per Character: 1.4224637746810913\n",
            "Train Cycle: 2802\n",
            "Loss Per Character: 1.471495509147644\n",
            "Train Cycle: 2803\n",
            "Loss Per Character: 1.4946136474609375\n",
            "Train Cycle: 2804\n",
            "Loss Per Character: 1.5607997179031372\n",
            "Train Cycle: 2805\n",
            "Loss Per Character: 1.3999080657958984\n",
            "Train Cycle: 2806\n",
            "Loss Per Character: 1.392655611038208\n",
            "Train Cycle: 2807\n",
            "Loss Per Character: 1.3989191055297852\n",
            "Train Cycle: 2808\n",
            "Loss Per Character: 1.6205089092254639\n",
            "Train Cycle: 2809\n",
            "Loss Per Character: 1.4377026557922363\n",
            "Train Cycle: 2810\n",
            "Loss Per Character: 1.3706567287445068\n",
            "Train Cycle: 2811\n",
            "Loss Per Character: 1.4229768514633179\n",
            "Train Cycle: 2812\n",
            "Loss Per Character: 1.4420377016067505\n",
            "Train Cycle: 2813\n",
            "Loss Per Character: 1.4548243284225464\n",
            "Train Cycle: 2814\n",
            "Loss Per Character: 1.4488624334335327\n",
            "Train Cycle: 2815\n",
            "Loss Per Character: 1.447601079940796\n",
            "Train Cycle: 2816\n",
            "Loss Per Character: 1.6096524000167847\n",
            "Train Cycle: 2817\n",
            "Loss Per Character: 1.4662566184997559\n",
            "Train Cycle: 2818\n",
            "Loss Per Character: 1.3866910934448242\n",
            "Train Cycle: 2819\n",
            "Loss Per Character: 1.465433120727539\n",
            "Train Cycle: 2820\n",
            "Loss Per Character: 1.3416357040405273\n",
            "Train Cycle: 2821\n",
            "Loss Per Character: 1.396209955215454\n",
            "Train Cycle: 2822\n",
            "Loss Per Character: 1.4850462675094604\n",
            "Train Cycle: 2823\n",
            "Loss Per Character: 1.42997407913208\n",
            "Train Cycle: 2824\n",
            "Loss Per Character: 1.2713420391082764\n",
            "Train Cycle: 2825\n",
            "Loss Per Character: 1.5767009258270264\n",
            "Train Cycle: 2826\n",
            "Loss Per Character: 1.5645920038223267\n",
            "Train Cycle: 2827\n",
            "Loss Per Character: 1.2272264957427979\n",
            "Train Cycle: 2828\n",
            "Loss Per Character: 1.5496976375579834\n",
            "Train Cycle: 2829\n",
            "Loss Per Character: 1.50916588306427\n",
            "Train Cycle: 2830\n",
            "Loss Per Character: 1.3161238431930542\n",
            "Train Cycle: 2831\n",
            "Loss Per Character: 1.351563811302185\n",
            "Train Cycle: 2832\n",
            "Loss Per Character: 1.420710802078247\n",
            "Train Cycle: 2833\n",
            "Loss Per Character: 1.3598928451538086\n",
            "Train Cycle: 2834\n",
            "Loss Per Character: 1.373543381690979\n",
            "Train Cycle: 2835\n",
            "Loss Per Character: 1.1913135051727295\n",
            "Train Cycle: 2836\n",
            "Loss Per Character: 1.2631293535232544\n",
            "Train Cycle: 2837\n",
            "Loss Per Character: 1.2609645128250122\n",
            "Train Cycle: 2838\n",
            "Loss Per Character: 1.4571415185928345\n",
            "Train Cycle: 2839\n",
            "Loss Per Character: 1.535548448562622\n",
            "Train Cycle: 2840\n",
            "Loss Per Character: 1.3678098917007446\n",
            "Train Cycle: 2841\n",
            "Loss Per Character: 1.4031926393508911\n",
            "Train Cycle: 2842\n",
            "Loss Per Character: 1.5424028635025024\n",
            "Train Cycle: 2843\n",
            "Loss Per Character: 1.3617247343063354\n",
            "Train Cycle: 2844\n",
            "Loss Per Character: 1.3635468482971191\n",
            "Train Cycle: 2845\n",
            "Loss Per Character: 1.2111408710479736\n",
            "Train Cycle: 2846\n",
            "Loss Per Character: 1.2525993585586548\n",
            "Train Cycle: 2847\n",
            "Loss Per Character: 1.3815845251083374\n",
            "Train Cycle: 2848\n",
            "Loss Per Character: 1.5322685241699219\n",
            "Train Cycle: 2849\n",
            "Loss Per Character: 1.5804978609085083\n",
            "Man in his fill of thas triest one..<br//><or / standshe to becase outs of moves outside out ou linger to becime too.. To be a gunniest. I am serial. They shappeated is stans..<br. Thase isn'te across teen they hope out togrin and stand outson and anTrain Cycle: 2850\n",
            "Loss Per Character: 1.4186300039291382\n",
            "Train Cycle: 2851\n",
            "Loss Per Character: 1.3964712619781494\n",
            "Train Cycle: 2852\n",
            "Loss Per Character: 1.4385082721710205\n",
            "Train Cycle: 2853\n",
            "Loss Per Character: 1.505481243133545\n",
            "Train Cycle: 2854\n",
            "Loss Per Character: 1.3899190425872803\n",
            "Train Cycle: 2855\n",
            "Loss Per Character: 1.364121437072754\n",
            "Train Cycle: 2856\n",
            "Loss Per Character: 1.2556241750717163\n",
            "Train Cycle: 2857\n",
            "Loss Per Character: 1.317440152168274\n",
            "Train Cycle: 2858\n",
            "Loss Per Character: 1.3848973512649536\n",
            "Train Cycle: 2859\n",
            "Loss Per Character: 1.2035683393478394\n",
            "Train Cycle: 2860\n",
            "Loss Per Character: 1.3409883975982666\n",
            "Train Cycle: 2861\n",
            "Loss Per Character: 1.3903687000274658\n",
            "Train Cycle: 2862\n",
            "Loss Per Character: 1.2950001955032349\n",
            "Train Cycle: 2863\n",
            "Loss Per Character: 1.4186267852783203\n",
            "Train Cycle: 2864\n",
            "Loss Per Character: 1.4551316499710083\n",
            "Train Cycle: 2865\n",
            "Loss Per Character: 1.424314260482788\n",
            "Train Cycle: 2866\n",
            "Loss Per Character: 1.560811161994934\n",
            "Train Cycle: 2867\n",
            "Loss Per Character: 1.4648610353469849\n",
            "Train Cycle: 2868\n",
            "Loss Per Character: 1.4064643383026123\n",
            "Train Cycle: 2869\n",
            "Loss Per Character: 1.281506896018982\n",
            "Train Cycle: 2870\n",
            "Loss Per Character: 1.4772614240646362\n",
            "Train Cycle: 2871\n",
            "Loss Per Character: 1.3319157361984253\n",
            "Train Cycle: 2872\n",
            "Loss Per Character: 1.394639492034912\n",
            "Train Cycle: 2873\n",
            "Loss Per Character: 1.278806447982788\n",
            "Train Cycle: 2874\n",
            "Loss Per Character: 1.2609672546386719\n",
            "Train Cycle: 2875\n",
            "Loss Per Character: 1.3410437107086182\n",
            "Train Cycle: 2876\n",
            "Loss Per Character: 1.2891424894332886\n",
            "Train Cycle: 2877\n",
            "Loss Per Character: 1.1675232648849487\n",
            "Train Cycle: 2878\n",
            "Loss Per Character: 1.290872573852539\n",
            "Train Cycle: 2879\n",
            "Loss Per Character: 1.3358025550842285\n",
            "Train Cycle: 2880\n",
            "Loss Per Character: 1.4518249034881592\n",
            "Train Cycle: 2881\n",
            "Loss Per Character: 1.2835670709609985\n",
            "Train Cycle: 2882\n",
            "Loss Per Character: 1.5646252632141113\n",
            "Train Cycle: 2883\n",
            "Loss Per Character: 1.2813974618911743\n",
            "Train Cycle: 2884\n",
            "Loss Per Character: 1.3761922121047974\n",
            "Train Cycle: 2885\n",
            "Loss Per Character: 1.1221427917480469\n",
            "Train Cycle: 2886\n",
            "Loss Per Character: 1.252166986465454\n",
            "Train Cycle: 2887\n",
            "Loss Per Character: 1.1742833852767944\n",
            "Train Cycle: 2888\n",
            "Loss Per Character: 1.2045092582702637\n",
            "Train Cycle: 2889\n",
            "Loss Per Character: 1.229515790939331\n",
            "Train Cycle: 2890\n",
            "Loss Per Character: 1.2437673807144165\n",
            "Train Cycle: 2891\n",
            "Loss Per Character: 1.4196714162826538\n",
            "Train Cycle: 2892\n",
            "Loss Per Character: 1.0541247129440308\n",
            "Train Cycle: 2893\n",
            "Loss Per Character: 1.268123745918274\n",
            "Train Cycle: 2894\n",
            "Loss Per Character: 1.3724216222763062\n",
            "Train Cycle: 2895\n",
            "Loss Per Character: 1.336719274520874\n",
            "Train Cycle: 2896\n",
            "Loss Per Character: 1.2162361145019531\n",
            "Train Cycle: 2897\n",
            "Loss Per Character: 1.1583046913146973\n",
            "Train Cycle: 2898\n",
            "Loss Per Character: 1.4210591316223145\n",
            "Train Cycle: 2899\n",
            "Loss Per Character: 1.1492756605148315\n",
            "Y wasn't. Anyway if.<br //< />I was the who movativalise. A laudie and. Alto tell it is an acrousism that therounders or somathers/rankly.<r two title there's.<r /> To men tims that'r hore. Too bries took man when I hightrappience one orivial an origTrain Cycle: 2900\n",
            "Loss Per Character: 1.52346932888031\n",
            "Train Cycle: 2901\n",
            "Loss Per Character: 1.1458357572555542\n",
            "Train Cycle: 2902\n",
            "Loss Per Character: 1.2067053318023682\n",
            "Train Cycle: 2903\n",
            "Loss Per Character: 1.415561318397522\n",
            "Train Cycle: 2904\n",
            "Loss Per Character: 1.3996460437774658\n",
            "Train Cycle: 2905\n",
            "Loss Per Character: 1.6358153820037842\n",
            "Train Cycle: 2906\n",
            "Loss Per Character: 1.3967026472091675\n",
            "Train Cycle: 2907\n",
            "Loss Per Character: 1.8117882013320923\n",
            "Train Cycle: 2908\n",
            "Loss Per Character: 1.611148715019226\n",
            "Train Cycle: 2909\n",
            "Loss Per Character: 1.7438817024230957\n",
            "Train Cycle: 2910\n",
            "Loss Per Character: 1.4198907613754272\n",
            "Train Cycle: 2911\n",
            "Loss Per Character: 1.4475892782211304\n",
            "Train Cycle: 2912\n",
            "Loss Per Character: 1.508561134338379\n",
            "Train Cycle: 2913\n",
            "Loss Per Character: 1.228541612625122\n",
            "Train Cycle: 2914\n",
            "Loss Per Character: 1.3607618808746338\n",
            "Train Cycle: 2915\n",
            "Loss Per Character: 1.5539970397949219\n",
            "Train Cycle: 2916\n",
            "Loss Per Character: 1.4289299249649048\n",
            "Train Cycle: 2917\n",
            "Loss Per Character: 1.6172946691513062\n",
            "Train Cycle: 2918\n",
            "Loss Per Character: 1.6043987274169922\n",
            "Train Cycle: 2919\n",
            "Loss Per Character: 1.5457323789596558\n",
            "Train Cycle: 2920\n",
            "Loss Per Character: 1.3944084644317627\n",
            "Train Cycle: 2921\n",
            "Loss Per Character: 1.4342290163040161\n",
            "Train Cycle: 2922\n",
            "Loss Per Character: 1.55081307888031\n",
            "Train Cycle: 2923\n",
            "Loss Per Character: 1.4515386819839478\n",
            "Train Cycle: 2924\n",
            "Loss Per Character: 1.6460119485855103\n",
            "Train Cycle: 2925\n",
            "Loss Per Character: 1.6202874183654785\n",
            "Train Cycle: 2926\n",
            "Loss Per Character: 1.6439424753189087\n",
            "Train Cycle: 2927\n",
            "Loss Per Character: 1.4413052797317505\n",
            "Train Cycle: 2928\n",
            "Loss Per Character: 1.671010136604309\n",
            "Train Cycle: 2929\n",
            "Loss Per Character: 1.298062801361084\n",
            "Train Cycle: 2930\n",
            "Loss Per Character: 1.4377081394195557\n",
            "Train Cycle: 2931\n",
            "Loss Per Character: 1.4117544889450073\n",
            "Train Cycle: 2932\n",
            "Loss Per Character: 1.4773459434509277\n",
            "Train Cycle: 2933\n",
            "Loss Per Character: 1.657511830329895\n",
            "Train Cycle: 2934\n",
            "Loss Per Character: 1.559831142425537\n",
            "Train Cycle: 2935\n",
            "Loss Per Character: 1.4142712354660034\n",
            "Train Cycle: 2936\n",
            "Loss Per Character: 1.4254145622253418\n",
            "Train Cycle: 2937\n",
            "Loss Per Character: 1.3693581819534302\n",
            "Train Cycle: 2938\n",
            "Loss Per Character: 1.4038387537002563\n",
            "Train Cycle: 2939\n",
            "Loss Per Character: 1.2887043952941895\n",
            "Train Cycle: 2940\n",
            "Loss Per Character: 1.3486402034759521\n",
            "Train Cycle: 2941\n",
            "Loss Per Character: 1.421252727508545\n",
            "Train Cycle: 2942\n",
            "Loss Per Character: 1.7803928852081299\n",
            "Train Cycle: 2943\n",
            "Loss Per Character: 1.2693556547164917\n",
            "Train Cycle: 2944\n",
            "Loss Per Character: 1.3367066383361816\n",
            "Train Cycle: 2945\n",
            "Loss Per Character: 1.1783769130706787\n",
            "Train Cycle: 2946\n",
            "Loss Per Character: 1.4505680799484253\n",
            "Train Cycle: 2947\n",
            "Loss Per Character: 1.3072457313537598\n",
            "Train Cycle: 2948\n",
            "Loss Per Character: 1.4863899946212769\n",
            "Train Cycle: 2949\n",
            "Loss Per Character: 1.3510884046554565\n",
            "Oh to seen't stop the student. To womant is nowhen an extround..<br. They.<r / an it. Trished trandity. I hoppered that's allone on heaven. All try traisiness try this off telling as a londer or soully. Itsel stop anytingles or. And. Andiveragous. ThTrain Cycle: 2950\n",
            "Loss Per Character: 1.4273383617401123\n",
            "Train Cycle: 2951\n",
            "Loss Per Character: 1.3339086771011353\n",
            "Train Cycle: 2952\n",
            "Loss Per Character: 1.4836949110031128\n",
            "Train Cycle: 2953\n",
            "Loss Per Character: 1.4407587051391602\n",
            "Train Cycle: 2954\n",
            "Loss Per Character: 1.5413895845413208\n",
            "Train Cycle: 2955\n",
            "Loss Per Character: 1.3641445636749268\n",
            "Train Cycle: 2956\n",
            "Loss Per Character: 1.6349680423736572\n",
            "Train Cycle: 2957\n",
            "Loss Per Character: 1.4421581029891968\n",
            "Train Cycle: 2958\n",
            "Loss Per Character: 1.451522707939148\n",
            "Train Cycle: 2959\n",
            "Loss Per Character: 1.6144378185272217\n",
            "Train Cycle: 2960\n",
            "Loss Per Character: 1.4796974658966064\n",
            "Train Cycle: 2961\n",
            "Loss Per Character: 1.611721158027649\n",
            "Train Cycle: 2962\n",
            "Loss Per Character: 1.4941136837005615\n",
            "Train Cycle: 2963\n",
            "Loss Per Character: 1.5476104021072388\n",
            "Train Cycle: 2964\n",
            "Loss Per Character: 1.5803792476654053\n",
            "Train Cycle: 2965\n",
            "Loss Per Character: 1.66199791431427\n",
            "Train Cycle: 2966\n",
            "Loss Per Character: 1.3943060636520386\n",
            "Train Cycle: 2967\n",
            "Loss Per Character: 1.330972671508789\n",
            "Train Cycle: 2968\n",
            "Loss Per Character: 1.3447867631912231\n",
            "Train Cycle: 2969\n",
            "Loss Per Character: 1.661827802658081\n",
            "Train Cycle: 2970\n",
            "Loss Per Character: 1.314753532409668\n",
            "Train Cycle: 2971\n",
            "Loss Per Character: 1.3557332754135132\n",
            "Train Cycle: 2972\n",
            "Loss Per Character: 1.1898025274276733\n",
            "Train Cycle: 2973\n",
            "Loss Per Character: 1.2423430681228638\n",
            "Train Cycle: 2974\n",
            "Loss Per Character: 1.3461562395095825\n",
            "Train Cycle: 2975\n",
            "Loss Per Character: 1.5097296237945557\n",
            "Train Cycle: 2976\n",
            "Loss Per Character: 1.453914761543274\n",
            "Train Cycle: 2977\n",
            "Loss Per Character: 1.2256078720092773\n",
            "Train Cycle: 2978\n",
            "Loss Per Character: 1.5184578895568848\n",
            "Train Cycle: 2979\n",
            "Loss Per Character: 1.3794585466384888\n",
            "Train Cycle: 2980\n",
            "Loss Per Character: 1.2908563613891602\n",
            "Train Cycle: 2981\n",
            "Loss Per Character: 1.502540946006775\n",
            "Train Cycle: 2982\n",
            "Loss Per Character: 1.3477811813354492\n",
            "Train Cycle: 2983\n",
            "Loss Per Character: 1.3878016471862793\n",
            "Train Cycle: 2984\n",
            "Loss Per Character: 1.2197283506393433\n",
            "Train Cycle: 2985\n",
            "Loss Per Character: 1.5017541646957397\n",
            "Train Cycle: 2986\n",
            "Loss Per Character: 1.248110294342041\n",
            "Train Cycle: 2987\n",
            "Loss Per Character: 1.4742838144302368\n",
            "Train Cycle: 2988\n",
            "Loss Per Character: 1.4522184133529663\n",
            "Train Cycle: 2989\n",
            "Loss Per Character: 1.2593982219696045\n",
            "Train Cycle: 2990\n",
            "Loss Per Character: 1.400138258934021\n",
            "Train Cycle: 2991\n",
            "Loss Per Character: 1.3983999490737915\n",
            "Train Cycle: 2992\n",
            "Loss Per Character: 1.433150053024292\n",
            "Train Cycle: 2993\n",
            "Loss Per Character: 1.3885658979415894\n",
            "Train Cycle: 2994\n",
            "Loss Per Character: 1.3317785263061523\n",
            "Train Cycle: 2995\n",
            "Loss Per Character: 1.6595604419708252\n",
            "Train Cycle: 2996\n",
            "Loss Per Character: 1.4478788375854492\n",
            "Train Cycle: 2997\n",
            "Loss Per Character: 1.4061635732650757\n",
            "Train Cycle: 2998\n",
            "Loss Per Character: 1.2598962783813477\n",
            "Train Cycle: 2999\n",
            "Loss Per Character: 1.2767466306686401\n",
            "butchild. I wound times anding...<br/watch the movie. I'vi decisesticately) wifthes terrabitylist isnet a straighabromickly in thires trash is supposes if the made.<or in hour and then intoly buy in hour tharow and...<br and. A most. Its lese in ticlTrain Cycle: 3000\n",
            "Loss Per Character: 1.353188157081604\n",
            "Train Cycle: 3001\n",
            "Loss Per Character: 1.2667115926742554\n",
            "Train Cycle: 3002\n",
            "Loss Per Character: 1.397172212600708\n",
            "Train Cycle: 3003\n",
            "Loss Per Character: 1.4901401996612549\n",
            "Train Cycle: 3004\n",
            "Loss Per Character: 1.3183889389038086\n",
            "Train Cycle: 3005\n",
            "Loss Per Character: 1.3502142429351807\n",
            "Train Cycle: 3006\n",
            "Loss Per Character: 1.3955620527267456\n",
            "Train Cycle: 3007\n",
            "Loss Per Character: 1.36185622215271\n",
            "Train Cycle: 3008\n",
            "Loss Per Character: 1.4887012243270874\n",
            "Train Cycle: 3009\n",
            "Loss Per Character: 1.4142264127731323\n",
            "Train Cycle: 3010\n",
            "Loss Per Character: 1.3882911205291748\n",
            "Train Cycle: 3011\n",
            "Loss Per Character: 1.3951057195663452\n",
            "Train Cycle: 3012\n",
            "Loss Per Character: 1.1342908143997192\n",
            "Train Cycle: 3013\n",
            "Loss Per Character: 1.4641683101654053\n",
            "Train Cycle: 3014\n",
            "Loss Per Character: 1.4630261659622192\n",
            "Train Cycle: 3015\n",
            "Loss Per Character: 1.3280134201049805\n",
            "Train Cycle: 3016\n",
            "Loss Per Character: 1.4059441089630127\n",
            "Train Cycle: 3017\n",
            "Loss Per Character: 1.5782867670059204\n",
            "Train Cycle: 3018\n",
            "Loss Per Character: 1.3072317838668823\n",
            "Train Cycle: 3019\n",
            "Loss Per Character: 1.4409863948822021\n",
            "Train Cycle: 3020\n",
            "Loss Per Character: 1.411411166191101\n",
            "Train Cycle: 3021\n",
            "Loss Per Character: 1.4407405853271484\n",
            "Train Cycle: 3022\n",
            "Loss Per Character: 1.4802287817001343\n",
            "Train Cycle: 3023\n",
            "Loss Per Character: 1.406556248664856\n",
            "Train Cycle: 3024\n",
            "Loss Per Character: 1.4165520668029785\n",
            "Train Cycle: 3025\n",
            "Loss Per Character: 1.461693525314331\n",
            "Train Cycle: 3026\n",
            "Loss Per Character: 1.499382734298706\n",
            "Train Cycle: 3027\n",
            "Loss Per Character: 1.358944058418274\n",
            "Train Cycle: 3028\n",
            "Loss Per Character: 1.4728342294692993\n",
            "Train Cycle: 3029\n",
            "Loss Per Character: 1.299678087234497\n",
            "Train Cycle: 3030\n",
            "Loss Per Character: 1.4174261093139648\n",
            "Train Cycle: 3031\n",
            "Loss Per Character: 1.5297671556472778\n",
            "Train Cycle: 3032\n",
            "Loss Per Character: 1.5418680906295776\n",
            "Train Cycle: 3033\n",
            "Loss Per Character: 1.4054750204086304\n",
            "Train Cycle: 3034\n",
            "Loss Per Character: 1.425317645072937\n",
            "Train Cycle: 3035\n",
            "Loss Per Character: 1.4726910591125488\n",
            "Train Cycle: 3036\n",
            "Loss Per Character: 1.4596686363220215\n",
            "Train Cycle: 3037\n",
            "Loss Per Character: 1.463828682899475\n",
            "Train Cycle: 3038\n",
            "Loss Per Character: 1.3086261749267578\n",
            "Train Cycle: 3039\n",
            "Loss Per Character: 1.3447719812393188\n",
            "Train Cycle: 3040\n",
            "Loss Per Character: 1.5201308727264404\n",
            "Train Cycle: 3041\n",
            "Loss Per Character: 1.403289556503296\n",
            "Train Cycle: 3042\n",
            "Loss Per Character: 1.3553297519683838\n",
            "Train Cycle: 3043\n",
            "Loss Per Character: 1.26904296875\n",
            "Train Cycle: 3044\n",
            "Loss Per Character: 1.5366915464401245\n",
            "Train Cycle: 3045\n",
            "Loss Per Character: 1.505534052848816\n",
            "Train Cycle: 3046\n",
            "Loss Per Character: 1.4404391050338745\n",
            "Train Cycle: 3047\n",
            "Loss Per Character: 1.327589511871338\n",
            "Train Cycle: 3048\n",
            "Loss Per Character: 1.546128749847412\n",
            "Train Cycle: 3049\n",
            "Loss Per Character: 1.5303843021392822\n",
            ")<br Fulci' assis thin thas intomper out on her flathing at.<bor acrovatus at team topic is.<br // a firscessomerocon that it've end ones. Try or somethat the set offeriancy in a filmed..<bl/way attround a bom so but it. Triedy anythind in an one staTrain Cycle: 3050\n",
            "Loss Per Character: 1.1802632808685303\n",
            "Train Cycle: 3051\n",
            "Loss Per Character: 1.185893177986145\n",
            "Train Cycle: 3052\n",
            "Loss Per Character: 1.2009172439575195\n",
            "Train Cycle: 3053\n",
            "Loss Per Character: 1.3984688520431519\n",
            "Train Cycle: 3054\n",
            "Loss Per Character: 1.501460075378418\n",
            "Train Cycle: 3055\n",
            "Loss Per Character: 1.3441660404205322\n",
            "Train Cycle: 3056\n",
            "Loss Per Character: 1.253685474395752\n",
            "Train Cycle: 3057\n",
            "Loss Per Character: 1.3818022012710571\n",
            "Train Cycle: 3058\n",
            "Loss Per Character: 1.3054118156433105\n",
            "Train Cycle: 3059\n",
            "Loss Per Character: 1.4447396993637085\n",
            "Train Cycle: 3060\n",
            "Loss Per Character: 1.5313053131103516\n",
            "Train Cycle: 3061\n",
            "Loss Per Character: 1.4690829515457153\n",
            "Train Cycle: 3062\n",
            "Loss Per Character: 1.5743123292922974\n",
            "Train Cycle: 3063\n",
            "Loss Per Character: 1.4058213233947754\n",
            "Train Cycle: 3064\n",
            "Loss Per Character: 1.3620212078094482\n",
            "Train Cycle: 3065\n",
            "Loss Per Character: 1.38443922996521\n",
            "Train Cycle: 3066\n",
            "Loss Per Character: 1.3021577596664429\n",
            "Train Cycle: 3067\n",
            "Loss Per Character: 1.406722068786621\n",
            "Train Cycle: 3068\n",
            "Loss Per Character: 1.1138830184936523\n",
            "Train Cycle: 3069\n",
            "Loss Per Character: 1.2847037315368652\n",
            "Train Cycle: 3070\n",
            "Loss Per Character: 1.400591254234314\n",
            "Train Cycle: 3071\n",
            "Loss Per Character: 1.184244155883789\n",
            "Train Cycle: 3072\n",
            "Loss Per Character: 1.3850077390670776\n",
            "Train Cycle: 3073\n",
            "Loss Per Character: 1.6691848039627075\n",
            "Train Cycle: 3074\n",
            "Loss Per Character: 1.348083734512329\n",
            "Train Cycle: 3075\n",
            "Loss Per Character: 1.4295988082885742\n",
            "Train Cycle: 3076\n",
            "Loss Per Character: 1.585152268409729\n",
            "Train Cycle: 3077\n",
            "Loss Per Character: 1.4693262577056885\n",
            "Train Cycle: 3078\n",
            "Loss Per Character: 1.4768009185791016\n",
            "Train Cycle: 3079\n",
            "Loss Per Character: 1.248431921005249\n",
            "Train Cycle: 3080\n",
            "Loss Per Character: 1.6013842821121216\n",
            "Train Cycle: 3081\n",
            "Loss Per Character: 1.4074909687042236\n",
            "Train Cycle: 3082\n",
            "Loss Per Character: 1.6214501857757568\n",
            "Train Cycle: 3083\n",
            "Loss Per Character: 1.1736747026443481\n",
            "Train Cycle: 3084\n",
            "Loss Per Character: 1.4840532541275024\n",
            "Train Cycle: 3085\n",
            "Loss Per Character: 1.5260282754898071\n",
            "Train Cycle: 3086\n",
            "Loss Per Character: 1.279209017753601\n",
            "Train Cycle: 3087\n",
            "Loss Per Character: 1.383101224899292\n",
            "Train Cycle: 3088\n",
            "Loss Per Character: 1.4278514385223389\n",
            "Train Cycle: 3089\n",
            "Loss Per Character: 1.6039531230926514\n",
            "Train Cycle: 3090\n",
            "Loss Per Character: 1.3820465803146362\n",
            "Train Cycle: 3091\n",
            "Loss Per Character: 1.512492299079895\n",
            "Train Cycle: 3092\n",
            "Loss Per Character: 1.3713425397872925\n",
            "Train Cycle: 3093\n",
            "Loss Per Character: 1.2017759084701538\n",
            "Train Cycle: 3094\n",
            "Loss Per Character: 1.2346961498260498\n",
            "Train Cycle: 3095\n",
            "Loss Per Character: 1.3239306211471558\n",
            "Train Cycle: 3096\n",
            "Loss Per Character: 1.7145240306854248\n",
            "Train Cycle: 3097\n",
            "Loss Per Character: 1.5134373903274536\n",
            "Train Cycle: 3098\n",
            "Loss Per Character: 1.4281680583953857\n",
            "Train Cycle: 3099\n",
            "Loss Per Character: 1.3358304500579834\n",
            "director it isn't true and this ones outs arrished to. Alson't...that's! Tom thirding.<br Betty\"\" as themse channa stuppings. It's abloss is. A littly seen them.... that itself! I can've told. That it seriou togrously.<br Thas is shottice its this moTrain Cycle: 3100\n",
            "Loss Per Character: 1.2907761335372925\n",
            "Train Cycle: 3101\n",
            "Loss Per Character: 1.205432415008545\n",
            "Train Cycle: 3102\n",
            "Loss Per Character: 1.490753412246704\n",
            "Train Cycle: 3103\n",
            "Loss Per Character: 1.1854921579360962\n",
            "Train Cycle: 3104\n",
            "Loss Per Character: 1.3593957424163818\n",
            "Train Cycle: 3105\n",
            "Loss Per Character: 1.5464943647384644\n",
            "Train Cycle: 3106\n",
            "Loss Per Character: 1.267403483390808\n",
            "Train Cycle: 3107\n",
            "Loss Per Character: 1.3919363021850586\n",
            "Train Cycle: 3108\n",
            "Loss Per Character: 1.3499112129211426\n",
            "Train Cycle: 3109\n",
            "Loss Per Character: 1.3841160535812378\n",
            "Train Cycle: 3110\n",
            "Loss Per Character: 1.2853785753250122\n",
            "Train Cycle: 3111\n",
            "Loss Per Character: 1.318534255027771\n",
            "Train Cycle: 3112\n",
            "Loss Per Character: 1.2879983186721802\n",
            "Train Cycle: 3113\n",
            "Loss Per Character: 1.1476532220840454\n",
            "Train Cycle: 3114\n",
            "Loss Per Character: 1.1535286903381348\n",
            "Train Cycle: 3115\n",
            "Loss Per Character: 1.428828477859497\n",
            "Train Cycle: 3116\n",
            "Loss Per Character: 1.2792693376541138\n",
            "Train Cycle: 3117\n",
            "Loss Per Character: 1.5437597036361694\n",
            "Train Cycle: 3118\n",
            "Loss Per Character: 1.2965809106826782\n",
            "Train Cycle: 3119\n",
            "Loss Per Character: 1.3087149858474731\n",
            "Train Cycle: 3120\n",
            "Loss Per Character: 1.353371262550354\n",
            "Train Cycle: 3121\n",
            "Loss Per Character: 1.3912739753723145\n",
            "Train Cycle: 3122\n",
            "Loss Per Character: 1.8136394023895264\n",
            "Train Cycle: 3123\n",
            "Loss Per Character: 1.4222774505615234\n",
            "Train Cycle: 3124\n",
            "Loss Per Character: 1.6041558980941772\n",
            "Train Cycle: 3125\n",
            "Loss Per Character: 1.4842329025268555\n",
            "Train Cycle: 3126\n",
            "Loss Per Character: 1.2786818742752075\n",
            "Train Cycle: 3127\n",
            "Loss Per Character: 1.2664456367492676\n",
            "Train Cycle: 3128\n",
            "Loss Per Character: 1.692586898803711\n",
            "Train Cycle: 3129\n",
            "Loss Per Character: 1.4298176765441895\n",
            "Train Cycle: 3130\n",
            "Loss Per Character: 1.4558037519454956\n",
            "Train Cycle: 3131\n",
            "Loss Per Character: 1.4256587028503418\n",
            "Train Cycle: 3132\n",
            "Loss Per Character: 1.5091869831085205\n",
            "Train Cycle: 3133\n",
            "Loss Per Character: 1.3816474676132202\n",
            "Train Cycle: 3134\n",
            "Loss Per Character: 1.2867614030838013\n",
            "Train Cycle: 3135\n",
            "Loss Per Character: 1.4038623571395874\n",
            "Train Cycle: 3136\n",
            "Loss Per Character: 1.2654743194580078\n",
            "Train Cycle: 3137\n",
            "Loss Per Character: 1.2823140621185303\n",
            "Train Cycle: 3138\n",
            "Loss Per Character: 1.4079973697662354\n",
            "Train Cycle: 3139\n",
            "Loss Per Character: 1.4538471698760986\n",
            "Train Cycle: 3140\n",
            "Loss Per Character: 1.4004533290863037\n",
            "Train Cycle: 3141\n",
            "Loss Per Character: 1.4347426891326904\n",
            "Train Cycle: 3142\n",
            "Loss Per Character: 1.2087011337280273\n",
            "Train Cycle: 3143\n",
            "Loss Per Character: 1.3958029747009277\n",
            "Train Cycle: 3144\n",
            "Loss Per Character: 1.276913046836853\n",
            "Train Cycle: 3145\n",
            "Loss Per Character: 1.4640352725982666\n",
            "Train Cycle: 3146\n",
            "Loss Per Character: 1.344462513923645\n",
            "Train Cycle: 3147\n",
            "Loss Per Character: 1.7100144624710083\n",
            "Train Cycle: 3148\n",
            "Loss Per Character: 1.3957760334014893\n",
            "Train Cycle: 3149\n",
            "Loss Per Character: 1.3656573295593262\n",
            "I dia withil it' wounds' way.<br /><br.<br/>If yuse togethical. They woul wather. Anout off to base in them and tasted to trying.. I was all sorrivied to trastines on he is.<br Tom Bak there'down is thatsone in thir main a more firsh its stiricative Train Cycle: 3150\n",
            "Loss Per Character: 1.3830506801605225\n",
            "Train Cycle: 3151\n",
            "Loss Per Character: 1.4494884014129639\n",
            "Train Cycle: 3152\n",
            "Loss Per Character: 1.1834635734558105\n",
            "Train Cycle: 3153\n",
            "Loss Per Character: 1.3640902042388916\n",
            "Train Cycle: 3154\n",
            "Loss Per Character: 1.3016817569732666\n",
            "Train Cycle: 3155\n",
            "Loss Per Character: 1.1751378774642944\n",
            "Train Cycle: 3156\n",
            "Loss Per Character: 1.319130301475525\n",
            "Train Cycle: 3157\n",
            "Loss Per Character: 1.2694834470748901\n",
            "Train Cycle: 3158\n",
            "Loss Per Character: 1.4850410223007202\n",
            "Train Cycle: 3159\n",
            "Loss Per Character: 1.194749355316162\n",
            "Train Cycle: 3160\n",
            "Loss Per Character: 1.2881885766983032\n",
            "Train Cycle: 3161\n",
            "Loss Per Character: 1.2138139009475708\n",
            "Train Cycle: 3162\n",
            "Loss Per Character: 1.332868218421936\n",
            "Train Cycle: 3163\n",
            "Loss Per Character: 1.144155502319336\n",
            "Train Cycle: 3164\n",
            "Loss Per Character: 1.6853622198104858\n",
            "Train Cycle: 3165\n",
            "Loss Per Character: 1.6541227102279663\n",
            "Train Cycle: 3166\n",
            "Loss Per Character: 1.3486049175262451\n",
            "Train Cycle: 3167\n",
            "Loss Per Character: 1.3332898616790771\n",
            "Train Cycle: 3168\n",
            "Loss Per Character: 1.4393179416656494\n",
            "Train Cycle: 3169\n",
            "Loss Per Character: 1.7866597175598145\n",
            "Train Cycle: 3170\n",
            "Loss Per Character: 1.252825379371643\n",
            "Train Cycle: 3171\n",
            "Loss Per Character: 1.5118584632873535\n",
            "Train Cycle: 3172\n",
            "Loss Per Character: 1.462373971939087\n",
            "Train Cycle: 3173\n",
            "Loss Per Character: 1.4375085830688477\n",
            "Train Cycle: 3174\n",
            "Loss Per Character: 1.345018744468689\n",
            "Train Cycle: 3175\n",
            "Loss Per Character: 1.43019700050354\n",
            "Train Cycle: 3176\n",
            "Loss Per Character: 1.5082627534866333\n",
            "Train Cycle: 3177\n",
            "Loss Per Character: 1.4535032510757446\n",
            "Train Cycle: 3178\n",
            "Loss Per Character: 1.447409987449646\n",
            "Train Cycle: 3179\n",
            "Loss Per Character: 1.4183892011642456\n",
            "Train Cycle: 3180\n",
            "Loss Per Character: 1.5981906652450562\n",
            "Train Cycle: 3181\n",
            "Loss Per Character: 1.207617163658142\n",
            "Train Cycle: 3182\n",
            "Loss Per Character: 1.4997423887252808\n",
            "Train Cycle: 3183\n",
            "Loss Per Character: 1.2365769147872925\n",
            "Train Cycle: 3184\n",
            "Loss Per Character: 1.4235601425170898\n",
            "Train Cycle: 3185\n",
            "Loss Per Character: 1.4810978174209595\n",
            "Train Cycle: 3186\n",
            "Loss Per Character: 1.403475046157837\n",
            "Train Cycle: 3187\n",
            "Loss Per Character: 1.3805304765701294\n",
            "Train Cycle: 3188\n",
            "Loss Per Character: 1.1975526809692383\n",
            "Train Cycle: 3189\n",
            "Loss Per Character: 1.4811023473739624\n",
            "Train Cycle: 3190\n",
            "Loss Per Character: 1.3716665506362915\n",
            "Train Cycle: 3191\n",
            "Loss Per Character: 1.3236558437347412\n",
            "Train Cycle: 3192\n",
            "Loss Per Character: 1.4746018648147583\n",
            "Train Cycle: 3193\n",
            "Loss Per Character: 1.2510970830917358\n",
            "Train Cycle: 3194\n",
            "Loss Per Character: 1.3027585744857788\n",
            "Train Cycle: 3195\n",
            "Loss Per Character: 1.4627214670181274\n",
            "Train Cycle: 3196\n",
            "Loss Per Character: 1.5289686918258667\n",
            "Train Cycle: 3197\n",
            "Loss Per Character: 1.3005157709121704\n",
            "Train Cycle: 3198\n",
            "Loss Per Character: 1.231139898300171\n",
            "Train Cycle: 3199\n",
            "Loss Per Character: 1.340858817100525\n",
            "one it's structly sensele of tradedly. At a great anywaire. In a gong of. To bling an anowitele was a sen is a stock. Thatsevorse wat when.. Thant they..<ob wise the many. Tone..... THES.. Took.<br//> In strip than alse this fill it sains. Thing.<br Train Cycle: 3200\n",
            "Loss Per Character: 1.4748282432556152\n",
            "Train Cycle: 3201\n",
            "Loss Per Character: 1.4888339042663574\n",
            "Train Cycle: 3202\n",
            "Loss Per Character: 1.6020278930664062\n",
            "Train Cycle: 3203\n",
            "Loss Per Character: 1.5197569131851196\n",
            "Train Cycle: 3204\n",
            "Loss Per Character: 1.413435459136963\n",
            "Train Cycle: 3205\n",
            "Loss Per Character: 1.208583116531372\n",
            "Train Cycle: 3206\n",
            "Loss Per Character: 1.4058408737182617\n",
            "Train Cycle: 3207\n",
            "Loss Per Character: 1.2132344245910645\n",
            "Train Cycle: 3208\n",
            "Loss Per Character: 1.3107311725616455\n",
            "Train Cycle: 3209\n",
            "Loss Per Character: 1.3442785739898682\n",
            "Train Cycle: 3210\n",
            "Loss Per Character: 1.3749223947525024\n",
            "Train Cycle: 3211\n",
            "Loss Per Character: 1.4870412349700928\n",
            "Train Cycle: 3212\n",
            "Loss Per Character: 1.3336777687072754\n",
            "Train Cycle: 3213\n",
            "Loss Per Character: 1.4272228479385376\n",
            "Train Cycle: 3214\n",
            "Loss Per Character: 1.335298776626587\n",
            "Train Cycle: 3215\n",
            "Loss Per Character: 1.3436933755874634\n",
            "Train Cycle: 3216\n",
            "Loss Per Character: 1.5001229047775269\n",
            "Train Cycle: 3217\n",
            "Loss Per Character: 1.5755211114883423\n",
            "Train Cycle: 3218\n",
            "Loss Per Character: 1.3102961778640747\n",
            "Train Cycle: 3219\n",
            "Loss Per Character: 1.1528382301330566\n",
            "Train Cycle: 3220\n",
            "Loss Per Character: 1.259507417678833\n",
            "Train Cycle: 3221\n",
            "Loss Per Character: 1.6034135818481445\n",
            "Train Cycle: 3222\n",
            "Loss Per Character: 1.5042465925216675\n",
            "Train Cycle: 3223\n",
            "Loss Per Character: 1.363735556602478\n",
            "Train Cycle: 3224\n",
            "Loss Per Character: 1.5963611602783203\n",
            "Train Cycle: 3225\n",
            "Loss Per Character: 1.095768690109253\n",
            "Train Cycle: 3226\n",
            "Loss Per Character: 1.2786513566970825\n",
            "Train Cycle: 3227\n",
            "Loss Per Character: 1.3773120641708374\n",
            "Train Cycle: 3228\n",
            "Loss Per Character: 1.3405777215957642\n",
            "Train Cycle: 3229\n",
            "Loss Per Character: 1.1511750221252441\n",
            "Train Cycle: 3230\n",
            "Loss Per Character: 1.4994536638259888\n",
            "Train Cycle: 3231\n",
            "Loss Per Character: 1.2818659543991089\n",
            "Train Cycle: 3232\n",
            "Loss Per Character: 1.554961085319519\n",
            "Train Cycle: 3233\n",
            "Loss Per Character: 1.386419653892517\n",
            "Train Cycle: 3234\n",
            "Loss Per Character: 1.2901725769042969\n",
            "Train Cycle: 3235\n",
            "Loss Per Character: 1.3626694679260254\n",
            "Train Cycle: 3236\n",
            "Loss Per Character: 1.6388444900512695\n",
            "Train Cycle: 3237\n",
            "Loss Per Character: 1.4897339344024658\n",
            "Train Cycle: 3238\n",
            "Loss Per Character: 1.4519225358963013\n",
            "Train Cycle: 3239\n",
            "Loss Per Character: 1.2567026615142822\n",
            "Train Cycle: 3240\n",
            "Loss Per Character: 1.4806063175201416\n",
            "Train Cycle: 3241\n",
            "Loss Per Character: 1.3470101356506348\n",
            "Train Cycle: 3242\n",
            "Loss Per Character: 1.252935528755188\n",
            "Train Cycle: 3243\n",
            "Loss Per Character: 1.3021132946014404\n",
            "Train Cycle: 3244\n",
            "Loss Per Character: 1.350594401359558\n",
            "Train Cycle: 3245\n",
            "Loss Per Character: 1.4546763896942139\n",
            "Train Cycle: 3246\n",
            "Loss Per Character: 1.3528759479522705\n",
            "Train Cycle: 3247\n",
            "Loss Per Character: 1.2287960052490234\n",
            "Train Cycle: 3248\n",
            "Loss Per Character: 1.261343002319336\n",
            "Train Cycle: 3249\n",
            "Loss Per Character: 1.3499983549118042\n",
            "\" was alwayshind a casiderate. Anywoyed\" they.... andstypinies took to talled too. In her cartons\")......this is alseady'mpetsice.<bl / was a group too bad thank toger a convolied truses. All if..... The pensionalliss as an all trash the shap of almeTrain Cycle: 3250\n",
            "Loss Per Character: 1.4378161430358887\n",
            "Train Cycle: 3251\n",
            "Loss Per Character: 1.1790663003921509\n",
            "Train Cycle: 3252\n",
            "Loss Per Character: 1.6029022932052612\n",
            "Train Cycle: 3253\n",
            "Loss Per Character: 1.5634877681732178\n",
            "Train Cycle: 3254\n",
            "Loss Per Character: 1.571856141090393\n",
            "Train Cycle: 3255\n",
            "Loss Per Character: 1.4616426229476929\n",
            "Train Cycle: 3256\n",
            "Loss Per Character: 1.330594539642334\n",
            "Train Cycle: 3257\n",
            "Loss Per Character: 1.5336406230926514\n",
            "Train Cycle: 3258\n",
            "Loss Per Character: 1.3485519886016846\n",
            "Train Cycle: 3259\n",
            "Loss Per Character: 1.4201778173446655\n",
            "Train Cycle: 3260\n",
            "Loss Per Character: 1.3927297592163086\n",
            "Train Cycle: 3261\n",
            "Loss Per Character: 1.553107500076294\n",
            "Train Cycle: 3262\n",
            "Loss Per Character: 1.5155961513519287\n",
            "Train Cycle: 3263\n",
            "Loss Per Character: 1.7060062885284424\n",
            "Train Cycle: 3264\n",
            "Loss Per Character: 1.543300986289978\n",
            "Train Cycle: 3265\n",
            "Loss Per Character: 1.6446127891540527\n",
            "Train Cycle: 3266\n",
            "Loss Per Character: 1.5956496000289917\n",
            "Train Cycle: 3267\n",
            "Loss Per Character: 1.5967258214950562\n",
            "Train Cycle: 3268\n",
            "Loss Per Character: 1.3633612394332886\n",
            "Train Cycle: 3269\n",
            "Loss Per Character: 1.441165804862976\n",
            "Train Cycle: 3270\n",
            "Loss Per Character: 1.4854146242141724\n",
            "Train Cycle: 3271\n",
            "Loss Per Character: 1.2601605653762817\n",
            "Train Cycle: 3272\n",
            "Loss Per Character: 1.5087907314300537\n",
            "Train Cycle: 3273\n",
            "Loss Per Character: 1.481851577758789\n",
            "Train Cycle: 3274\n",
            "Loss Per Character: 1.5424383878707886\n",
            "Train Cycle: 3275\n",
            "Loss Per Character: 1.350283145904541\n",
            "Train Cycle: 3276\n",
            "Loss Per Character: 1.2763769626617432\n",
            "Train Cycle: 3277\n",
            "Loss Per Character: 1.1683815717697144\n",
            "Train Cycle: 3278\n",
            "Loss Per Character: 1.427510380744934\n",
            "Train Cycle: 3279\n",
            "Loss Per Character: 1.3684943914413452\n",
            "Train Cycle: 3280\n",
            "Loss Per Character: 1.5523931980133057\n",
            "Train Cycle: 3281\n",
            "Loss Per Character: 1.5262776613235474\n",
            "Train Cycle: 3282\n",
            "Loss Per Character: 1.4853802919387817\n",
            "Train Cycle: 3283\n",
            "Loss Per Character: 1.3775888681411743\n",
            "Train Cycle: 3284\n",
            "Loss Per Character: 1.263655424118042\n",
            "Train Cycle: 3285\n",
            "Loss Per Character: 1.397585153579712\n",
            "Train Cycle: 3286\n",
            "Loss Per Character: 1.4073560237884521\n",
            "Train Cycle: 3287\n",
            "Loss Per Character: 1.0903295278549194\n",
            "Train Cycle: 3288\n",
            "Loss Per Character: 1.412759780883789\n",
            "Train Cycle: 3289\n",
            "Loss Per Character: 1.5397759675979614\n",
            "Train Cycle: 3290\n",
            "Loss Per Character: 1.6259593963623047\n",
            "Train Cycle: 3291\n",
            "Loss Per Character: 1.444547176361084\n",
            "Train Cycle: 3292\n",
            "Loss Per Character: 1.2903003692626953\n",
            "Train Cycle: 3293\n",
            "Loss Per Character: 1.3324226140975952\n",
            "Train Cycle: 3294\n",
            "Loss Per Character: 1.156313180923462\n",
            "Train Cycle: 3295\n",
            "Loss Per Character: 1.3415215015411377\n",
            "Train Cycle: 3296\n",
            "Loss Per Character: 1.2633495330810547\n",
            "Train Cycle: 3297\n",
            "Loss Per Character: 1.211621642112732\n",
            "Train Cycle: 3298\n",
            "Loss Per Character: 1.3345367908477783\n",
            "Train Cycle: 3299\n",
            "Loss Per Character: 1.3498808145523071\n",
            "beshere trailer in trase... and torn offeriers anyto shourd.<bor a falt one of a strugg and shalleds.. I tore it any fact a film an instear tot this onlest. I haven't terrines on all overage. Also a stupio toot anyoness an intendingling a few they'viTrain Cycle: 3300\n",
            "Loss Per Character: 1.3722269535064697\n",
            "Train Cycle: 3301\n",
            "Loss Per Character: 1.4259601831436157\n",
            "Train Cycle: 3302\n",
            "Loss Per Character: 1.5408042669296265\n",
            "Train Cycle: 3303\n",
            "Loss Per Character: 1.2743901014328003\n",
            "Train Cycle: 3304\n",
            "Loss Per Character: 1.319870114326477\n",
            "Train Cycle: 3305\n",
            "Loss Per Character: 1.570776343345642\n",
            "Train Cycle: 3306\n",
            "Loss Per Character: 1.4753987789154053\n",
            "Train Cycle: 3307\n",
            "Loss Per Character: 1.2262699604034424\n",
            "Train Cycle: 3308\n",
            "Loss Per Character: 1.3847475051879883\n",
            "Train Cycle: 3309\n",
            "Loss Per Character: 1.5443685054779053\n",
            "Train Cycle: 3310\n",
            "Loss Per Character: 1.2054628133773804\n",
            "Train Cycle: 3311\n",
            "Loss Per Character: 1.4430781602859497\n",
            "Train Cycle: 3312\n",
            "Loss Per Character: 1.4121665954589844\n",
            "Train Cycle: 3313\n",
            "Loss Per Character: 1.1869773864746094\n",
            "Train Cycle: 3314\n",
            "Loss Per Character: 1.353554368019104\n",
            "Train Cycle: 3315\n",
            "Loss Per Character: 1.2501929998397827\n",
            "Train Cycle: 3316\n",
            "Loss Per Character: 1.2364155054092407\n",
            "Train Cycle: 3317\n",
            "Loss Per Character: 1.4376585483551025\n",
            "Train Cycle: 3318\n",
            "Loss Per Character: 1.4323954582214355\n",
            "Train Cycle: 3319\n",
            "Loss Per Character: 1.259581208229065\n",
            "Train Cycle: 3320\n",
            "Loss Per Character: 1.2414681911468506\n",
            "Train Cycle: 3321\n",
            "Loss Per Character: 1.3482869863510132\n",
            "Train Cycle: 3322\n",
            "Loss Per Character: 1.4696035385131836\n",
            "Train Cycle: 3323\n",
            "Loss Per Character: 1.5024654865264893\n",
            "Train Cycle: 3324\n",
            "Loss Per Character: 1.4995523691177368\n",
            "Train Cycle: 3325\n",
            "Loss Per Character: 1.709211826324463\n",
            "Train Cycle: 3326\n",
            "Loss Per Character: 1.3445487022399902\n",
            "Train Cycle: 3327\n",
            "Loss Per Character: 1.353150486946106\n",
            "Train Cycle: 3328\n",
            "Loss Per Character: 1.509149432182312\n",
            "Train Cycle: 3329\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f23451fc2e9d>\u001b[0m in \u001b[0;36m<cell line: 154>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m   \u001b[0mtraining_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m   \u001b[0mtest_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-f23451fc2e9d>\u001b[0m in \u001b[0;36mtraining_cycle\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m       \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stacked LSTM implementation\n",
        "import torch\n",
        "from torchtext import datasets\n",
        "from torch import nn\n",
        "import random\n",
        "\n",
        "# Architecture hyperparams\n",
        "num_chars = 64\n",
        "LSTM_sz = 512\n",
        "LSTM_layer_n = 3\n",
        "sample_topk = 3\n",
        "sample_freq = 50\n",
        "special_char = [\" \", \"<\", \">\", \"/\", \"\\\"\", \"\\'\", \":\", \";\", \".\", \"(\", \")\", \"!\"]\n",
        "\n",
        "# learning rate\n",
        "lr = 3e-4\n",
        "\n",
        "# Epochs to train model for (each epoch loops through the corpus once)\n",
        "epochs = 50\n",
        "\n",
        "# Number of characters to sample from model each testing cycle\n",
        "sample_length = 250\n",
        "\n",
        "# Convert character to index\n",
        "def char_index(x):\n",
        "  if ord(x) < 91 and ord(x) > 64:\n",
        "    return ord(x) - 65\n",
        "  if ord(x) < 123 and ord(x) > 96:\n",
        "    return ord(x) - 71\n",
        "  return special_char.index(x) + 52\n",
        "\n",
        "# Filter characters in input data for relevant characters\n",
        "def keep_char(x):\n",
        "  return (ord(x) < 91 and ord(x) > 64) or (ord(x) < 123 and ord(x) > 96) or x in special_char\n",
        "\n",
        "# Convert index to character\n",
        "def index_char(ind):\n",
        "  if ind < 26:\n",
        "    return chr(ind + 65)\n",
        "  if ind < 52:\n",
        "    return chr(ind + 71)\n",
        "  return special_char[ind - 52]\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self, sz):\n",
        "    super().__init__()\n",
        "\n",
        "    self.f = nn.Sequential(\n",
        "        nn.Linear(2 * sz, sz),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "    self.i1 = nn.Sequential(\n",
        "        nn.Linear(2 * sz, sz),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "    self.i2 = nn.Sequential(\n",
        "        nn.Linear(2 * sz, sz),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "    self.o = nn.Sequential(\n",
        "        nn.Linear(2 * sz, sz),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "    self.dropout = nn.Dropout(0.4)\n",
        "\n",
        "  def forward(self, inp, hidden, cell):\n",
        "    new_cell = cell\n",
        "\n",
        "    inp = self.dropout(inp)\n",
        "    xh = torch.cat((inp, hidden))\n",
        "    new_cell = new_cell * self.f(xh)\n",
        "    new_cell = new_cell + (self.i1(xh) * self.i2(xh))\n",
        "    new_hidden = torch.tanh(new_cell) * self.o(xh)\n",
        "\n",
        "    return new_out, new_hidden, new_cell\n",
        "\n",
        "class StackedLSTM(nn.Module):\n",
        "  def __init__(self, layer_num, inp_sz, LSTM_sz):\n",
        "    super().__init__()\n",
        "    self.layers = layer_num\n",
        "\n",
        "    self.embed = nn.Embedding(inp_sz, LSTM_sz)\n",
        "    self.LSTM_layers = [LSTM(LSTM_sz) for _ in range(layer_num)]\n",
        "    self.output = nn.Linear(LSTM_sz, inp_sz)\n",
        "\n",
        "  def forward(self, inp, hidden_states, cell_states):\n",
        "    out = self.embed(inp)\n",
        "    new_hidden = hidden_states\n",
        "    new_cell = cell_states\n",
        "\n",
        "    for i in range(self.layers):\n",
        "      out, new_hidden[i], new_cell[i] = self.LSTM_layers[i](out, new_hidden[i], new_cell[i])\n",
        "\n",
        "    out = self.output(out)\n",
        "\n",
        "    return out, new_hidden, new_cell\n",
        "\n",
        "# Training data (IMDB database in torchtext)\n",
        "training_dataloader = iter(datasets.IMDB(split=\"train\", root = \"data\"))\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "\n",
        "model = StackedLSTM(LSTM_layer_n, num_chars, LSTM_hidden_sz).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Train model\n",
        "def training_cycle():\n",
        "  model.train()\n",
        "\n",
        "  train_cycle_num = 1\n",
        "\n",
        "  while True:\n",
        "    try:\n",
        "      print(f\"Train Cycle: {train_cycle_num}\")\n",
        "\n",
        "      train_cycle_num += 1\n",
        "      string = ''.join(filter(keep_char, next(training_dataloader)[1]))\n",
        "\n",
        "      hidden = torch.zeros((LSTM_layer_n, LSTM_sz)).to(device)\n",
        "      cell = torch.zeros((LSTM_layer_n, LSTM_sz)).to(device)\n",
        "      loss = 0\n",
        "\n",
        "      for i in range(0, len(string) - 1):\n",
        "        out, hidden, cell = model(torch.tensor(char_index(string[i])).to(device), hidden, cell)\n",
        "\n",
        "        loss += criterion(torch.log(out), torch.tensor(char_index(string[i+1])).to(device))\n",
        "\n",
        "      loss = loss / (len(string) - 1)\n",
        "\n",
        "      model.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
        "      opt.step()\n",
        "\n",
        "      print(f\"Loss Per Character: {loss}\")\n",
        "\n",
        "      if (train_cycle_num % sample_freq == 0):\n",
        "        test_cycle()\n",
        "\n",
        "    except StopIteration:\n",
        "      break\n",
        "\n",
        "# Sample from model to test progress every once in a while\n",
        "def test_cycle():\n",
        "  model.eval()\n",
        "  init_char = random.randint(0, num_chars - 1)\n",
        "\n",
        "  hidden = torch.zeros((LSTM_layer_n, LSTM_sz)).to(device)\n",
        "  cell = torch.zeros((LSTM_layer_n, LSTM_sz)).to(device)\n",
        "\n",
        "  for i in range(sample_length):\n",
        "    print(index_char(init_char), end = \"\")\n",
        "\n",
        "    out, hidden, cell = model(torch.tensor(init_char).to(device), hidden, cell)\n",
        "    top_chars = torch.topk(out, sample_topk)\n",
        "    init_char = top_chars[1][list(torch.utils.data.WeightedRandomSampler(nn.functional.softmax(top_chars[0], dim = 0), 1))[0]].item()\n",
        "\n",
        "# Training cycles\n",
        "for epoch in range(epochs):\n",
        "  print(f\"Epoch {epoch}\")\n",
        "  training_cycle()\n",
        "\n",
        "  test_cycle()"
      ],
      "metadata": {
        "id": "DPpwSiorqjmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "a = torch.tensor([1, 2, 3])\n",
        "\n",
        "print(nn.Softmax(a))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrFYvmopZ9kY",
        "outputId": "dff7630e-940e-4c84-b69d-3d5a2059eec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax(dim=tensor([1, 2, 3]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "id": "57EjvD8yn8jP",
        "outputId": "b5eaab20-514c-4b3d-cc42-b40eb88d9c04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "Train Cycle: 1\n",
            "JJPuuuuuddUdddRRR      UUAverage Loss Per Char: 1.9559210205078126\n",
            "B BBiBBBBXXXXXXXXXllkkAkuAverage Loss Per Char: 1.8833927917480469\n",
            "dddddinnnRRRRRRRRrccJeeeeAverage Loss Per Char: 1.9804585266113282\n",
            "iiiiiiiiiwwwTTTTTTTTTTHHiAverage Loss Per Char: 1.8996270751953126\n",
            "BBBBCCVwVBBgggggiiss  MERAverage Loss Per Char: 1.908892364501953\n",
            "peeeepppiiBBBBBBSSSSssOOOAverage Loss Per Char: 1.85684326171875\n",
            "eeeeeeBJBBBBBBCCCBBBBBBBiAverage Loss Per Char: 1.84258056640625\n",
            "lSSSSSSSddddSSSSSSASSSSQAAverage Loss Per Char: 1.8156915283203126\n",
            "JJJBBBBBBBBBBBBAAAAAAAAAAAverage Loss Per Char: 1.777398681640625\n",
            "eeeeeeeeeeeeeeBBBBAAAAAA Average Loss Per Char: 1.7998463439941406\n",
            "yyyyyyyyyKGGKeeeAAAAAAAAAAverage Loss Per Char: 1.7867172241210938\n",
            "yyBBBBBBBBBBwAAAAABBBBQQBAverage Loss Per Char: 1.7754226684570313\n",
            "eeAAAAAAAAAABBBBBBBAAABBBAverage Loss Per Char: 1.7643565368652343\n",
            "AAmmmmmBBBBAAAAAAAAAAAAAAAverage Loss Per Char: 1.7601129150390624\n",
            "AAAAAAAAAAABBBBBBBBBAAAAAAverage Loss Per Char: 1.744033660888672\n",
            "AAAAAAATest Cycle\n",
            "BBBwwBBwBwBBBGwBwwwBwwBBwwBBwBBGBBwuQBGBwBBBwwwBwBwxwBBBwBBBxwBBBBBBBBxwBBBBwBBBJBBBwBwwwBBwwBwBBBBBBBBBBGwxBwBwwBBBwBwBBwBGBwwBBwBBBBBQBBBwBuwxxwBBBBBBxwBBwwuBwBuBBBxBBuBBBQwQBBwJBBwBBBBwwBBBBwwwwxwBBwwBBwBwBBBQwwBwBwwBBJBBBGBwwwuBwBBBBGxBJBxwBBuwBTrain Cycle: 2\n",
            "AAAAAAAAAAAAAAAAAAAAAAAAAAverage Loss Per Char: 1.7353495788574218\n",
            "AAAAAAAAAAAAAAAAAAAAAAAAAAverage Loss Per Char: 1.7224378967285157\n",
            "AAAAAAAAAAAAAAAAAAAAAAAAAAverage Loss Per Char: 1.692183837890625\n",
            "ABBBBAABBAAAAAAAAAAAAAAAAAverage Loss Per Char: 1.6816900634765626\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-299d21c9dd5e>\u001b[0m in \u001b[0;36m<cell line: 228>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m   \u001b[0mtraining_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-299d21c9dd5e>\u001b[0m in \u001b[0;36mtraining_cycle\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m           \u001b[0mmodel_output_l1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_temp_hidden_state_l1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_temp_cell_state_l1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM_model_layer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_onehot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_hidden_states_l1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_cell_states_l1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m           \u001b[0mmodel_output_l2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_temp_hidden_state_l2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_temp_cell_state_l2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM_model_layer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_output_l1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_hidden_states_l2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_cell_states_l2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m           \u001b[0mmodel_output_l3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_temp_hidden_state_l3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_temp_cell_state_l3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM_model_layer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_output_l2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_hidden_states_l3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_cell_states_l3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m           \u001b[0mmodel_output_l1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_temp_hidden_state_l1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_temp_cell_state_l1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM_model_layer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_hidden_states_l1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_cell_states_l1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-299d21c9dd5e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, h, c)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mnew_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_cell\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_inp\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_f\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mnew_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_cell\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_inp\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_ii\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_ii\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_inp\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_iC\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_iC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mnew_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_inp\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_o\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_o\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_cell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Experimental test\n",
        "\n",
        "import torch\n",
        "from torchtext import datasets\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "from time import sleep\n",
        "from torch import nn\n",
        "\n",
        "# Main LSTM architecture definition\n",
        "class LSTM(nn.Module):\n",
        "  # Initialize state\n",
        "  def __init__(self, inp_sz, hidden_sz, out_sz):\n",
        "    super().__init__()\n",
        "    self.inp_sz = inp_sz\n",
        "    self.hidden_sz = hidden_sz\n",
        "    self.cell_sz = hidden_sz\n",
        "    self.output_sz = out_sz\n",
        "\n",
        "    self.W_f = torch.nn.Parameter(torch.Tensor(self.inp_sz + self.hidden_sz, self.cell_sz))\n",
        "    self.b_f = torch.nn.Parameter(torch.Tensor(self.cell_sz))\n",
        "\n",
        "    self.W_ii = torch.nn.Parameter(torch.Tensor(self.inp_sz + self.hidden_sz, self.cell_sz))\n",
        "    self.b_ii = torch.nn.Parameter(torch.Tensor(self.cell_sz))\n",
        "    self.W_iC = torch.nn.Parameter(torch.Tensor(self.inp_sz + self.hidden_sz, self.cell_sz))\n",
        "    self.b_iC = torch.nn.Parameter(torch.Tensor(self.cell_sz))\n",
        "\n",
        "    self.W_o = torch.nn.Parameter(torch.Tensor(self.inp_sz + self.hidden_sz, self.cell_sz))\n",
        "    self.b_o = torch.nn.Parameter(torch.Tensor(self.cell_sz))\n",
        "\n",
        "    self.W_out = torch.nn.Parameter(torch.Tensor(self.cell_sz, self.output_sz))\n",
        "    self.b_out = torch.nn.Parameter(torch.Tensor(self.output_sz))\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "  # Initialize weights in LSTM according to Xavier Initialization\n",
        "  def init_weights(self):\n",
        "    for weight in self.parameters():\n",
        "      weight.data.uniform_(-1.0/math.sqrt(self.inp_sz / 6.0), 1.0/math.sqrt(self.inp_sz / 6.0))\n",
        "\n",
        "  # Forward Propagation Logic\n",
        "  def forward(self, x, h, c):\n",
        "    new_cell = c\n",
        "    new_hidden = h\n",
        "\n",
        "    hidden_inp = torch.cat((x, h))\n",
        "\n",
        "    new_cell = new_cell * torch.sigmoid(hidden_inp @ self.W_f + self.b_f)\n",
        "    new_cell = new_cell + (torch.sigmoid(hidden_inp @ self.W_ii + self.b_ii) * torch.tanh(hidden_inp @ self.W_iC + self.b_iC))\n",
        "\n",
        "    new_hidden = torch.sigmoid(hidden_inp @ self.W_o + self.b_o) * torch.tanh(new_cell)\n",
        "\n",
        "    new_output = torch.tanh(new_hidden @ self.W_out + self.b_out)\n",
        "\n",
        "    return new_output, new_hidden, new_cell\n",
        "\n",
        "# Filter characters in input data for relevant characters (alphabetic and spaces)\n",
        "def keep_char(x):\n",
        "  return x.isalpha() or x.isspace()\n",
        "\n",
        "# Convert character to index in one-hot vector representation of characters\n",
        "def char_index(x):\n",
        "  if ord(x) < 90 and ord(x) != 32:\n",
        "    return ord(x) - 65\n",
        "  if ord(x) < 122 and ord(x) != 32:\n",
        "    return ord(x) - 71\n",
        "  return 52\n",
        "\n",
        "# Convert character vector\n",
        "def onehot_to_char(x):\n",
        "  ind = np.argmax(x)\n",
        "\n",
        "  if ind < 26:\n",
        "    return chr(ind + 65)\n",
        "  if ind != 52:\n",
        "    return chr(ind + 71)\n",
        "  return chr(32)\n",
        "\n",
        "# Training data (IMDB database in torchtext)\n",
        "training_dataloader = iter(datasets.IMDB(split=\"train\", root = \"data\"))\n",
        "\n",
        "# Epochs to train model for (each epoch loops through the corpus once)\n",
        "epochs = 50\n",
        "\n",
        "# Number of characters to sample from model each testing cycle\n",
        "sample_length = 250\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "\n",
        "# Model, optimizer, and loss function instances\n",
        "LSTM_model_layer1 = LSTM(53, 512, 256).to(device)\n",
        "LSTM_model_layer2 = LSTM(256, 512, 256).to(device)\n",
        "LSTM_model_layer3 = LSTM(256, 512, 53).to(device)\n",
        "optimizer = torch.optim.Adam(list(LSTM_model_layer1.parameters()) + list(LSTM_model_layer2.parameters()) + list(LSTM_model_layer3.parameters()))\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train model\n",
        "def training_cycle():\n",
        "  LSTM_model_layer1.train()\n",
        "  LSTM_model_layer2.train()\n",
        "  LSTM_model_layer3.train()\n",
        "\n",
        "  train_cycle_num = 1\n",
        "\n",
        "  while True:\n",
        "    try:\n",
        "\n",
        "      print(f\"Train Cycle: {train_cycle_num}\")\n",
        "      train_cycle_num += 1\n",
        "\n",
        "      string = ''.join(filter(keep_char,next(training_dataloader)[1]))\n",
        "\n",
        "      string_onehot = []\n",
        "\n",
        "      ind = 0\n",
        "      for char in string:\n",
        "        ind += 1\n",
        "\n",
        "        onehot_char = torch.zeros(53)\n",
        "        # onehot_char[char_index(char)] = 1.0\n",
        "        onehot_char[ind%2] = 1.0\n",
        "\n",
        "        string_onehot.append(onehot_char)\n",
        "\n",
        "      for i in range(1, len(string_onehot)):\n",
        "        if i % 100 == 1:\n",
        "          model_loss = 0\n",
        "          model_hidden_states_l1 = []\n",
        "          model_cell_states_l1 = []\n",
        "          model_hidden_states_l2 = []\n",
        "          model_cell_states_l2 = []\n",
        "          model_hidden_states_l3 = []\n",
        "          model_cell_states_l3 = []\n",
        "\n",
        "          model_hidden_states_l1.append(torch.zeros(512))\n",
        "          model_cell_states_l1.append(torch.zeros(512))\n",
        "          model_hidden_states_l2.append(torch.zeros(512))\n",
        "          model_cell_states_l2.append(torch.zeros(512))\n",
        "          model_hidden_states_l3.append(torch.zeros(512))\n",
        "          model_cell_states_l3.append(torch.zeros(512))\n",
        "\n",
        "        if i % 100 < 75:\n",
        "          model_output_l1, model_temp_hidden_state_l1, model_temp_cell_state_l1 = LSTM_model_layer1(string_onehot[i-1] * 2 - 1, model_hidden_states_l1[-1], model_cell_states_l1[-1])\n",
        "          model_output_l2, model_temp_hidden_state_l2, model_temp_cell_state_l2 = LSTM_model_layer2(model_output_l1, model_hidden_states_l2[-1], model_cell_states_l2[-1])\n",
        "          model_output_l3, model_temp_hidden_state_l3, model_temp_cell_state_l3 = LSTM_model_layer3(model_output_l2, model_hidden_states_l3[-1], model_cell_states_l3[-1])\n",
        "        else:\n",
        "          model_output_l1, model_temp_hidden_state_l1, model_temp_cell_state_l1 = LSTM_model_layer1(pred_char, model_hidden_states_l1[-1], model_cell_states_l1[-1])\n",
        "          model_output_l2, model_temp_hidden_state_l2, model_temp_cell_state_l2 = LSTM_model_layer2(model_output_l1, model_hidden_states_l2[-1], model_cell_states_l2[-1])\n",
        "          model_output_l3, model_temp_hidden_state_l3, model_temp_cell_state_l3 = LSTM_model_layer3(model_output_l2, model_hidden_states_l3[-1], model_cell_states_l3[-1])\n",
        "\n",
        "          # print(onehot_to_char(string_onehot[i]), end = \"\")\n",
        "          print(onehot_to_char(model_output_l3.detach()), end = \"\")\n",
        "          #print(string_onehot[i])\n",
        "          model_loss += loss((model_output_l3 +1) / 2, string_onehot[i])\n",
        "\n",
        "        model_hidden_states_l1.append(model_temp_hidden_state_l1)\n",
        "        model_cell_states_l1.append(model_temp_cell_state_l1)\n",
        "        model_hidden_states_l2.append(model_temp_hidden_state_l2)\n",
        "        model_cell_states_l2.append(model_temp_cell_state_l2)\n",
        "        model_hidden_states_l3.append(model_temp_hidden_state_l3)\n",
        "        model_cell_states_l3.append(model_temp_cell_state_l3)\n",
        "\n",
        "        pred_char = model_output_l3\n",
        "\n",
        "        if i % 100 == 0:\n",
        "          print(f\"Average Loss Per Char: {model_loss.item() / 50}\")\n",
        "          LSTM_model_layer1.zero_grad()\n",
        "          LSTM_model_layer2.zero_grad()\n",
        "          LSTM_model_layer3.zero_grad()\n",
        "          model_loss.backward()\n",
        "\n",
        "          torch.nn.utils.clip_grad_norm_(LSTM_model_layer1.parameters(), 1)\n",
        "          torch.nn.utils.clip_grad_norm_(LSTM_model_layer2.parameters(), 1)\n",
        "          torch.nn.utils.clip_grad_norm_(LSTM_model_layer3.parameters(), 1)\n",
        "          optimizer.step()\n",
        "\n",
        "      # Sample from model\n",
        "      if train_cycle_num % 1 == 0:\n",
        "        test_cycle()\n",
        "\n",
        "    except StopIteration:\n",
        "      break\n",
        "\n",
        "# Sample from model to test progress every once in a while\n",
        "def test_cycle():\n",
        "  LSTM_model_layer1.eval()\n",
        "  LSTM_model_layer2.eval()\n",
        "  LSTM_model_layer3.eval()\n",
        "\n",
        "  print(\"Test Cycle\")\n",
        "  char_dist = torch.ones(53)\n",
        "\n",
        "  for i in range(1, 250):\n",
        "    prev_char = torch.zeros(53)\n",
        "    prev_char[list(torch.utils.data.WeightedRandomSampler(char_dist, 1))[0]] = 1.0\n",
        "\n",
        "    model_hidden_states_l1 = []\n",
        "    model_cell_states_l1 = []\n",
        "    model_hidden_states_l2 = []\n",
        "    model_cell_states_l2 = []\n",
        "    model_hidden_states_l3 = []\n",
        "    model_cell_states_l3 = []\n",
        "\n",
        "    model_hidden_states_l1.append(torch.zeros(512))\n",
        "    model_cell_states_l1.append(torch.zeros(512))\n",
        "    model_hidden_states_l2.append(torch.zeros(512))\n",
        "    model_cell_states_l2.append(torch.zeros(512))\n",
        "    model_hidden_states_l3.append(torch.zeros(512))\n",
        "    model_cell_states_l3.append(torch.zeros(512))\n",
        "\n",
        "    model_output_l1, model_temp_hidden_state_l1, model_temp_cell_state_l1 = LSTM_model_layer1(prev_char * 2 - 1, model_hidden_states_l1[-1], model_cell_states_l1[-1])\n",
        "    model_output_l2, model_temp_hidden_state_l2, model_temp_cell_state_l2 = LSTM_model_layer2(model_output_l1, model_hidden_states_l2[-1], model_cell_states_l2[-1])\n",
        "    model_output_l3, model_temp_hidden_state_l3, model_temp_cell_state_l3 = LSTM_model_layer3(model_output_l2, model_hidden_states_l3[-1], model_cell_states_l3[-1])\n",
        "\n",
        "    model_hidden_states_l1.append(model_temp_hidden_state_l1)\n",
        "    model_cell_states_l1.append(model_temp_cell_state_l1)\n",
        "    model_hidden_states_l2.append(model_temp_hidden_state_l2)\n",
        "    model_cell_states_l2.append(model_temp_cell_state_l2)\n",
        "    model_hidden_states_l3.append(model_temp_hidden_state_l3)\n",
        "    model_cell_states_l3.append(model_temp_cell_state_l3)\n",
        "\n",
        "    char_dist = torch.nn.functional.sigmoid(model_output_l3)\n",
        "    print(onehot_to_char(model_output_l3.detach()), end = \"\")\n",
        "\n",
        "# Training cycles\n",
        "for epoch in range(epochs):\n",
        "  print(f\"Epoch {epoch}\")\n",
        "  training_cycle()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experimental test\n",
        "\n",
        "import torch\n",
        "from torchtext import datasets\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "from time import sleep\n",
        "from torch import nn\n",
        "\n",
        "# Main LSTM architecture definition\n",
        "class LSTM(nn.Module):\n",
        "  # Initialize state\n",
        "  def __init__(self, inp_sz, hidden_sz, out_sz):\n",
        "    super().__init__()\n",
        "    self.inp_sz = inp_sz\n",
        "    self.hidden_sz = hidden_sz\n",
        "    self.cell_sz = hidden_sz\n",
        "    self.output_sz = out_sz\n",
        "\n",
        "    self.W_f = torch.nn.Parameter(torch.Tensor(self.inp_sz + self.hidden_sz, self.cell_sz))\n",
        "    self.b_f = torch.nn.Parameter(torch.Tensor(self.cell_sz))\n",
        "\n",
        "    self.W_ii = torch.nn.Parameter(torch.Tensor(self.inp_sz + self.hidden_sz, self.cell_sz))\n",
        "    self.b_ii = torch.nn.Parameter(torch.Tensor(self.cell_sz))\n",
        "    self.W_iC = torch.nn.Parameter(torch.Tensor(self.inp_sz + self.hidden_sz, self.cell_sz))\n",
        "    self.b_iC = torch.nn.Parameter(torch.Tensor(self.cell_sz))\n",
        "\n",
        "    self.W_o = torch.nn.Parameter(torch.Tensor(self.inp_sz + self.hidden_sz, self.cell_sz))\n",
        "    self.b_o = torch.nn.Parameter(torch.Tensor(self.cell_sz))\n",
        "\n",
        "    self.W_out = torch.nn.Parameter(torch.Tensor(self.cell_sz, self.output_sz))\n",
        "    self.b_out = torch.nn.Parameter(torch.Tensor(self.output_sz))\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "  # Initialize weights in LSTM according to Xavier Initialization\n",
        "  def init_weights(self):\n",
        "    for weight in self.parameters():\n",
        "      weight.data.uniform_(-1.0/math.sqrt(self.inp_sz / 6.0), 1.0/math.sqrt(self.inp_sz / 6.0))\n",
        "\n",
        "  # Forward Propagation Logic\n",
        "  def forward(self, x, h, c):\n",
        "    new_cell = c\n",
        "    new_hidden = h\n",
        "\n",
        "    hidden_inp = torch.cat((x, h))\n",
        "\n",
        "    new_cell = new_cell * torch.sigmoid(hidden_inp @ self.W_f + self.b_f)\n",
        "    new_cell = new_cell + (torch.sigmoid(hidden_inp @ self.W_ii + self.b_ii) * torch.tanh(hidden_inp @ self.W_iC + self.b_iC))\n",
        "\n",
        "    new_hidden = torch.sigmoid(hidden_inp @ self.W_o + self.b_o) * torch.tanh(new_cell)\n",
        "\n",
        "    new_output = torch.tanh(new_hidden @ self.W_out + self.b_out)\n",
        "\n",
        "    return new_output, new_hidden, new_cell\n",
        "\n",
        "# Filter characters in input data for relevant characters (alphabetic and spaces)\n",
        "def keep_char(x):\n",
        "  return x.isalpha() or x.isspace()\n",
        "\n",
        "# Convert character to index in one-hot vector representation of characters\n",
        "def char_index(x):\n",
        "  if ord(x) < 90 and ord(x) != 32:\n",
        "    return ord(x) - 65\n",
        "  if ord(x) < 122 and ord(x) != 32:\n",
        "    return ord(x) - 71\n",
        "  return 52\n",
        "\n",
        "# Convert character vector\n",
        "def onehot_to_char(x):\n",
        "  ind = np.argmax(x)\n",
        "\n",
        "  if ind < 26:\n",
        "    return chr(ind + 65)\n",
        "  if ind != 52:\n",
        "    return chr(ind + 71)\n",
        "  return chr(32)\n",
        "\n",
        "# Training data (IMDB database in torchtext)\n",
        "training_dataloader = iter(datasets.IMDB(split=\"train\", root = \"data\"))\n",
        "\n",
        "# Epochs to train model for (each epoch loops through the corpus once)\n",
        "epochs = 50\n",
        "\n",
        "# Number of characters to sample from model each testing cycle\n",
        "sample_length = 250\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "\n",
        "# Model, optimizer, and loss function instances\n",
        "LSTM_model_layer1 = LSTM(53, 512, 53).to(device)\n",
        "optimizer = torch.optim.Adam(LSTM_model_layer1.parameters())\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train model\n",
        "def training_cycle():\n",
        "  LSTM_model_layer1.train()\n",
        "\n",
        "  train_cycle_num = 1\n",
        "\n",
        "  while True:\n",
        "    try:\n",
        "\n",
        "      print(f\"Train Cycle: {train_cycle_num}\")\n",
        "      train_cycle_num += 1\n",
        "\n",
        "      string = ''.join(filter(keep_char,next(training_dataloader)[1]))\n",
        "\n",
        "      string_onehot = []\n",
        "\n",
        "      ind = 0\n",
        "      for char in string:\n",
        "        ind += 1\n",
        "\n",
        "        onehot_char = torch.zeros(53)\n",
        "        # onehot_char[char_index(char)] = 1.0\n",
        "        onehot_char[ind%2] = 1.0\n",
        "\n",
        "        string_onehot.append(onehot_char)\n",
        "\n",
        "      for i in range(1, len(string_onehot)):\n",
        "        if i % 100 == 1:\n",
        "          model_loss = 0\n",
        "          model_hidden_states_l1 = []\n",
        "          model_cell_states_l1 = []\n",
        "\n",
        "          model_hidden_states_l1.append(torch.zeros(512))\n",
        "          model_cell_states_l1.append(torch.zeros(512))\n",
        "\n",
        "        if i % 100 < 75:\n",
        "          model_output_l1, model_temp_hidden_state_l1, model_temp_cell_state_l1 = LSTM_model_layer1(string_onehot[i-1] * 2 - 1, model_hidden_states_l1[-1], model_cell_states_l2[-1])\n",
        "          model_output_l2, model_temp_hidden_state_l2, model_temp_cell_state_l2 = LSTM_model_layer2(model_output_l1, model_hidden_states_l2[-1], model_cell_states_l2[-1])\n",
        "          model_output_l3, model_temp_hidden_state_l3, model_temp_cell_state_l3 = LSTM_model_layer3(model_output_l2, model_hidden_states_l3[-1], model_cell_states_l3[-1])\n",
        "        else:\n",
        "          model_output_l1, model_temp_hidden_state_l1, model_temp_cell_state_l1 = LSTM_model_layer1(pred_char, model_hidden_states_l1[-1], model_cell_states_l2[-1])\n",
        "          model_output_l2, model_temp_hidden_state_l2, model_temp_cell_state_l2 = LSTM_model_layer2(model_output_l1, model_hidden_states_l2[-1], model_cell_states_l2[-1])\n",
        "          model_output_l3, model_temp_hidden_state_l3, model_temp_cell_state_l3 = LSTM_model_layer3(model_output_l2, model_hidden_states_l3[-1], model_cell_states_l3[-1])\n",
        "\n",
        "          # print(onehot_to_char(string_onehot[i]), end = \"\")\n",
        "          print(onehot_to_char(model_output_l3.detach()), end = \"\")\n",
        "          #print(string_onehot[i])\n",
        "          model_loss += loss((model_output_l3 +1) / 2, string_onehot[i])\n",
        "\n",
        "        model_hidden_states_l1.append(model_temp_hidden_state_l1)\n",
        "        model_cell_states_l1.append(model_temp_cell_state_l1)\n",
        "        model_hidden_states_l2.append(model_temp_hidden_state_l2)\n",
        "        model_cell_states_l2.append(model_temp_cell_state_l2)\n",
        "        model_hidden_states_l3.append(model_temp_hidden_state_l3)\n",
        "        model_cell_states_l3.append(model_temp_cell_state_l3)\n",
        "\n",
        "        pred_char = model_output_l3\n",
        "\n",
        "        if i % 100 == 0:\n",
        "          print(f\"Average Loss Per Char: {model_loss.item() / 50}\")\n",
        "          LSTM_model_layer1.zero_grad()\n",
        "          LSTM_model_layer2.zero_grad()\n",
        "          LSTM_model_layer3.zero_grad()\n",
        "          model_loss.backward()\n",
        "\n",
        "          torch.nn.utils.clip_grad_norm_(LSTM_model_layer1.parameters(), 1)\n",
        "          torch.nn.utils.clip_grad_norm_(LSTM_model_layer2.parameters(), 1)\n",
        "          torch.nn.utils.clip_grad_norm_(LSTM_model_layer3.parameters(), 1)\n",
        "          optimizer.step()\n",
        "\n",
        "      # Sample from model\n",
        "      if train_cycle_num % 1 == 0:\n",
        "        test_cycle()\n",
        "\n",
        "    except StopIteration:\n",
        "      break\n",
        "\n",
        "# Sample from model to test progress every once in a while\n",
        "def test_cycle():\n",
        "  LSTM_model_layer1.eval()\n",
        "  LSTM_model_layer2.eval()\n",
        "  LSTM_model_layer3.eval()\n",
        "\n",
        "  print(\"Test Cycle\")\n",
        "  char_dist = torch.ones(53)\n",
        "\n",
        "  for i in range(1, 250):\n",
        "    prev_char = torch.zeros(53)\n",
        "    prev_char[list(torch.utils.data.WeightedRandomSampler(char_dist, 1))[0]] = 1.0\n",
        "\n",
        "    model_hidden_states_l1 = []\n",
        "    model_cell_states_l1 = []\n",
        "    model_hidden_states_l2 = []\n",
        "    model_cell_states_l2 = []\n",
        "    model_hidden_states_l3 = []\n",
        "    model_cell_states_l3 = []\n",
        "\n",
        "    model_hidden_states_l1.append(torch.zeros(512))\n",
        "    model_cell_states_l1.append(torch.zeros(512))\n",
        "    model_hidden_states_l2.append(torch.zeros(512))\n",
        "    model_cell_states_l2.append(torch.zeros(512))\n",
        "    model_hidden_states_l3.append(torch.zeros(512))\n",
        "    model_cell_states_l3.append(torch.zeros(512))\n",
        "\n",
        "    model_output_l1, model_temp_hidden_state_l1, model_temp_cell_state_l1 = LSTM_model_layer1(prev_char * 2 - 1, model_hidden_states_l1[-1], model_cell_states_l2[-1])\n",
        "    model_output_l2, model_temp_hidden_state_l2, model_temp_cell_state_l2 = LSTM_model_layer2(model_output_l1, model_hidden_states_l2[-1], model_cell_states_l2[-1])\n",
        "    model_output_l3, model_temp_hidden_state_l3, model_temp_cell_state_l3 = LSTM_model_layer3(model_output_l2, model_hidden_states_l3[-1], model_cell_states_l3[-1])\n",
        "\n",
        "    model_hidden_states_l1.append(model_temp_hidden_state_l1)\n",
        "    model_cell_states_l1.append(model_temp_cell_state_l1)\n",
        "    model_hidden_states_l2.append(model_temp_hidden_state_l2)\n",
        "    model_cell_states_l2.append(model_temp_cell_state_l2)\n",
        "    model_hidden_states_l3.append(model_temp_hidden_state_l3)\n",
        "    model_cell_states_l3.append(model_temp_cell_state_l3)\n",
        "\n",
        "    char_dist = torch.nn.functional.sigmoid(model_output_l3)\n",
        "    print(onehot_to_char(model_output_l3.detach()), end = \"\")\n",
        "\n",
        "# Training cycles\n",
        "for epoch in range(epochs):\n",
        "  print(f\"Epoch {epoch}\")\n",
        "  training_cycle()"
      ],
      "metadata": {
        "id": "kekFlLLXcvx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzxP6vRc1X7A"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}